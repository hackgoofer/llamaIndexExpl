Published as a conference paper at ICLR 2023
$FW7KLQN>)LUVW,QHHGWRILQGDSHSSHUVKDNHUPRUHOLNHO\WRDSSHDULQFDELQHWVFRXQWHUWRSV@$FW*RWRFDELQHW2EV2QWKHFDELQHW\RXVHHDYDVH+HUHWKHDJHQWJRWRFDELQHWWKHQFDELQHWWKHQFRXQWHUWRSDQG$FW*RWRFRXQWHUWRS2EV2QWKHFRXQWHUWRS\RXVHHDDSSOHDEUHDGDSHSSHUVKDNHUDQGDYDVH$FW7DNHSHSSHUVKDNHUIURPFRXQWHUWRS2EV<RXSLFNXSWKHSHSSHUVKDNHUIURPWKHFRXQWHUWRS$FW7KLQN>1RZ,ILQGDSHSSHUVKDNHU1H[W,QHHGWRSXWLWLQRQGUDZHU@$FW*RWRGUDZHU2EV'UDZHULVFORVHG$FW2SHQGUDZHU2EV<RXRSHQ'UDZHUÂª$FW3XWSHSSHUVKDNHULQRQGUDZHU2EV<RXSXWSHSSHUVKDNHULQRQWKHGUDZHUD6WDQGDUG$QVZHUL3RGE&R75HDVRQ2QO\7KRXJKW/HW
VWKLQNVWHSE\VWHS$SSOH5HPRWHZDVRULJLQDOO\GHVLJQHGWRLQWHUDFWZLWK$SSOH79$SSOH79FDQEHFRQWUROOHGE\L3KRQHL3DGDQGL3RG7RXFK6RWKHDQVZHULVL3KRQHL3DGDQGL3RG7RXFK$QVZHUL3KRQHL3DGL3RG7RXFKF$FW2QO\$FW6HDUFK>$SSOH5HPRWH@2EV7KH$SSOH5HPRWHLVDUHPRWHFRQWUROÂª$FW6HDUFK>)URQW5RZ@2EV&RXOGQRWILQG>)URQW5RZ@6LPLODUÂª$FW6HDUFK>)URQW5RZVRIWZDUH@2EV)URQW5RZLVDGLVFRQWLQXHGPHGLDFHQWHUVRIWZDUHÂª$FW)LQLVK>\HV@G5H$FW5HDVRQ$FW7KRXJKW,QHHGWRVHDUFK$SSOH5HPRWHDQGILQGWKHSURJUDPLWZDVRULJLQDOO\GHVLJQHGWRLQWHUDFWZLWK$FW6HDUFK>$SSOH5HPRWH@2EV7KH$SSOH5HPRWHLVDUHPRWHFRQWUROLQWURGXFHGLQ2FWREHUE\$SSOHÂªRULJLQDOO\GHVLJQHGWRFRQWUROWKH)URQW5RZPHGLDFHQWHUSURJUDPÂª7KRXJKW$SSOH5HPRWHZDVRULJLQDOO\GHVLJQHGWRFRQWUROWKH)URQW5RZPHGLDFHQWHUSURJUDP,QHHGWRVHDUFK)URQW5RZQH[WDQGILQGZKDWRWKHUGHYLFHFDQFRQWUROLW$FW6HDUFK>)URQW5RZ@2EV&RXOGQRWILQG>)URQW5RZ@6LPLODU>
)URQW5RZ6HDWWR(DUWK

)URQW5RZ0RWRUVSRUWV

)URQW5RZVRIWZDUH
Âª@7KRXJKW)URQW5RZLVQRWIRXQG,QHHGWRVHDUFK)URQW5RZVRIWZDUH$FW6HDUFK>)URQW5RZVRIWZDUH@2EV)URQW5RZLVDGLVFRQWLQXHGPHGLDFHQWHUVRIWZDUHÂª7KRXJKW)URQW5RZVRIWZDUHLVFRQWUROOHGE\DQ$SSOH5HPRWHRUWKHNH\ERDUGIXQFWLRQNH\V6RWKHDQVZHULVNH\ERDUGIXQFWLRQNH\V$FW)LQLVK>NH\ERDUGIXQFWLRQNH\V@Ò¼4XHVWLRQ$VLGHIURPWKH$SSOH5HPRWHZKDWRWKHUGHYLFHFDQFRQWUROWKHSURJUDP$SSOH5HPRWHZDVRULJLQDOO\GHVLJQHGWRLQWHUDFWZLWK"
D$FW2QO\$FW*RWRGUDZHU2EV7KHGUDZHULVFORVHG$FW2SHQGUDZHU2EV<RXRSHQWKHGUDZHU7KHGUDZHULVRSHQ,QLW\RXVHHDGLVKVSRQJHDQGDVSRRQ$FW*RWRVLQNEDVLQ2EV2QWKHVLQNEDVLQ\RXVHHDGLVKVSRQJHDVSDWXODDQGDVSRRQ$FW7DNHSHSSHUVKDNHUIURPVLQNEDVLQ2EV1RWKLQJKDSSHQV$FW7DNHSHSSHUVKDNHUIURPVLQNEDVLQ2EV1RWKLQJKDSSHQVE5H$FW5HDVRQ$FW
Ò¼<RXDUHLQWKHPLGGOHRIDURRP/RRNLQJTXLFNO\DURXQG\RX\RXVHHDFDELQHWDFDELQHWDFRIIHHPDFKLQHDFRXQWHUWRSDVWRYHEXUQHUDQGDWRDVWHU<RXUWDVNLVWR3XWVRPHSHSSHUVKDNHURQDGUDZHU$OI:RUOG+RWVSRW4$
Figure 1: (1) Comparison of 4 prompting methods, (a) Standard , (b) Chain-of-thought ( CoT,
Reason Only), (c) Act-only, and (d) ReAct (Reason+Act), solving a HotpotQA (Yang et al., 2018)
question. (2) Comparison of (a) Act-only and (b) ReAct prompting to solve an AlfWorld (Shridhar
et al., 2020b) game. In both domains, we omit in-context examples in the prompt, and only show task
solving trajectories generated by the model (Act, Thought) and the environment (Obs).
answers from questions in arithmetic, commonsense, and symbolic reasoning tasks (Wei et al.,
2022). However, this â€œchain-of-thoughtâ€ reasoning is a static black box, in that the model uses
its own internal representations to generate thoughts and is not grounded in the external world,
which limits its ability to reason reactively or update its knowledge. This can lead to issues like fact
hallucination and error propagation over the reasoning process (Figure 1 (1b)). On the other hand,
recent work has explored the use of pre-trained language models for planning and acting in interactive
environments (Ahn et al., 2022; Nakano et al., 2021; Yao et al., 2020; Huang et al., 2022a), with
a focus on predicting actions via language priors. These approaches usually convert multi-modal
observations into text, use a language model to generate domain-speciï¬c actions or plans, and then
use a controller to choose or execute them. However, they do not employ language models to reason
abstractly about high-level goals or maintain a working memory to support acting, barring Huang
et al. (2022b) who perform a limited form of verbal reasoning to reiterate spatial facts about the
current state. Beyond such simple embodied tasks to interact with a few blocks, there have not been
studies on how reasoning and acting can be combined in a synergistic manner for general task solving,
and if such a combination can bring systematic beneï¬ts compared to reasoning or acting alone.
In this work, we present ReAct , a general paradigm to combine reasoning and acting with language
models for solving diverse language reasoning and decision making tasks (Figure 1). ReAct
prompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an
interleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and
adjust high-level plans for acting (reason to act), while also interact with the external environments
(e.g. Wikipedia) to incorporate additional information into reasoning (act to reason).
2