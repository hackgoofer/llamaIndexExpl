{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying LlamaIndex Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure out the best way to grab the doc structure -- Decision: Use Sphinx/Doctree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Sphinx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pickle\n",
    "\n",
    "def read_doctree(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<document source=\"/Users/sasha/github/LlamaIndex/llama_index/docs/getting_started/installation.md\"><section ids=\"installation-and-setup\" myst-anchor=\"getting_started/installation.md#installation-and-setup\" names=\"installation\\ and\\ setup\"><title>Installation and Setup</title><section ids=\"installation-from-pip\" myst-anchor=\"getting_started/installation.md#installation-from-pip\" names=\"installation\\ from\\ pip\"><title>Installation from Pip</title><paragraph>Install from pip:</paragraph><literal_block language=\"default\" xml:space=\"preserve\">pip install llama-index\n",
      "</literal_block><paragraph><strong>NOTE:</strong> LlamaIndex may download and store local files for various packages (NLTK, HuggingFace, ‚Ä¶). Use the environment variable ‚ÄúLLAMA_INDEX_CACHE_DIR‚Äù to control where these files are saved.</paragraph><paragraph>If you prefer to install from source, see below.</paragraph></section><section ids=\"important-openai-environment-setup\" myst-anchor=\"getting_started/installation.md#important-openai-environment-setup\" names=\"important:\\ openai\\ environment\\ setup\"><title>Important: OpenAI Environment Setup</title><paragraph>By default, we use the OpenAI <literal>gpt-3.5-turbo</literal> model for text generation and <literal>text-embedding-ada-002</literal> for retrieval and embeddings. In order to use this, you must have an OPENAI_API_KEY set up as an environment variable.\n",
      "You can obtain an API key by logging into your OpenAI account and <reference refuri=\"https://platform.openai.com/account/api-keys\">and creating a new API key</reference>.</paragraph><tip><paragraph>You can also <pending_xref refdoc=\"getting_started/installation\" refdomain=\"True\" refexplicit=\"True\" reftarget=\"/module_guides/models/llms/usage_custom.md\" reftype=\"myst\" refwarn=\"True\"><inline classes=\"xref myst\">use one of many other available LLMs</inline></pending_xref>. You may\n",
      "need additional environment keys + tokens setup depending on the LLM provider.</paragraph></tip></section><section ids=\"local-model-setup\" myst-anchor=\"getting_started/installation.md#local-model-setup\" names=\"local\\ model\\ setup\"><title>Local Model Setup</title><paragraph>If you don‚Äôt wish to use OpenAI, consider setting up a local LLM and embedding model in the service context.</paragraph><paragraph>A full guide to using and configuring LLMs available <pending_xref refdoc=\"getting_started/installation\" refdomain=\"True\" refexplicit=\"True\" reftarget=\"/module_guides/models/llms.md\" reftype=\"myst\" refwarn=\"True\"><inline classes=\"xref myst\">here</inline></pending_xref>.</paragraph><paragraph>A full guide to using and configuring embedding models is available <pending_xref refdoc=\"getting_started/installation\" refdomain=\"True\" refexplicit=\"True\" reftarget=\"/module_guides/models/embeddings.md\" reftype=\"myst\" refwarn=\"True\"><inline classes=\"xref myst\">here</inline></pending_xref>.</paragraph></section><section ids=\"installation-from-source\" myst-anchor=\"getting_started/installation.md#installation-from-source\" names=\"installation\\ from\\ source\"><title>Installation from Source</title><paragraph>Git clone this repository: <literal>git clone https://github.com/jerryjliu/llama_index.git</literal>. Then do the following:</paragraph><bullet_list bullet=\"-\"><list_item><paragraph><reference refuri=\"https://python-poetry.org/docs/#installation\">Install poetry</reference> - this will help you manage package dependencies</paragraph></list_item><list_item><paragraph><literal>poetry shell</literal> - this command creates a virtual environment, which keeps installed packages contained to this project</paragraph></list_item><list_item><paragraph><literal>poetry install</literal> - this will install the core package requirements</paragraph></list_item><list_item><paragraph>(Optional) <literal>poetry install --with dev,docs</literal> - this will install all dependencies needed for most local development</paragraph></list_item></bullet_list></section><section ids=\"optional-dependencies\" myst-anchor=\"getting_started/installation.md#optional-dependencies\" names=\"optional\\ dependencies\"><title>Optional Dependencies</title><paragraph>By default LlamaIndex installs a core set of dependencies; we also provide a convenient way to install commonly-required optional dependencies. These are currently in three sets:</paragraph><bullet_list bullet=\"-\"><list_item><paragraph><literal>pip install llama-index[local_models]</literal> installs tools useful for private LLMs, local inference, and HuggingFace models</paragraph></list_item><list_item><paragraph><literal>pip install llama-index[postgres]</literal> is useful if you are working with Postgres, PGVector or Supabase</paragraph></list_item><list_item><paragraph><literal>pip install llama-index[query_tools]</literal> gives you tools for hybrid search, structured outputs, and node post-processing</paragraph></list_item></bullet_list></section></section></document>\n"
     ]
    }
   ],
   "source": [
    "# Replace 'path_to_doctree_file.doctree' with your .doctree file path\n",
    "# doctree = read_doctree('/Users/sasha/github/LlamaIndex/llama_index/docs/_build/doctrees/index.doctree')\n",
    "# agentic_strategies_doctree = read_doctree('/Users/sasha/github/LlamaIndex/llama_index/docs/_build/doctrees/optimizing/advanced_retrieval/advanced_retrieval.doctree')\n",
    "installation_doctree = read_doctree('/Users/sasha/github/LlamaIndex/llama_index/docs/_build/doctrees/getting_started/installation.doctree')\n",
    "# print(doctree)\n",
    "# print(agentic_strategies_doctree)\n",
    "print(installation_doctree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<class 'docutils.nodes.section'>, <class 'sphinx.addnodes.desc_signature'>, <class 'docutils.nodes.problematic'>, <class 'docutils.nodes.target'>, <class 'docutils.nodes.system_message'>}\n",
      "\n",
      "Examples for <class 'docutils.nodes.section'>:\n",
      "<section ids=\"welcome-to-llamaindex\" names=\"welcome\\ to\\ llamaindex\\ ü¶ô\\ !\"><title>Welcome to LlamaIndex ü¶ô !</title><paragraph>LlamaIndex is a data framework for <reference name=\"LLM\" refuri=\"https://en.wikipedia.org/wiki/Large_language_model\">LLM</reference><target ids=\"['llm']\" names=\"['llm']\" refuri=\"https://en.wikipedia.org/wiki/Large_language_model\"/>-based applications to ingest, structure, and access private or domain-specific data. It‚Äôs available in Python (these docs) and <reference name=\"Typescript\" refuri=\"https://ts.llamaindex.ai/\">Typescript</reference><target ids=\"['typescript']\" names=\"['typescript']\" refuri=\"https://ts.llamaindex.ai/\"/>.</paragraph><section ids=\"why-llamaindex\" names=\"üöÄ\\ why\\ llamaindex?\"><title>üöÄ Why LlamaIndex?</title><paragraph>LLMs offer a natural language interface between humans and data. Widely available models come pre-trained on huge amounts of publicly available data like Wikipedia, mailing lists, textbooks, source code and more.</paragraph><paragraph>However, while LLMs are trained on a great deal of data, they are not trained on <strong>your</strong> data, which may be private or specific to the problem you‚Äôre trying to solve. It‚Äôs behind APIs, in SQL databases, or trapped in PDFs and slide decks.</paragraph><paragraph>You may choose to <strong>fine-tune</strong> a LLM with your data, but:</paragraph><bullet_list bullet=\"-\"><list_item><paragraph>Training a LLM is <strong>expensive</strong>.</paragraph></list_item><list_item><paragraph>Due to the cost to train, it‚Äôs <strong>hard to update</strong> a LLM with latest information.</paragraph></list_item><list_item><paragraph><strong>Observability</strong> is lacking. When you ask a LLM a question, it‚Äôs not obvious how the LLM arrived at its answer.</paragraph></list_item></bullet_list><paragraph>LlamaIndex takes a different approach called <reference name=\"Retrieval-Augmented Generation (RAG)\" refuri=\"./getting_started/concepts.html\">Retrieval-Augmented Generation (RAG)</reference><target ids=\"['retrieval-augmented-generation-rag']\" names=\"['retrieval-augmented generation (rag)']\" refuri=\"./getting_started/concepts.html\"/>. Instead of asking LLM to generate an answer immediately, LlamaIndex:</paragraph><enumerated_list enumtype=\"arabic\" prefix=\"\" suffix=\".\"><list_item><paragraph>retrieves information from your data sources first,</paragraph></list_item><list_item><paragraph>adds it to your question as context, and</paragraph></list_item><list_item><paragraph>asks the LLM to answer based on the enriched prompt.</paragraph></list_item></enumerated_list><paragraph>RAG overcomes all three weaknesses of the fine-tuning approach:</paragraph><bullet_list bullet=\"-\"><list_item><paragraph>There‚Äôs no training involved, so it‚Äôs <strong>cheap</strong>.</paragraph></list_item><list_item><paragraph>Data is fetched only when you ask for them, so it‚Äôs <strong>always up to date</strong>.</paragraph></list_item><list_item><paragraph>LlamaIndex can show you the retrieved documents, so it‚Äôs <strong>more trustworthy</strong>.</paragraph></list_item></bullet_list><paragraph>LlamaIndex imposes no restriction on how you use LLMs. You can still use LLMs as auto-complete, chatbots, semi-autonomous agents, and more (see Use Cases on the left). It only makes LLMs more relevant to you.</paragraph></section><section ids=\"how-can-llamaindex-help\" names=\"ü¶ô\\ how\\ can\\ llamaindex\\ help?\"><title>ü¶ô How can LlamaIndex help?</title><paragraph>LlamaIndex provides the following tools:</paragraph><bullet_list bullet=\"-\"><list_item><paragraph><strong>Data connectors</strong> ingest your existing data from their native source and format. These could be APIs, PDFs, SQL, and (much) more.</paragraph></list_item><list_item><paragraph><strong>Data indexes</strong> structure your data in intermediate representations that are easy and performant for LLMs to consume.</paragraph></list_item><list_item><paragraph><strong>Engines</strong> provide natural language access to your data. For example:</paragraph><bullet_list bullet=\"-\"><list_item><paragraph>Query engines are powerful retrieval interfaces for knowledge-augmented output.</paragraph></list_item><list_item><paragraph>Chat engines are conversational interfaces for multi-message, ‚Äúback and forth‚Äù interactions with your data.</paragraph></list_item></bullet_list></list_item><list_item><paragraph><strong>Data agents</strong> are LLM-powered knowledge workers augmented by tools, from simple helper functions to API integrations and more.</paragraph></list_item><list_item><paragraph><strong>Application integrations</strong> tie LlamaIndex back into the rest of your ecosystem. This could be LangChain, Flask, Docker, ChatGPT, or‚Ä¶ anything else!</paragraph></list_item></bullet_list></section><section ids=\"who-is-llamaindex-for\" names=\"üë®‚Äçüë©‚Äçüëß‚Äçüë¶\\ who\\ is\\ llamaindex\\ for?\"><title>üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Who is LlamaIndex for?</title><paragraph>LlamaIndex provides tools for beginners, advanced users, and everyone in between.</paragraph><paragraph>Our high-level API allows beginner users to use LlamaIndex to ingest and query their data in 5 lines of code.</paragraph><paragraph>For more complex applications, our lower-level APIs allow advanced users to customize and extend any module‚Äîdata connectors, indices, retrievers, query engines, reranking modules‚Äîto fit their needs.</paragraph></section><section ids=\"getting-started\" names=\"getting\\ started\"><title>Getting Started</title><paragraph>To install the library:</paragraph><paragraph><literal>pip install llama-index</literal></paragraph><paragraph>We recommend starting at <reference name=\"how to read these docs\" refuri=\"./getting_started/reading.html\">how to read these docs</reference><target ids=\"['how-to-read-these-docs']\" names=\"['how to read these docs']\" refuri=\"./getting_started/reading.html\"/>, which will point you to the right place based on your experience level.</paragraph></section><section ids=\"ecosystem\" names=\"üó∫Ô∏è\\ ecosystem\"><title>üó∫Ô∏è Ecosystem</title><paragraph>To download or contribute, find LlamaIndex on:</paragraph><bullet_list bullet=\"-\"><list_item><paragraph>Github: <reference refuri=\"https://github.com/jerryjliu/llama_index\">https://github.com/jerryjliu/llama_index</reference></paragraph></list_item><list_item><paragraph>PyPi:</paragraph><bullet_list bullet=\"-\"><list_item><paragraph>LlamaIndex: <reference refuri=\"https://pypi.org/project/llama-index/\">https://pypi.org/project/llama-index/</reference>.</paragraph></list_item><list_item><paragraph>GPT Index (duplicate): <reference refuri=\"https://pypi.org/project/gpt-index/\">https://pypi.org/project/gpt-index/</reference>.</paragraph></list_item></bullet_list></list_item><list_item><definition_list><definition_list_item><term>NPM (Typescript/Javascript):</term><definition><bullet_list bullet=\"-\"><list_item><paragraph>Github: <reference refuri=\"https://github.com/run-llama/LlamaIndexTS\">https://github.com/run-llama/LlamaIndexTS</reference></paragraph></list_item><list_item><paragraph>Docs: <reference refuri=\"https://ts.llamaindex.ai/\">https://ts.llamaindex.ai/</reference></paragraph></list_item><list_item><paragraph>LlamaIndex.TS: <reference refuri=\"https://www.npmjs.com/package/llamaindex\">https://www.npmjs.com/package/llamaindex</reference></paragraph></list_item></bullet_list></definition></definition_list_item></definition_list></list_item></bullet_list><section ids=\"community\" names=\"community\"><title>Community</title><paragraph>Need help? Have a feature suggestion? Join the LlamaIndex community:</paragraph><bullet_list bullet=\"-\"><list_item><paragraph>Twitter: <reference refuri=\"https://twitter.com/llama_index\">https://twitter.com/llama_index</reference></paragraph></list_item><list_item><paragraph>Discord <reference refuri=\"https://discord.gg/dGcwcsnxhU\">https://discord.gg/dGcwcsnxhU</reference></paragraph></list_item></bullet_list></section><section ids=\"associated-projects\" names=\"associated\\ projects\"><title>Associated projects</title><bullet_list bullet=\"-\"><list_item><paragraph>üè° LlamaHub: <reference refuri=\"https://llamahub.ai\">https://llamahub.ai</reference> | A large (and growing!) collection of custom data connectors</paragraph></list_item><list_item><paragraph>üß™ LlamaLab: <reference refuri=\"https://github.com/run-llama/llama-lab\">https://github.com/run-llama/llama-lab</reference> | Ambitious projects built on top of LlamaIndex</paragraph></list_item></bullet_list><compound classes=\"toctree-wrapper\"><toctree caption=\"Getting Started\" entries=\"[(None, 'getting_started/installation'), (None, 'getting_started/reading'), (None, 'getting_started/starter_example'), (None, 'getting_started/concepts'), (None, 'getting_started/customization'), (None, 'getting_started/discover_llamaindex')]\" glob=\"False\" hidden=\"True\" includefiles=\"['getting_started/installation', 'getting_started/reading', 'getting_started/starter_example', 'getting_started/concepts', 'getting_started/customization', 'getting_started/discover_llamaindex']\" includehidden=\"False\" maxdepth=\"1\" numbered=\"0\" parent=\"index\" rawcaption=\"Getting Started\" rawentries=\"[]\" titlesonly=\"False\"/></compound><compound classes=\"toctree-wrapper\"><toctree caption=\"Use Cases\" entries=\"[(None, 'use_cases/q_and_a'), (None, 'use_cases/chatbots'), (None, 'use_cases/agents'), (None, 'use_cases/extraction'), (None, 'use_cases/multimodal')]\" glob=\"False\" hidden=\"True\" includefiles=\"['use_cases/q_and_a', 'use_cases/chatbots', 'use_cases/agents', 'use_cases/extraction', 'use_cases/multimodal']\" includehidden=\"False\" maxdepth=\"2\" numbered=\"0\" parent=\"index\" rawcaption=\"Use Cases\" rawentries=\"[]\" titlesonly=\"False\"/></compound><compound classes=\"toctree-wrapper\"><toctree caption=\"Understanding\" entries=\"[(None, 'understanding/understanding'), (None, 'understanding/using_llms/using_llms'), (None, 'understanding/loading/loading'), (None, 'understanding/indexing/indexing'), (None, 'understanding/storing/storing'), (None, 'understanding/querying/querying'), (None, 'understanding/putting_it_all_together/putting_it_all_together'), (None, 'understanding/tracing_and_debugging/tracing_and_debugging'), (None, 'understanding/evaluating/evaluating')]\" glob=\"False\" hidden=\"True\" includefiles=\"['understanding/understanding', 'understanding/using_llms/using_llms', 'understanding/loading/loading', 'understanding/indexing/indexing', 'understanding/storing/storing', 'understanding/querying/querying', 'understanding/putting_it_all_together/putting_it_all_together', 'understanding/tracing_and_debugging/tracing_and_debugging', 'understanding/evaluating/evaluating']\" includehidden=\"False\" maxdepth=\"2\" numbered=\"0\" parent=\"index\" rawcaption=\"Understanding\" rawentries=\"[]\" titlesonly=\"False\"/></compound><compound classes=\"toctree-wrapper\"><toctree caption=\"Optimizing\" entries=\"[(None, 'optimizing/basic_strategies/basic_strategies'), (None, 'optimizing/advanced_retrieval/advanced_retrieval'), (None, 'optimizing/agentic_strategies/agentic_strategies'), (None, 'optimizing/evaluation/evaluation'), (None, 'optimizing/fine-tuning/fine-tuning'), (None, 'optimizing/production_rag'), (None, 'optimizing/building_rag_from_scratch')]\" glob=\"False\" hidden=\"True\" includefiles=\"['optimizing/basic_strategies/basic_strategies', 'optimizing/advanced_retrieval/advanced_retrieval', 'optimizing/agentic_strategies/agentic_strategies', 'optimizing/evaluation/evaluation', 'optimizing/fine-tuning/fine-tuning', 'optimizing/production_rag', 'optimizing/building_rag_from_scratch']\" includehidden=\"False\" maxdepth=\"2\" numbered=\"0\" parent=\"index\" rawcaption=\"Optimizing\" rawentries=\"[]\" titlesonly=\"False\"/></compound><compound classes=\"toctree-wrapper\"><toctree caption=\"Module Guides\" entries=\"[(None, 'module_guides/models/models'), (None, 'module_guides/models/prompts'), (None, 'module_guides/loading/loading'), (None, 'module_guides/indexing/indexing'), (None, 'module_guides/storing/storing'), (None, 'module_guides/querying/querying'), (None, 'module_guides/observability/observability'), (None, 'module_guides/evaluating/root'), (None, 'module_guides/supporting_modules/supporting_modules')]\" glob=\"False\" hidden=\"True\" includefiles=\"['module_guides/models/models', 'module_guides/models/prompts', 'module_guides/loading/loading', 'module_guides/indexing/indexing', 'module_guides/storing/storing', 'module_guides/querying/querying', 'module_guides/observability/observability', 'module_guides/evaluating/root', 'module_guides/supporting_modules/supporting_modules']\" includehidden=\"False\" maxdepth=\"2\" numbered=\"0\" parent=\"index\" rawcaption=\"Module Guides\" rawentries=\"[]\" titlesonly=\"False\"/></compound><compound classes=\"toctree-wrapper\"><toctree caption=\"API Reference\" entries=\"[(None, 'api_reference/index')]\" glob=\"False\" hidden=\"True\" includefiles=\"['api_reference/index']\" includehidden=\"False\" maxdepth=\"1\" numbered=\"0\" parent=\"index\" rawcaption=\"API Reference\" rawentries=\"[]\" titlesonly=\"False\"/></compound><compound classes=\"toctree-wrapper\"><toctree caption=\"Community\" entries=\"[(None, 'community/integrations'), (None, 'community/frequently_asked_questions'), (None, 'community/full_stack_projects')]\" glob=\"False\" hidden=\"True\" includefiles=\"['community/integrations', 'community/frequently_asked_questions', 'community/full_stack_projects']\" includehidden=\"False\" maxdepth=\"2\" numbered=\"0\" parent=\"index\" rawcaption=\"Community\" rawentries=\"[]\" titlesonly=\"False\"/></compound><compound classes=\"toctree-wrapper\"><toctree caption=\"Contributing\" entries=\"[(None, 'contributing/contributing'), (None, 'contributing/documentation')]\" glob=\"False\" hidden=\"True\" includefiles=\"['contributing/contributing', 'contributing/documentation']\" includehidden=\"False\" maxdepth=\"2\" numbered=\"0\" parent=\"index\" rawcaption=\"Contributing\" rawentries=\"[]\" titlesonly=\"False\"/></compound><compound classes=\"toctree-wrapper\"><toctree caption=\"Changes\" entries=\"[(None, 'changes/changelog'), (None, 'changes/deprecated_terms')]\" glob=\"False\" hidden=\"True\" includefiles=\"['changes/changelog', 'changes/deprecated_terms']\" includehidden=\"False\" maxdepth=\"2\" numbered=\"0\" parent=\"index\" rawcaption=\"Changes\" rawentries=\"[]\" titlesonly=\"False\"/></compound></section></section></section>\n",
      "\n",
      "Examples for <class 'docutils.nodes.target'>:\n",
      "<target ids=\"['llm']\" names=\"['llm']\" refuri=\"https://en.wikipedia.org/wiki/Large_language_model\"/>\n",
      "\n",
      "Examples for <class 'sphinx.addnodes.desc_signature'>:\n",
      "<desc_signature _toc_name=\"CohereRerankerFinetuneEngine\" _toc_parts=\"('llama_index.finetuning', 'CohereRerankerFinetuneEngine')\" class=\"\" classes=\"sig sig-object\" fullname=\"CohereRerankerFinetuneEngine\" ids=\"llama_index.finetuning.CohereRerankerFinetuneEngine\" module=\"llama_index.finetuning\"><desc_annotation xml:space=\"preserve\">class<desc_sig_space classes=\"w\"> </desc_sig_space></desc_annotation><desc_addname classes=\"sig-prename descclassname\" xml:space=\"preserve\">llama_index.finetuning.</desc_addname><desc_name classes=\"sig-name descname\" xml:space=\"preserve\">CohereRerankerFinetuneEngine</desc_name><desc_parameterlist xml:space=\"preserve\"><desc_parameter xml:space=\"preserve\"><desc_sig_name classes=\"n\">train_file_name</desc_sig_name><desc_sig_punctuation classes=\"p\">:</desc_sig_punctuation><desc_sig_space classes=\"w\"> </desc_sig_space><desc_sig_name classes=\"n\"><pending_xref py:class=\"True\" py:module=\"llama_index.finetuning\" refdomain=\"py\" refspecific=\"False\" reftarget=\"str\" reftype=\"class\">str</pending_xref></desc_sig_name><desc_sig_space classes=\"w\"> </desc_sig_space><desc_sig_operator classes=\"o\">=</desc_sig_operator><desc_sig_space classes=\"w\"> </desc_sig_space><inline classes=\"default_value\" support_smartquotes=\"False\">'train.jsonl'</inline></desc_parameter><desc_parameter xml:space=\"preserve\"><desc_sig_name classes=\"n\">val_file_name</desc_sig_name><desc_sig_punctuation classes=\"p\">:</desc_sig_punctuation><desc_sig_space classes=\"w\"> </desc_sig_space><desc_sig_name classes=\"n\"><pending_xref py:class=\"True\" py:module=\"llama_index.finetuning\" refdomain=\"py\" refspecific=\"False\" reftarget=\"typing.Optional\" reftype=\"obj\">Optional</pending_xref><desc_sig_punctuation classes=\"p\">[</desc_sig_punctuation><pending_xref py:class=\"True\" py:module=\"llama_index.finetuning\" refdomain=\"py\" refspecific=\"False\" reftarget=\"str\" reftype=\"class\">str</pending_xref><desc_sig_punctuation classes=\"p\">]</desc_sig_punctuation></desc_sig_name><desc_sig_space classes=\"w\"> </desc_sig_space><desc_sig_operator classes=\"o\">=</desc_sig_operator><desc_sig_space classes=\"w\"> </desc_sig_space><inline classes=\"default_value\" support_smartquotes=\"False\">None</inline></desc_parameter><desc_parameter xml:space=\"preserve\"><desc_sig_name classes=\"n\">model_name</desc_sig_name><desc_sig_punctuation classes=\"p\">:</desc_sig_punctuation><desc_sig_space classes=\"w\"> </desc_sig_space><desc_sig_name classes=\"n\"><pending_xref py:class=\"True\" py:module=\"llama_index.finetuning\" refdomain=\"py\" refspecific=\"False\" reftarget=\"str\" reftype=\"class\">str</pending_xref></desc_sig_name><desc_sig_space classes=\"w\"> </desc_sig_space><desc_sig_operator classes=\"o\">=</desc_sig_operator><desc_sig_space classes=\"w\"> </desc_sig_space><inline classes=\"default_value\" support_smartquotes=\"False\">'exp_finetune'</inline></desc_parameter><desc_parameter xml:space=\"preserve\"><desc_sig_name classes=\"n\">model_type</desc_sig_name><desc_sig_punctuation classes=\"p\">:</desc_sig_punctuation><desc_sig_space classes=\"w\"> </desc_sig_space><desc_sig_name classes=\"n\"><pending_xref py:class=\"True\" py:module=\"llama_index.finetuning\" refdomain=\"py\" refspecific=\"False\" reftarget=\"str\" reftype=\"class\">str</pending_xref></desc_sig_name><desc_sig_space classes=\"w\"> </desc_sig_space><desc_sig_operator classes=\"o\">=</desc_sig_operator><desc_sig_space classes=\"w\"> </desc_sig_space><inline classes=\"default_value\" support_smartquotes=\"False\">'RERANK'</inline></desc_parameter><desc_parameter xml:space=\"preserve\"><desc_sig_name classes=\"n\">base_model</desc_sig_name><desc_sig_punctuation classes=\"p\">:</desc_sig_punctuation><desc_sig_space classes=\"w\"> </desc_sig_space><desc_sig_name classes=\"n\"><pending_xref py:class=\"True\" py:module=\"llama_index.finetuning\" refdomain=\"py\" refspecific=\"False\" reftarget=\"str\" reftype=\"class\">str</pending_xref></desc_sig_name><desc_sig_space classes=\"w\"> </desc_sig_space><desc_sig_operator classes=\"o\">=</desc_sig_operator><desc_sig_space classes=\"w\"> </desc_sig_space><inline classes=\"default_value\" support_smartquotes=\"False\">'english'</inline></desc_parameter><desc_parameter xml:space=\"preserve\"><desc_sig_name classes=\"n\">api_key</desc_sig_name><desc_sig_punctuation classes=\"p\">:</desc_sig_punctuation><desc_sig_space classes=\"w\"> </desc_sig_space><desc_sig_name classes=\"n\"><pending_xref py:class=\"True\" py:module=\"llama_index.finetuning\" refdomain=\"py\" refspecific=\"False\" reftarget=\"typing.Optional\" reftype=\"obj\">Optional</pending_xref><desc_sig_punctuation classes=\"p\">[</desc_sig_punctuation><pending_xref py:class=\"True\" py:module=\"llama_index.finetuning\" refdomain=\"py\" refspecific=\"False\" reftarget=\"str\" reftype=\"class\">str</pending_xref><desc_sig_punctuation classes=\"p\">]</desc_sig_punctuation></desc_sig_name><desc_sig_space classes=\"w\"> </desc_sig_space><desc_sig_operator classes=\"o\">=</desc_sig_operator><desc_sig_space classes=\"w\"> </desc_sig_space><inline classes=\"default_value\" support_smartquotes=\"False\">None</inline></desc_parameter></desc_parameterlist></desc_signature>\n",
      "\n",
      "Examples for <class 'docutils.nodes.system_message'>:\n",
      "<system_message backrefs=\"id2\" ids=\"id1\" level=\"3\" line=\"1\" source=\"/Users/sasha/github/LlamaIndex/llama_index/llama_index/vector_stores/postgres.py:docstring of llama_index.vector_stores.postgres.PGVectorStore\" type=\"ERROR\"><paragraph>Unknown interpreted text role ‚Äúparamref‚Äù.</paragraph></system_message>\n",
      "\n",
      "Examples for <class 'docutils.nodes.problematic'>:\n",
      "<problematic ids=\"id2\" refid=\"id1\">:paramref:`_sql.Select.with_only_columns.maintain_column_froms`</problematic>\n"
     ]
    }
   ],
   "source": [
    "# sphinx.addnodes.document\n",
    "# links are: docutils.nodes.target, section is docutils.nodes.section \n",
    "# ids is linked content, target and sections\n",
    "\n",
    "# Print all the children types of all the doctrees ids, and store in a dict\n",
    "import glob\n",
    "\n",
    "unique_types = set()\n",
    "doctree_files = glob.glob('/Users/sasha/github/LlamaIndex/llama_index/docs/_build/doctrees/**/*.doctree', recursive=True)\n",
    "doctree_files += glob.glob('/Users/sasha/github/LlamaIndex/llama_index/docs/_build/doctrees/*.doctree')\n",
    "\n",
    "type_examples = {}\n",
    "\n",
    "for file_path in doctree_files:\n",
    "    doctree = read_doctree(file_path)\n",
    "    for id in doctree.ids:\n",
    "        type_id = type(doctree.ids[id])\n",
    "        unique_types.add(type_id)\n",
    "        if type_id not in type_examples:\n",
    "            type_examples[type_id] = []\n",
    "        type_examples[type_id].append(doctree.ids[id])\n",
    "\n",
    "print(unique_types)\n",
    "for type_id, examples in type_examples.items():\n",
    "    print(f\"\\nExamples for {type_id}:\")\n",
    "    for example in examples:\n",
    "        print(example)\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discovered: these are different types of values for ids:\n",
    "- docutils.nodes.section,\n",
    "- docutils.nodes.target,\n",
    "- sphinx.addnodes.desc_signature,\n",
    "- docutils.nodes.system_message,\n",
    "- docutils.nodes.problematic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure\n",
      "{'rawsource': '', 'children': [<title: <#text: 'Welcome to Lla ...'>>, <paragraph: <#text: 'LlamaIndex is  ...'><reference...><target \"llm\" ...>, <section \"üöÄ why llamaindex?\": <title...><paragraph...><paragraph...><paragraph...><bul ...>, <section \"ü¶ô how can llamaindex help?\": <title...><paragraph...><bullet_list...>>, <section \"üë®‚Äçüë©‚Äçüëß‚Äçüë¶ who is llamaindex for?\": <title...><paragraph...><paragraph...><paragraph...>>, <section \"getting started\": <title...><paragraph...><paragraph...><paragraph...>>, <section \"üó∫Ô∏è ecosystem\": <title...><paragraph...><bullet_list...><section \"commun ...>], 'attributes': {'ids': ['welcome-to-llamaindex'], 'classes': [], 'names': ['welcome to llamaindex ü¶ô !'], 'dupnames': [], 'backrefs': []}, 'tagname': 'section', 'parent': <document: <section \"welcome to llamaindex ü¶ô !\"...>>, 'document': <document: <section \"welcome to llamaindex ü¶ô !\"...>>, 'source': '/Users/sasha/github/LlamaIndex/llama_index/docs/index.rst', 'line': 2}\n",
      "dict_keys(['rawsource', 'children', 'attributes', 'tagname', 'parent', 'document', 'source', 'line'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<title: <#text: 'Welcome to Lla ...'>>,\n",
       " <paragraph: <#text: 'LlamaIndex is  ...'><reference...><target \"llm\" ...>,\n",
       " <section \"üöÄ why llamaindex?\": <title...><paragraph...><paragraph...><paragraph...><bul ...>,\n",
       " <section \"ü¶ô how can llamaindex help?\": <title...><paragraph...><bullet_list...>>,\n",
       " <section \"üë®‚Äçüë©‚Äçüëß‚Äçüë¶ who is llamaindex for?\": <title...><paragraph...><paragraph...><paragraph...>>,\n",
       " <section \"getting started\": <title...><paragraph...><paragraph...><paragraph...>>,\n",
       " <section \"üó∫Ô∏è ecosystem\": <title...><paragraph...><bullet_list...><section \"commun ...>]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_index = 0\n",
    "target_index = 1\n",
    "desc_index = 2\n",
    "system_message_index = 3\n",
    "problematic_index = 4\n",
    "all_sections = list(type_examples.values())[section_index]\n",
    "print(f\"Structure\")\n",
    "print(all_sections[0].__dict__)\n",
    "print(all_sections[0].__dict__.keys())\n",
    "all_sections[0].children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_children_type = collections.defaultdict(list)\n",
    "for section in all_sections:\n",
    "    for children in section.children:\n",
    "        section_children_type[type(children)].append(children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([<class 'docutils.nodes.title'>, <class 'docutils.nodes.paragraph'>, <class 'docutils.nodes.section'>, <class 'docutils.nodes.bullet_list'>, <class 'docutils.nodes.enumerated_list'>, <class 'docutils.nodes.compound'>, <class 'docutils.nodes.transition'>, <class 'docutils.nodes.reference'>, <class 'docutils.nodes.comment'>, <class 'docutils.nodes.block_quote'>, <class 'docutils.nodes.literal_block'>, <class 'docutils.nodes.tip'>, <class 'docutils.nodes.target'>, <class 'docutils.nodes.container'>, <class 'sphinx.addnodes.index'>, <class 'sphinx.addnodes.desc'>, <class 'sphinx.addnodes.tabular_col_spec'>, <class 'sphinx.ext.autosummary.autosummary_table'>, <class 'sphinx.ext.autosummary.autosummary_toc'>, <class 'docutils.nodes.line_block'>, <class 'docutils.nodes.raw'>, <class 'docutils.nodes.table'>, <class 'docutils.nodes.warning'>, <class 'docutils.nodes.admonition'>])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_children_type.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: <class 'docutils.nodes.title'>\n",
      "First Value Dict: {'rawsource': 'Welcome to LlamaIndex ü¶ô !', 'children': [<#text: 'Welcome to LlamaIndex ü¶ô !'>], 'attributes': {'ids': [], 'classes': [], 'names': [], 'dupnames': [], 'backrefs': []}, 'tagname': 'title', 'parent': <section \"welcome to llamaindex ü¶ô !\": <title...><paragraph...><section \"üöÄ why llamaindex?\"...> ...>, 'document': <document: <section \"welcome to llamaindex ü¶ô !\"...>>, 'source': '/Users/sasha/github/LlamaIndex/llama_index/docs/index.rst', 'line': 2}\n"
     ]
    }
   ],
   "source": [
    "for key, values in section_children_type.items():\n",
    "    print(f\"Key: {key}\")\n",
    "    print(f\"First Value Dict: {values[0].__dict__}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: Inspecting the section is interesting, but I think what benefit me more now is to look at the structure of 1 document hollistically, which I will experiment in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Holistically look at the doctree for index.doctree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<title: <#text: 'Community'>>,\n",
       " <paragraph: <#text: 'Need help? Hav ...'>>,\n",
       " <bullet_list: <list_item...><list_item...>>]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doctree.ids[\"ecosystem\"].children[3].children\n",
    "\n",
    "# ecosystem is a section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: this informs me that I can build a parser that auto populates the content for each documents to store/index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Langchain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "markdown_path = \"/Users/sasha/github/LlamaIndex/llama_index/docs/index.rst\"\n",
    "loader = UnstructuredMarkdownLoader(markdown_path)\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Welcome to LlamaIndex ü¶ô !\\n\\nLlamaIndex is a data framework for LLM <https://en.wikipedia.org/wiki/Large_language_model>-based applications to ingest, structure, and access private or domain-specific data. It\\'s available in Python (these docs) and Typescript <https://ts.llamaindex.ai/>.\\n\\nüöÄ Why LlamaIndex?\\n\\nLLMs offer a natural language interface between humans and data. Widely available models come pre-trained on huge amounts of publicly available data like Wikipedia, mailing lists, textbooks, source code and more.\\n\\nHowever, while LLMs are trained on a great deal of data, they are not trained on your data, which may be private or specific to the problem you\\'re trying to solve. It\\'s behind APIs, in SQL databases, or trapped in PDFs and slide decks.\\n\\nYou may choose to fine-tune a LLM with your data, but:\\n\\nTraining a LLM is expensive.\\n\\nDue to the cost to train, it\\'s hard to update a LLM with latest information.\\n\\nObservability is lacking. When you ask a LLM a question, it\\'s not obvious how the LLM arrived at its answer.\\n\\nLlamaIndex takes a different approach called Retrieval-Augmented Generation (RAG) <./getting_started/concepts.html>_. Instead of asking LLM to generate an answer immediately, LlamaIndex:\\n\\nretrieves information from your data sources first,\\n\\nadds it to your question as context, and\\n\\nasks the LLM to answer based on the enriched prompt.\\n\\nRAG overcomes all three weaknesses of the fine-tuning approach:\\n\\nThere\\'s no training involved, so it\\'s cheap.\\n\\nData is fetched only when you ask for them, so it\\'s always up to date.\\n\\nLlamaIndex can show you the retrieved documents, so it\\'s more trustworthy.\\n\\nLlamaIndex imposes no restriction on how you use LLMs. You can still use LLMs as auto-complete, chatbots, semi-autonomous agents, and more (see Use Cases on the left). It only makes LLMs more relevant to you.\\n\\nü¶ô How can LlamaIndex help?\\n\\nLlamaIndex provides the following tools:\\n\\nData connectors ingest your existing data from their native source and format. These could be APIs, PDFs, SQL, and (much) more.\\n\\nData indexes structure your data in intermediate representations that are easy and performant for LLMs to consume.\\n\\nEngines provide natural language access to your data. For example:\\n\\nQuery engines are powerful retrieval interfaces for knowledge-augmented output.\\n\\nChat engines are conversational interfaces for multi-message, \"back and forth\" interactions with your data.\\n\\nData agents are LLM-powered knowledge workers augmented by tools, from simple helper functions to API integrations and more.\\n\\nApplication integrations tie LlamaIndex back into the rest of your ecosystem. This could be LangChain, Flask, Docker, ChatGPT, or‚Ä¶ anything else!\\n\\nüë®\\u200düë©\\u200düëß\\u200düë¶ Who is LlamaIndex for?\\n\\nLlamaIndex provides tools for beginners, advanced users, and everyone in between.\\n\\nOur high-level API allows beginner users to use LlamaIndex to ingest and query their data in 5 lines of code.\\n\\nFor more complex applications, our lower-level APIs allow advanced users to customize and extend any module‚Äîdata connectors, indices, retrievers, query engines, reranking modules‚Äîto fit their needs.\\n\\nGetting Started\\n\\nTo install the library:\\n\\npip install llama-index\\n\\nWe recommend starting at how to read these docs <./getting_started/reading.html>_, which will point you to the right place based on your experience level.\\n\\nüó∫Ô∏è Ecosystem\\n\\nTo download or contribute, find LlamaIndex on:\\n\\nGithub: https://github.com/jerryjliu/llama_index\\n\\nPyPi:\\n\\nLlamaIndex: https://pypi.org/project/llama-index/.\\n\\nGPT Index (duplicate): https://pypi.org/project/gpt-index/.\\n\\nNPM (Typescript/Javascript):\\n\\nGithub: https://github.com/run-llama/LlamaIndexTS\\n\\nDocs: https://ts.llamaindex.ai/\\n\\nLlamaIndex.TS: https://www.npmjs.com/package/llamaindex\\n\\nCommunity\\n\\nNeed help? Have a feature suggestion? Join the LlamaIndex community:\\n\\nTwitter: https://twitter.com/llama_index\\n\\nDiscord https://discord.gg/dGcwcsnxhU\\n\\nAssociated projects\\n\\nüè° LlamaHub: https://llamahub.ai | A large (and growing!) collection of custom data connectors\\n\\nüß™ LlamaLab: https://github.com/run-llama/llama-lab | Ambitious projects built on top of LlamaIndex\\n\\n.. toctree::\\n   :maxdepth: 1\\n   :caption: Getting Started\\n   :hidden:\\n\\ngetting_started/installation.md\\n   getting_started/reading.md\\n   getting_started/starter_example.md\\n   getting_started/concepts.md\\n   getting_started/customization.rst\\n   getting_started/discover_llamaindex.md\\n\\n.. toctree::\\n   :maxdepth: 2\\n   :caption: Use Cases\\n   :hidden:\\n\\nuse_cases/q_and_a.md\\n   use_cases/chatbots.md\\n   use_cases/agents.md\\n   use_cases/extraction.md\\n   use_cases/multimodal.md\\n\\n.. toctree::\\n   :maxdepth: 2\\n   :caption: Understanding\\n   :hidden:\\n\\nunderstanding/understanding.md\\n   understanding/using_llms/using_llms.md\\n   understanding/loading/loading.md\\n   understanding/indexing/indexing.md\\n   understanding/storing/storing.md\\n   understanding/querying/querying.md\\n   understanding/putting_it_all_together/putting_it_all_together.md\\n   understanding/tracing_and_debugging/tracing_and_debugging.md\\n   understanding/evaluating/evaluating.md\\n\\n.. toctree::\\n   :maxdepth: 2\\n   :caption: Optimizing\\n   :hidden:\\n\\noptimizing/basic_strategies/basic_strategies.md\\n   optimizing/advanced_retrieval/advanced_retrieval.md\\n   optimizing/agentic_strategies/agentic_strategies.md\\n   optimizing/evaluation/evaluation.md\\n   optimizing/fine-tuning/fine-tuning.md\\n   optimizing/production_rag.md\\n   optimizing/building_rag_from_scratch.md\\n.. toctree::\\n   :maxdepth: 2\\n   :caption: Module Guides\\n   :hidden:\\n\\nmodule_guides/models/models.md\\n   module_guides/models/prompts.md\\n   module_guides/loading/loading.md\\n   module_guides/indexing/indexing.md\\n   module_guides/storing/storing.md\\n   module_guides/querying/querying.md\\n   module_guides/observability/observability.md\\n   module_guides/evaluating/root.md\\n   module_guides/supporting_modules/supporting_modules.md\\n\\n.. toctree::\\n   :maxdepth: 1\\n   :caption: API Reference\\n   :hidden:\\n\\napi_reference/index.rst\\n\\n.. toctree::\\n   :maxdepth: 2\\n   :caption: Community\\n   :hidden:\\n\\ncommunity/integrations.md\\n   community/frequently_asked_questions.md\\n   community/full_stack_projects.md\\n\\n.. toctree::\\n   :maxdepth: 2\\n   :caption: Contributing\\n   :hidden:\\n\\ncontributing/contributing.rst\\n   contributing/documentation.rst\\n\\n.. toctree::\\n   :maxdepth: 2\\n   :caption: Changes\\n   :hidden:\\n\\nchanges/changelog.rst\\n   changes/deprecated_terms.md'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].__dict__[\"page_content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this does okay, but it did not capture the fact that \"use_cases/agents.md\" etc are links "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Unstructured?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observe index.rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.NarrativeText at 0x1bc36e190>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1bc3dc810>,\n",
       " <unstructured.documents.elements.Title at 0x1b82da3d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1b857ce90>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x179965d90>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1bba39f50>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bba3abd0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb9828d0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb981350>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1bb9822d0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb982650>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb982910>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb982290>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1bb982d50>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb981050>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb982750>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb9812d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1bb982c10>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1bb9836d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1bb980b50>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb981110>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb981d50>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb981fd0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb983450>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb9831d0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb982fd0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb983050>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1bb982e90>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1bb982ad0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1bb9838d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1bb983510>,\n",
       " <unstructured.documents.elements.Title at 0x1bb980090>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1bb981ad0>,\n",
       " <unstructured.documents.elements.Title at 0x1bb9813d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1bb981910>,\n",
       " <unstructured.documents.elements.Title at 0x1b84fae50>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x177803950>,\n",
       " <unstructured.documents.elements.ListItem at 0x1b85dded0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1b868ed50>,\n",
       " <unstructured.documents.elements.ListItem at 0x179a41bd0>,\n",
       " <unstructured.documents.elements.ListItem at 0x179cc2750>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bc340690>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bc3406d0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bc3404d0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bc340550>,\n",
       " <unstructured.documents.elements.Title at 0x1bc340350>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1bc340190>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bc3402d0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1b85bcf50>,\n",
       " <unstructured.documents.elements.Title at 0x1bbdaac90>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb97f250>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb97fbd0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1bb97fcd0>,\n",
       " <unstructured.documents.elements.Text at 0x1bb97ff90>,\n",
       " <unstructured.documents.elements.Text at 0x1bb97fd90>,\n",
       " <unstructured.documents.elements.Text at 0x1bb97f0d0>,\n",
       " <unstructured.documents.elements.Text at 0x1bb97f3d0>,\n",
       " <unstructured.documents.elements.Text at 0x1bb97f8d0>,\n",
       " <unstructured.documents.elements.Text at 0x1bb97eed0>,\n",
       " <unstructured.documents.elements.Text at 0x1bb97efd0>,\n",
       " <unstructured.documents.elements.Text at 0x1bb97ef50>,\n",
       " <unstructured.documents.elements.Text at 0x1bb97f4d0>,\n",
       " <unstructured.documents.elements.Title at 0x1bb97f610>,\n",
       " <unstructured.documents.elements.Text at 0x1b856eb10>,\n",
       " <unstructured.documents.elements.Title at 0x1b8666210>,\n",
       " <unstructured.documents.elements.Text at 0x1bc3dca10>,\n",
       " <unstructured.documents.elements.Title at 0x1bc3dca90>,\n",
       " <unstructured.documents.elements.Text at 0x1bc3dcc10>,\n",
       " <unstructured.documents.elements.Title at 0x1bc3dcd50>]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unstructured.partition.md import partition_md\n",
    "elements = partition_md(filename=\"/Users/sasha/github/LlamaIndex/llama_index/docs/index.rst\")\n",
    "elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'.. toctree::\\n   :maxdepth: 1\\n   :caption: Getting Started\\n   :hidden:'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(elements))\n",
    "elements[52].text # everything after 52 for index.rst is structure, observe another random file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observe another random md file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.Title at 0x162565f10>,\n",
       " <unstructured.documents.elements.Title at 0x16255c7d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x16258c710>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x162566d10>,\n",
       " <unstructured.documents.elements.Title at 0x16258cc90>,\n",
       " <unstructured.documents.elements.Title at 0x16258cf10>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x16258d010>,\n",
       " <unstructured.documents.elements.Title at 0x16258d110>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x16258d210>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x16258d2d0>,\n",
       " <unstructured.documents.elements.Title at 0x16258d390>,\n",
       " <unstructured.documents.elements.Title at 0x16258d490>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x16258d590>,\n",
       " <unstructured.documents.elements.Title at 0x16258d690>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x16258d790>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x16258d890>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x16258d990>,\n",
       " <unstructured.documents.elements.ListItem at 0x16258dad0>,\n",
       " <unstructured.documents.elements.Text at 0x16258dc10>,\n",
       " <unstructured.documents.elements.ListItem at 0x16258dc90>,\n",
       " <unstructured.documents.elements.Title at 0x16258ddd0>,\n",
       " <unstructured.documents.elements.ListItem at 0x16258ded0>,\n",
       " <unstructured.documents.elements.Title at 0x16258dfd0>,\n",
       " <unstructured.documents.elements.Title at 0x16258e0d0>,\n",
       " <unstructured.documents.elements.Title at 0x16258e1d0>,\n",
       " <unstructured.documents.elements.Title at 0x16258e2d0>,\n",
       " <unstructured.documents.elements.Title at 0x16258e3d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x16258e4d0>,\n",
       " <unstructured.documents.elements.Title at 0x16258e5d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x16258e6d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x16258e7d0>,\n",
       " <unstructured.documents.elements.Title at 0x16258e8d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x16258e9d0>,\n",
       " <unstructured.documents.elements.Title at 0x16258ead0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x16258ebd0>,\n",
       " <unstructured.documents.elements.Title at 0x16258ecd0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x16258edd0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x16258eed0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x16258efd0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x16258f0d0>,\n",
       " <unstructured.documents.elements.Title at 0x16258f1d0>,\n",
       " <unstructured.documents.elements.Title at 0x16258f2d0>,\n",
       " <unstructured.documents.elements.Title at 0x16258f3d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x16258f4d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x16258f5d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x16258f6d0>,\n",
       " <unstructured.documents.elements.Title at 0x16258f7d0>,\n",
       " <unstructured.documents.elements.Title at 0x16258f8d0>,\n",
       " <unstructured.documents.elements.Title at 0x16258f9d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x16258fb10>,\n",
       " <unstructured.documents.elements.ListItem at 0x16258fc10>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x16258fd10>,\n",
       " <unstructured.documents.elements.ListItem at 0x16258fe10>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x16258ff10>,\n",
       " <unstructured.documents.elements.ListItem at 0x1625a0090>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1625a0190>,\n",
       " <unstructured.documents.elements.Title at 0x1625a0290>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1625a0390>,\n",
       " <unstructured.documents.elements.Title at 0x1625a0490>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1625a0590>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1625a0690>,\n",
       " <unstructured.documents.elements.Title at 0x1625a0790>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1625a0890>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1625a0990>,\n",
       " <unstructured.documents.elements.Title at 0x1625a0a90>,\n",
       " <unstructured.documents.elements.Title at 0x1625a0b90>,\n",
       " <unstructured.documents.elements.Title at 0x1625a0c90>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unstructured.partition.md import partition_md\n",
    "elements = partition_md(filename=\"/Users/sasha/github/LlamaIndex/llama_index/docs/module_guides/loading/documents_and_nodes/usage_documents.md\")\n",
    "elements "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "<class 'unstructured.documents.elements.Text'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'python\\ndocument = Document(\\n    text=\"text\",\\n    metadata={\"filename\": \"<doc_file_name>\", \"category\": \"<category>\"},\\n)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(elements))\n",
    "index = 18 # 61\n",
    "print(type(elements[index]))\n",
    "elements[index].text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspecting links (this maybe a deal breaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'unstructured.documents.elements.NarrativeText'>\n",
      "By default, all of our data loaders (including those offered on LlamaHub) return Document objects through the load_data function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'last_modified': '2024-01-08T15:19:23',\n",
       " '_known_field_names': frozenset({'attached_to_filename',\n",
       "            'category_depth',\n",
       "            'coordinates',\n",
       "            'data_source',\n",
       "            'detection_class_prob',\n",
       "            'detection_origin',\n",
       "            'emphasized_text_contents',\n",
       "            'emphasized_text_tags',\n",
       "            'file_directory',\n",
       "            'filename',\n",
       "            'filetype',\n",
       "            'header_footer_type',\n",
       "            'image_base64',\n",
       "            'image_mime_type',\n",
       "            'image_path',\n",
       "            'is_continuation',\n",
       "            'languages',\n",
       "            'last_modified',\n",
       "            'link_texts',\n",
       "            'link_urls',\n",
       "            'links',\n",
       "            'page_name',\n",
       "            'page_number',\n",
       "            'parent_id',\n",
       "            'regex_metadata',\n",
       "            'section',\n",
       "            'sent_from',\n",
       "            'sent_to',\n",
       "            'subject',\n",
       "            'text_as_html',\n",
       "            'url'}),\n",
       " 'page_number': 1,\n",
       " 'languages': ['eng'],\n",
       " 'parent_id': '6edae678f82d241a17e6eb7bb78f3b86',\n",
       " 'filetype': 'text/markdown',\n",
       " 'file_directory': '/Users/sasha/github/LlamaIndex/llama_index/docs/module_guides/loading/documents_and_nodes',\n",
       " 'filename': 'usage_documents.md'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 3\n",
    "print(type(elements[index]))\n",
    "print(elements[index].text) # note this is supposed to have a link under \"dataloader\"\n",
    "elements[index].metadata.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: its not super clear the difference between narrativeText with Text, and also links are not captured in markdown with unstructured... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding how the sidebar structure is determined, in the root.md file, pay attention to toctree structure's includefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<document: <section \"querying\"...>>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace 'path_to_doctree_file.doctree' with your .doctree file path\n",
    "# doctree = read_doctree('/Users/sasha/github/LlamaIndex/llama_index/docs/_build/doctrees/index.doctree')\n",
    "# agentic_strategies_doctree = read_doctree('/Users/sasha/github/LlamaIndex/llama_index/docs/_build/doctrees/optimizing/advanced_retrieval/advanced_retrieval.doctree')\n",
    "querying_doctree = read_doctree('/Users/sasha/github/LlamaIndex/llama_index/docs/_build/doctrees/module_guides/querying/querying.doctree')\n",
    "# print(doctree)\n",
    "# print(agentic_strategies_doctree)\n",
    "querying_doctree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rawsource': '',\n",
       " 'children': [],\n",
       " 'attributes': {'ids': [],\n",
       "  'classes': [],\n",
       "  'names': [],\n",
       "  'dupnames': [],\n",
       "  'backrefs': [],\n",
       "  'parent': 'module_guides/querying/querying',\n",
       "  'entries': [(None, 'module_guides/deploying/query_engine/root'),\n",
       "   (None, 'module_guides/deploying/chat_engines/root'),\n",
       "   (None, 'module_guides/deploying/agents/root'),\n",
       "   (None, 'module_guides/querying/retriever/root'),\n",
       "   (None, 'module_guides/querying/response_synthesizers/root'),\n",
       "   (None, 'module_guides/querying/router/root'),\n",
       "   (None, 'module_guides/querying/node_postprocessors/root'),\n",
       "   (None, 'module_guides/querying/structured_outputs/structured_outputs')],\n",
       "  'includefiles': ['module_guides/deploying/query_engine/root',\n",
       "   'module_guides/deploying/chat_engines/root',\n",
       "   'module_guides/deploying/agents/root',\n",
       "   'module_guides/querying/retriever/root',\n",
       "   'module_guides/querying/response_synthesizers/root',\n",
       "   'module_guides/querying/router/root',\n",
       "   'module_guides/querying/node_postprocessors/root',\n",
       "   'module_guides/querying/structured_outputs/structured_outputs'],\n",
       "  'maxdepth': 1,\n",
       "  'caption': None,\n",
       "  'glob': False,\n",
       "  'hidden': False,\n",
       "  'includehidden': False,\n",
       "  'numbered': 0,\n",
       "  'titlesonly': False,\n",
       "  'rawentries': []},\n",
       " 'tagname': 'toctree',\n",
       " 'source': '/Users/sasha/github/LlamaIndex/llama_index/docs/module_guides/querying/querying.md',\n",
       " 'line': 21,\n",
       " 'parent': <compound: <toctree...>>}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "querying_doctree.ids[\"query-modules\"].children[1].children[0].__dict__\n",
    "# interesting that <compound: <toctree...>>\n",
    "# in a given section basically denotes the sidebar content, this is the structure of toctree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if ipynb is included in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<document: <section \"agents\"...>>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_doctree = read_doctree('/Users/sasha/github/LlamaIndex/llama_index/docs/_build/doctrees/use_cases/agents.doctree')\n",
    "agent_doctree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rawsource': '',\n",
       " 'children': [],\n",
       " 'attributes': {'ids': [],\n",
       "  'classes': [],\n",
       "  'names': [],\n",
       "  'dupnames': [],\n",
       "  'backrefs': [],\n",
       "  'parent': 'use_cases/agents',\n",
       "  'entries': [('Agents (Putting your RAG Pipeline Together)',\n",
       "    'understanding/putting_it_all_together/agents'),\n",
       "   ('Agentic Strategies (Optimizing your RAG Pipeline)',\n",
       "    'optimizing/agentic_strategies/agentic_strategies')],\n",
       "  'includefiles': ['understanding/putting_it_all_together/agents',\n",
       "   'optimizing/agentic_strategies/agentic_strategies'],\n",
       "  'maxdepth': 1,\n",
       "  'caption': None,\n",
       "  'glob': False,\n",
       "  'hidden': False,\n",
       "  'includehidden': False,\n",
       "  'numbered': 0,\n",
       "  'titlesonly': False,\n",
       "  'rawentries': ['Agents (Putting your RAG Pipeline Together)',\n",
       "   'Agentic Strategies (Optimizing your RAG Pipeline)']},\n",
       " 'tagname': 'toctree',\n",
       " 'source': '/Users/sasha/github/LlamaIndex/llama_index/docs/use_cases/agents.md',\n",
       " 'line': 24,\n",
       " 'parent': <compound: <toctree...>>}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_doctree.children[0].children[5].children[2].children[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<document: <paragraph...><section \"context-augmented openai agent\"...>>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notebook_tree = read_doctree('/Users/sasha/github/LlamaIndex/llama_index/docs/_build/doctrees/examples/agent/openai_agent_context_retrieval.doctree')\n",
    "notebook_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rawsource': '!pip install llama-index',\n",
       " 'children': [<#text: '!pip install llama-index'>],\n",
       " 'attributes': {'ids': [],\n",
       "  'classes': [],\n",
       "  'names': [],\n",
       "  'dupnames': [],\n",
       "  'backrefs': [],\n",
       "  'language': 'ipython3',\n",
       "  'xml:space': 'preserve'},\n",
       " 'tagname': 'literal_block',\n",
       " 'source': '/Users/sasha/github/LlamaIndex/llama_index/docs/examples/agent/openai_agent_context_retrieval.ipynb',\n",
       " 'line': 70002,\n",
       " 'parent': <container: <literal_block...>>,\n",
       " 'document': <document: <paragraph...><section \"context-augmented openai agent\"...>>}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notebook_tree.children[1].children[2][3].children[0].children[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<paragraph: <raw...><raw...><raw...>>,\n",
       " <section \"context-augmented openai agent\": <title...><paragraph...><section \"initial setup\"...>>]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notebook_tree.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<title: <#text: 'Context-Augmen ...'>>,\n",
       " <paragraph: <#text: 'In this tutori ...'><literal...><#text: ' imple ...>,\n",
       " <section \"initial setup\": <title...><paragraph...><paragraph...><container...><con ...>]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notebook_tree.children[1].children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rawsource': '',\n",
       " 'children': [<title: <#text: 'Initial Setup'>>,\n",
       "  <paragraph: <#text: 'Here we setup  ...'>>,\n",
       "  <paragraph: <#text: 'If you‚Äôre open ...'>>,\n",
       "  <container: <container...>>,\n",
       "  <container: <container...>>,\n",
       "  <container: <container...>>,\n",
       "  <paragraph: <#text: 'Download Data'>>,\n",
       "  <container: <container...>>,\n",
       "  <container: <container...>>,\n",
       "  <container: <container...>>,\n",
       "  <container: <container...>>,\n",
       "  <section \"try context-augmented agent\": <title...><paragraph...><bullet_list...><container...><c ...>,\n",
       "  <section \"use uber 10-q as context, use calculator as tool\": <title...><container...><container...><container...><con ...>],\n",
       " 'attributes': {'ids': ['initial-setup'],\n",
       "  'classes': [],\n",
       "  'names': ['initial setup'],\n",
       "  'dupnames': [],\n",
       "  'backrefs': [],\n",
       "  'myst-anchor': 'examples/agent/openai_agent_context_retrieval.ipynb#initial-setup'},\n",
       " 'tagname': 'section',\n",
       " 'line': 40002,\n",
       " 'source': '/Users/sasha/github/LlamaIndex/llama_index/docs/examples/agent/openai_agent_context_retrieval.ipynb',\n",
       " 'parent': <section \"context-augmented openai agent\": <title...><paragraph...><section \"initial setup\"...>>,\n",
       " 'document': <document: <paragraph...><section \"context-augmented openai agent\"...>>}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notebook_tree.children[1].children[2].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rawsource': '',\n",
       " 'children': [<container: <literal_block...>>],\n",
       " 'attributes': {'ids': [],\n",
       "  'classes': ['cell'],\n",
       "  'names': [],\n",
       "  'dupnames': [],\n",
       "  'backrefs': [],\n",
       "  'nb_element': 'cell_code',\n",
       "  'cell_index': 6,\n",
       "  'exec_count': None,\n",
       "  'cell_metadata': {}},\n",
       " 'tagname': 'container',\n",
       " 'line': 70002,\n",
       " 'source': '/Users/sasha/github/LlamaIndex/llama_index/docs/examples/agent/openai_agent_context_retrieval.ipynb',\n",
       " 'parent': <section \"initial setup\": <title...><paragraph...><paragraph...><container...><con ...>,\n",
       " 'document': <document: <paragraph...><section \"context-augmented openai agent\"...>>}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each container block is code and within each container block, it has additional container\n",
    "notebook_tree.children[1].children[2][3].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rawsource': '', 'parent': <literal_block: <#text: '!pip install l ...'>>}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notebook_tree.children[1].children[2][3].children[0].children[0].children[0].__dict__ # , it has an additional container which leads to a literal block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion is that I should use sphinx as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab the structure using Sphinx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_doctree = read_doctree('/Users/sasha/github/LlamaIndex/llama_index/docs/_build/doctrees/index.doctree')\n",
    "\n",
    "# Question what is the difference between ids and children?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
