{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying LlamaIndex Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure out the best way to grab the doc structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Sphinx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<document source=\"/Users/sasha/github/LlamaIndex/llama_index/docs/getting_started/installation.md\"><section ids=\"installation-and-setup\" myst-anchor=\"getting_started/installation.md#installation-and-setup\" names=\"installation\\ and\\ setup\"><title>Installation and Setup</title><section ids=\"installation-from-pip\" myst-anchor=\"getting_started/installation.md#installation-from-pip\" names=\"installation\\ from\\ pip\"><title>Installation from Pip</title><paragraph>Install from pip:</paragraph><literal_block language=\"default\" xml:space=\"preserve\">pip install llama-index\n",
      "</literal_block><paragraph><strong>NOTE:</strong> LlamaIndex may download and store local files for various packages (NLTK, HuggingFace, ‚Ä¶). Use the environment variable ‚ÄúLLAMA_INDEX_CACHE_DIR‚Äù to control where these files are saved.</paragraph><paragraph>If you prefer to install from source, see below.</paragraph></section><section ids=\"important-openai-environment-setup\" myst-anchor=\"getting_started/installation.md#important-openai-environment-setup\" names=\"important:\\ openai\\ environment\\ setup\"><title>Important: OpenAI Environment Setup</title><paragraph>By default, we use the OpenAI <literal>gpt-3.5-turbo</literal> model for text generation and <literal>text-embedding-ada-002</literal> for retrieval and embeddings. In order to use this, you must have an OPENAI_API_KEY set up as an environment variable.\n",
      "You can obtain an API key by logging into your OpenAI account and <reference refuri=\"https://platform.openai.com/account/api-keys\">and creating a new API key</reference>.</paragraph><tip><paragraph>You can also <pending_xref refdoc=\"getting_started/installation\" refdomain=\"True\" refexplicit=\"True\" reftarget=\"/module_guides/models/llms/usage_custom.md\" reftype=\"myst\" refwarn=\"True\"><inline classes=\"xref myst\">use one of many other available LLMs</inline></pending_xref>. You may\n",
      "need additional environment keys + tokens setup depending on the LLM provider.</paragraph></tip></section><section ids=\"local-model-setup\" myst-anchor=\"getting_started/installation.md#local-model-setup\" names=\"local\\ model\\ setup\"><title>Local Model Setup</title><paragraph>If you don‚Äôt wish to use OpenAI, consider setting up a local LLM and embedding model in the service context.</paragraph><paragraph>A full guide to using and configuring LLMs available <pending_xref refdoc=\"getting_started/installation\" refdomain=\"True\" refexplicit=\"True\" reftarget=\"/module_guides/models/llms.md\" reftype=\"myst\" refwarn=\"True\"><inline classes=\"xref myst\">here</inline></pending_xref>.</paragraph><paragraph>A full guide to using and configuring embedding models is available <pending_xref refdoc=\"getting_started/installation\" refdomain=\"True\" refexplicit=\"True\" reftarget=\"/module_guides/models/embeddings.md\" reftype=\"myst\" refwarn=\"True\"><inline classes=\"xref myst\">here</inline></pending_xref>.</paragraph></section><section ids=\"installation-from-source\" myst-anchor=\"getting_started/installation.md#installation-from-source\" names=\"installation\\ from\\ source\"><title>Installation from Source</title><paragraph>Git clone this repository: <literal>git clone https://github.com/jerryjliu/llama_index.git</literal>. Then do the following:</paragraph><bullet_list bullet=\"-\"><list_item><paragraph><reference refuri=\"https://python-poetry.org/docs/#installation\">Install poetry</reference> - this will help you manage package dependencies</paragraph></list_item><list_item><paragraph><literal>poetry shell</literal> - this command creates a virtual environment, which keeps installed packages contained to this project</paragraph></list_item><list_item><paragraph><literal>poetry install</literal> - this will install the core package requirements</paragraph></list_item><list_item><paragraph>(Optional) <literal>poetry install --with dev,docs</literal> - this will install all dependencies needed for most local development</paragraph></list_item></bullet_list></section><section ids=\"optional-dependencies\" myst-anchor=\"getting_started/installation.md#optional-dependencies\" names=\"optional\\ dependencies\"><title>Optional Dependencies</title><paragraph>By default LlamaIndex installs a core set of dependencies; we also provide a convenient way to install commonly-required optional dependencies. These are currently in three sets:</paragraph><bullet_list bullet=\"-\"><list_item><paragraph><literal>pip install llama-index[local_models]</literal> installs tools useful for private LLMs, local inference, and HuggingFace models</paragraph></list_item><list_item><paragraph><literal>pip install llama-index[postgres]</literal> is useful if you are working with Postgres, PGVector or Supabase</paragraph></list_item><list_item><paragraph><literal>pip install llama-index[query_tools]</literal> gives you tools for hybrid search, structured outputs, and node post-processing</paragraph></list_item></bullet_list></section></section></document>\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def read_doctree(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "# Replace 'path_to_doctree_file.doctree' with your .doctree file path\n",
    "# doctree = read_doctree('/Users/sasha/github/LlamaIndex/llama_index/docs/_build/doctrees/index.doctree')\n",
    "# agentic_strategies_doctree = read_doctree('/Users/sasha/github/LlamaIndex/llama_index/docs/_build/doctrees/optimizing/advanced_retrieval/advanced_retrieval.doctree')\n",
    "installation_doctree = read_doctree('/Users/sasha/github/LlamaIndex/llama_index/docs/_build/doctrees/getting_started/installation.doctree')\n",
    "# print(doctree)\n",
    "# print(agentic_strategies_doctree)\n",
    "print(installation_doctree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<class 'docutils.nodes.section'>, <class 'sphinx.addnodes.desc_signature'>, <class 'docutils.nodes.problematic'>, <class 'docutils.nodes.target'>, <class 'docutils.nodes.system_message'>}\n",
      "\n",
      "Examples for <class 'docutils.nodes.section'>:\n",
      "<section ids=\"welcome-to-llamaindex\" names=\"welcome\\ to\\ llamaindex\\ ü¶ô\\ !\"><title>Welcome to LlamaIndex ü¶ô !</title><paragraph>LlamaIndex is a data framework for <reference name=\"LLM\" refuri=\"https://en.wikipedia.org/wiki/Large_language_model\">LLM</reference><target ids=\"['llm']\" names=\"['llm']\" refuri=\"https://en.wikipedia.org/wiki/Large_language_model\"/>-based applications to ingest, structure, and access private or domain-specific data. It‚Äôs available in Python (these docs) and <reference name=\"Typescript\" refuri=\"https://ts.llamaindex.ai/\">Typescript</reference><target ids=\"['typescript']\" names=\"['typescript']\" refuri=\"https://ts.llamaindex.ai/\"/>.</paragraph><section ids=\"why-llamaindex\" names=\"üöÄ\\ why\\ llamaindex?\"><title>üöÄ Why LlamaIndex?</title><paragraph>LLMs offer a natural language interface between humans and data. Widely available models come pre-trained on huge amounts of publicly available data like Wikipedia, mailing lists, textbooks, source code and more.</paragraph><paragraph>However, while LLMs are trained on a great deal of data, they are not trained on <strong>your</strong> data, which may be private or specific to the problem you‚Äôre trying to solve. It‚Äôs behind APIs, in SQL databases, or trapped in PDFs and slide decks.</paragraph><paragraph>You may choose to <strong>fine-tune</strong> a LLM with your data, but:</paragraph><bullet_list bullet=\"-\"><list_item><paragraph>Training a LLM is <strong>expensive</strong>.</paragraph></list_item><list_item><paragraph>Due to the cost to train, it‚Äôs <strong>hard to update</strong> a LLM with latest information.</paragraph></list_item><list_item><paragraph><strong>Observability</strong> is lacking. When you ask a LLM a question, it‚Äôs not obvious how the LLM arrived at its answer.</paragraph></list_item></bullet_list><paragraph>LlamaIndex takes a different approach called <reference name=\"Retrieval-Augmented Generation (RAG)\" refuri=\"./getting_started/concepts.html\">Retrieval-Augmented Generation (RAG)</reference><target ids=\"['retrieval-augmented-generation-rag']\" names=\"['retrieval-augmented generation (rag)']\" refuri=\"./getting_started/concepts.html\"/>. Instead of asking LLM to generate an answer immediately, LlamaIndex:</paragraph><enumerated_list enumtype=\"arabic\" prefix=\"\" suffix=\".\"><list_item><paragraph>retrieves information from your data sources first,</paragraph></list_item><list_item><paragraph>adds it to your question as context, and</paragraph></list_item><list_item><paragraph>asks the LLM to answer based on the enriched prompt.</paragraph></list_item></enumerated_list><paragraph>RAG overcomes all three weaknesses of the fine-tuning approach:</paragraph><bullet_list bullet=\"-\"><list_item><paragraph>There‚Äôs no training involved, so it‚Äôs <strong>cheap</strong>.</paragraph></list_item><list_item><paragraph>Data is fetched only when you ask for them, so it‚Äôs <strong>always up to date</strong>.</paragraph></list_item><list_item><paragraph>LlamaIndex can show you the retrieved documents, so it‚Äôs <strong>more trustworthy</strong>.</paragraph></list_item></bullet_list><paragraph>LlamaIndex imposes no restriction on how you use LLMs. You can still use LLMs as auto-complete, chatbots, semi-autonomous agents, and more (see Use Cases on the left). It only makes LLMs more relevant to you.</paragraph></section><section ids=\"how-can-llamaindex-help\" names=\"ü¶ô\\ how\\ can\\ llamaindex\\ help?\"><title>ü¶ô How can LlamaIndex help?</title><paragraph>LlamaIndex provides the following tools:</paragraph><bullet_list bullet=\"-\"><list_item><paragraph><strong>Data connectors</strong> ingest your existing data from their native source and format. These could be APIs, PDFs, SQL, and (much) more.</paragraph></list_item><list_item><paragraph><strong>Data indexes</strong> structure your data in intermediate representations that are easy and performant for LLMs to consume.</paragraph></list_item><list_item><paragraph><strong>Engines</strong> provide natural language access to your data. For example:</paragraph><bullet_list bullet=\"-\"><list_item><paragraph>Query engines are powerful retrieval interfaces for knowledge-augmented output.</paragraph></list_item><list_item><paragraph>Chat engines are conversational interfaces for multi-message, ‚Äúback and forth‚Äù interactions with your data.</paragraph></list_item></bullet_list></list_item><list_item><paragraph><strong>Data agents</strong> are LLM-powered knowledge workers augmented by tools, from simple helper functions to API integrations and more.</paragraph></list_item><list_item><paragraph><strong>Application integrations</strong> tie LlamaIndex back into the rest of your ecosystem. This could be LangChain, Flask, Docker, ChatGPT, or‚Ä¶ anything else!</paragraph></list_item></bullet_list></section><section ids=\"who-is-llamaindex-for\" names=\"üë®‚Äçüë©‚Äçüëß‚Äçüë¶\\ who\\ is\\ llamaindex\\ for?\"><title>üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Who is LlamaIndex for?</title><paragraph>LlamaIndex provides tools for beginners, advanced users, and everyone in between.</paragraph><paragraph>Our high-level API allows beginner users to use LlamaIndex to ingest and query their data in 5 lines of code.</paragraph><paragraph>For more complex applications, our lower-level APIs allow advanced users to customize and extend any module‚Äîdata connectors, indices, retrievers, query engines, reranking modules‚Äîto fit their needs.</paragraph></section><section ids=\"getting-started\" names=\"getting\\ started\"><title>Getting Started</title><paragraph>To install the library:</paragraph><paragraph><literal>pip install llama-index</literal></paragraph><paragraph>We recommend starting at <reference name=\"how to read these docs\" refuri=\"./getting_started/reading.html\">how to read these docs</reference><target ids=\"['how-to-read-these-docs']\" names=\"['how to read these docs']\" refuri=\"./getting_started/reading.html\"/>, which will point you to the right place based on your experience level.</paragraph></section><section ids=\"ecosystem\" names=\"üó∫Ô∏è\\ ecosystem\"><title>üó∫Ô∏è Ecosystem</title><paragraph>To download or contribute, find LlamaIndex on:</paragraph><bullet_list bullet=\"-\"><list_item><paragraph>Github: <reference refuri=\"https://github.com/jerryjliu/llama_index\">https://github.com/jerryjliu/llama_index</reference></paragraph></list_item><list_item><paragraph>PyPi:</paragraph><bullet_list bullet=\"-\"><list_item><paragraph>LlamaIndex: <reference refuri=\"https://pypi.org/project/llama-index/\">https://pypi.org/project/llama-index/</reference>.</paragraph></list_item><list_item><paragraph>GPT Index (duplicate): <reference refuri=\"https://pypi.org/project/gpt-index/\">https://pypi.org/project/gpt-index/</reference>.</paragraph></list_item></bullet_list></list_item><list_item><definition_list><definition_list_item><term>NPM (Typescript/Javascript):</term><definition><bullet_list bullet=\"-\"><list_item><paragraph>Github: <reference refuri=\"https://github.com/run-llama/LlamaIndexTS\">https://github.com/run-llama/LlamaIndexTS</reference></paragraph></list_item><list_item><paragraph>Docs: <reference refuri=\"https://ts.llamaindex.ai/\">https://ts.llamaindex.ai/</reference></paragraph></list_item><list_item><paragraph>LlamaIndex.TS: <reference refuri=\"https://www.npmjs.com/package/llamaindex\">https://www.npmjs.com/package/llamaindex</reference></paragraph></list_item></bullet_list></definition></definition_list_item></definition_list></list_item></bullet_list><section ids=\"community\" names=\"community\"><title>Community</title><paragraph>Need help? Have a feature suggestion? Join the LlamaIndex community:</paragraph><bullet_list bullet=\"-\"><list_item><paragraph>Twitter: <reference refuri=\"https://twitter.com/llama_index\">https://twitter.com/llama_index</reference></paragraph></list_item><list_item><paragraph>Discord <reference refuri=\"https://discord.gg/dGcwcsnxhU\">https://discord.gg/dGcwcsnxhU</reference></paragraph></list_item></bullet_list></section><section ids=\"associated-projects\" names=\"associated\\ projects\"><title>Associated projects</title><bullet_list bullet=\"-\"><list_item><paragraph>üè° LlamaHub: <reference refuri=\"https://llamahub.ai\">https://llamahub.ai</reference> | A large (and growing!) collection of custom data connectors</paragraph></list_item><list_item><paragraph>üß™ LlamaLab: <reference refuri=\"https://github.com/run-llama/llama-lab\">https://github.com/run-llama/llama-lab</reference> | Ambitious projects built on top of LlamaIndex</paragraph></list_item></bullet_list><compound classes=\"toctree-wrapper\"><toctree caption=\"Getting Started\" entries=\"[(None, 'getting_started/installation'), (None, 'getting_started/reading'), (None, 'getting_started/starter_example'), (None, 'getting_started/concepts'), (None, 'getting_started/customization'), (None, 'getting_started/discover_llamaindex')]\" glob=\"False\" hidden=\"True\" includefiles=\"['getting_started/installation', 'getting_started/reading', 'getting_started/starter_example', 'getting_started/concepts', 'getting_started/customization', 'getting_started/discover_llamaindex']\" includehidden=\"False\" maxdepth=\"1\" numbered=\"0\" parent=\"index\" rawcaption=\"Getting Started\" rawentries=\"[]\" titlesonly=\"False\"/></compound><compound classes=\"toctree-wrapper\"><toctree caption=\"Use Cases\" entries=\"[(None, 'use_cases/q_and_a'), (None, 'use_cases/chatbots'), (None, 'use_cases/agents'), (None, 'use_cases/extraction'), (None, 'use_cases/multimodal')]\" glob=\"False\" hidden=\"True\" includefiles=\"['use_cases/q_and_a', 'use_cases/chatbots', 'use_cases/agents', 'use_cases/extraction', 'use_cases/multimodal']\" includehidden=\"False\" maxdepth=\"2\" numbered=\"0\" parent=\"index\" rawcaption=\"Use Cases\" rawentries=\"[]\" titlesonly=\"False\"/></compound><compound classes=\"toctree-wrapper\"><toctree caption=\"Understanding\" entries=\"[(None, 'understanding/understanding'), (None, 'understanding/using_llms/using_llms'), (None, 'understanding/loading/loading'), (None, 'understanding/indexing/indexing'), (None, 'understanding/storing/storing'), (None, 'understanding/querying/querying'), (None, 'understanding/putting_it_all_together/putting_it_all_together'), (None, 'understanding/tracing_and_debugging/tracing_and_debugging'), (None, 'understanding/evaluating/evaluating')]\" glob=\"False\" hidden=\"True\" includefiles=\"['understanding/understanding', 'understanding/using_llms/using_llms', 'understanding/loading/loading', 'understanding/indexing/indexing', 'understanding/storing/storing', 'understanding/querying/querying', 'understanding/putting_it_all_together/putting_it_all_together', 'understanding/tracing_and_debugging/tracing_and_debugging', 'understanding/evaluating/evaluating']\" includehidden=\"False\" maxdepth=\"2\" numbered=\"0\" parent=\"index\" rawcaption=\"Understanding\" rawentries=\"[]\" titlesonly=\"False\"/></compound><compound classes=\"toctree-wrapper\"><toctree caption=\"Optimizing\" entries=\"[(None, 'optimizing/basic_strategies/basic_strategies'), (None, 'optimizing/advanced_retrieval/advanced_retrieval'), (None, 'optimizing/agentic_strategies/agentic_strategies'), (None, 'optimizing/evaluation/evaluation'), (None, 'optimizing/fine-tuning/fine-tuning'), (None, 'optimizing/production_rag'), (None, 'optimizing/building_rag_from_scratch')]\" glob=\"False\" hidden=\"True\" includefiles=\"['optimizing/basic_strategies/basic_strategies', 'optimizing/advanced_retrieval/advanced_retrieval', 'optimizing/agentic_strategies/agentic_strategies', 'optimizing/evaluation/evaluation', 'optimizing/fine-tuning/fine-tuning', 'optimizing/production_rag', 'optimizing/building_rag_from_scratch']\" includehidden=\"False\" maxdepth=\"2\" numbered=\"0\" parent=\"index\" rawcaption=\"Optimizing\" rawentries=\"[]\" titlesonly=\"False\"/></compound><compound classes=\"toctree-wrapper\"><toctree caption=\"Module Guides\" entries=\"[(None, 'module_guides/models/models'), (None, 'module_guides/models/prompts'), (None, 'module_guides/loading/loading'), (None, 'module_guides/indexing/indexing'), (None, 'module_guides/storing/storing'), (None, 'module_guides/querying/querying'), (None, 'module_guides/observability/observability'), (None, 'module_guides/evaluating/root'), (None, 'module_guides/supporting_modules/supporting_modules')]\" glob=\"False\" hidden=\"True\" includefiles=\"['module_guides/models/models', 'module_guides/models/prompts', 'module_guides/loading/loading', 'module_guides/indexing/indexing', 'module_guides/storing/storing', 'module_guides/querying/querying', 'module_guides/observability/observability', 'module_guides/evaluating/root', 'module_guides/supporting_modules/supporting_modules']\" includehidden=\"False\" maxdepth=\"2\" numbered=\"0\" parent=\"index\" rawcaption=\"Module Guides\" rawentries=\"[]\" titlesonly=\"False\"/></compound><compound classes=\"toctree-wrapper\"><toctree caption=\"API Reference\" entries=\"[(None, 'api_reference/index')]\" glob=\"False\" hidden=\"True\" includefiles=\"['api_reference/index']\" includehidden=\"False\" maxdepth=\"1\" numbered=\"0\" parent=\"index\" rawcaption=\"API Reference\" rawentries=\"[]\" titlesonly=\"False\"/></compound><compound classes=\"toctree-wrapper\"><toctree caption=\"Community\" entries=\"[(None, 'community/integrations'), (None, 'community/frequently_asked_questions'), (None, 'community/full_stack_projects')]\" glob=\"False\" hidden=\"True\" includefiles=\"['community/integrations', 'community/frequently_asked_questions', 'community/full_stack_projects']\" includehidden=\"False\" maxdepth=\"2\" numbered=\"0\" parent=\"index\" rawcaption=\"Community\" rawentries=\"[]\" titlesonly=\"False\"/></compound><compound classes=\"toctree-wrapper\"><toctree caption=\"Contributing\" entries=\"[(None, 'contributing/contributing'), (None, 'contributing/documentation')]\" glob=\"False\" hidden=\"True\" includefiles=\"['contributing/contributing', 'contributing/documentation']\" includehidden=\"False\" maxdepth=\"2\" numbered=\"0\" parent=\"index\" rawcaption=\"Contributing\" rawentries=\"[]\" titlesonly=\"False\"/></compound><compound classes=\"toctree-wrapper\"><toctree caption=\"Changes\" entries=\"[(None, 'changes/changelog'), (None, 'changes/deprecated_terms')]\" glob=\"False\" hidden=\"True\" includefiles=\"['changes/changelog', 'changes/deprecated_terms']\" includehidden=\"False\" maxdepth=\"2\" numbered=\"0\" parent=\"index\" rawcaption=\"Changes\" rawentries=\"[]\" titlesonly=\"False\"/></compound></section></section></section>\n",
      "\n",
      "Examples for <class 'docutils.nodes.target'>:\n",
      "<target ids=\"['llm']\" names=\"['llm']\" refuri=\"https://en.wikipedia.org/wiki/Large_language_model\"/>\n",
      "\n",
      "Examples for <class 'sphinx.addnodes.desc_signature'>:\n",
      "<desc_signature _toc_name=\"CohereRerankerFinetuneEngine\" _toc_parts=\"('llama_index.finetuning', 'CohereRerankerFinetuneEngine')\" class=\"\" classes=\"sig sig-object\" fullname=\"CohereRerankerFinetuneEngine\" ids=\"llama_index.finetuning.CohereRerankerFinetuneEngine\" module=\"llama_index.finetuning\"><desc_annotation xml:space=\"preserve\">class<desc_sig_space classes=\"w\"> </desc_sig_space></desc_annotation><desc_addname classes=\"sig-prename descclassname\" xml:space=\"preserve\">llama_index.finetuning.</desc_addname><desc_name classes=\"sig-name descname\" xml:space=\"preserve\">CohereRerankerFinetuneEngine</desc_name><desc_parameterlist xml:space=\"preserve\"><desc_parameter xml:space=\"preserve\"><desc_sig_name classes=\"n\">train_file_name</desc_sig_name><desc_sig_punctuation classes=\"p\">:</desc_sig_punctuation><desc_sig_space classes=\"w\"> </desc_sig_space><desc_sig_name classes=\"n\"><pending_xref py:class=\"True\" py:module=\"llama_index.finetuning\" refdomain=\"py\" refspecific=\"False\" reftarget=\"str\" reftype=\"class\">str</pending_xref></desc_sig_name><desc_sig_space classes=\"w\"> </desc_sig_space><desc_sig_operator classes=\"o\">=</desc_sig_operator><desc_sig_space classes=\"w\"> </desc_sig_space><inline classes=\"default_value\" support_smartquotes=\"False\">'train.jsonl'</inline></desc_parameter><desc_parameter xml:space=\"preserve\"><desc_sig_name classes=\"n\">val_file_name</desc_sig_name><desc_sig_punctuation classes=\"p\">:</desc_sig_punctuation><desc_sig_space classes=\"w\"> </desc_sig_space><desc_sig_name classes=\"n\"><pending_xref py:class=\"True\" py:module=\"llama_index.finetuning\" refdomain=\"py\" refspecific=\"False\" reftarget=\"typing.Optional\" reftype=\"obj\">Optional</pending_xref><desc_sig_punctuation classes=\"p\">[</desc_sig_punctuation><pending_xref py:class=\"True\" py:module=\"llama_index.finetuning\" refdomain=\"py\" refspecific=\"False\" reftarget=\"str\" reftype=\"class\">str</pending_xref><desc_sig_punctuation classes=\"p\">]</desc_sig_punctuation></desc_sig_name><desc_sig_space classes=\"w\"> </desc_sig_space><desc_sig_operator classes=\"o\">=</desc_sig_operator><desc_sig_space classes=\"w\"> </desc_sig_space><inline classes=\"default_value\" support_smartquotes=\"False\">None</inline></desc_parameter><desc_parameter xml:space=\"preserve\"><desc_sig_name classes=\"n\">model_name</desc_sig_name><desc_sig_punctuation classes=\"p\">:</desc_sig_punctuation><desc_sig_space classes=\"w\"> </desc_sig_space><desc_sig_name classes=\"n\"><pending_xref py:class=\"True\" py:module=\"llama_index.finetuning\" refdomain=\"py\" refspecific=\"False\" reftarget=\"str\" reftype=\"class\">str</pending_xref></desc_sig_name><desc_sig_space classes=\"w\"> </desc_sig_space><desc_sig_operator classes=\"o\">=</desc_sig_operator><desc_sig_space classes=\"w\"> </desc_sig_space><inline classes=\"default_value\" support_smartquotes=\"False\">'exp_finetune'</inline></desc_parameter><desc_parameter xml:space=\"preserve\"><desc_sig_name classes=\"n\">model_type</desc_sig_name><desc_sig_punctuation classes=\"p\">:</desc_sig_punctuation><desc_sig_space classes=\"w\"> </desc_sig_space><desc_sig_name classes=\"n\"><pending_xref py:class=\"True\" py:module=\"llama_index.finetuning\" refdomain=\"py\" refspecific=\"False\" reftarget=\"str\" reftype=\"class\">str</pending_xref></desc_sig_name><desc_sig_space classes=\"w\"> </desc_sig_space><desc_sig_operator classes=\"o\">=</desc_sig_operator><desc_sig_space classes=\"w\"> </desc_sig_space><inline classes=\"default_value\" support_smartquotes=\"False\">'RERANK'</inline></desc_parameter><desc_parameter xml:space=\"preserve\"><desc_sig_name classes=\"n\">base_model</desc_sig_name><desc_sig_punctuation classes=\"p\">:</desc_sig_punctuation><desc_sig_space classes=\"w\"> </desc_sig_space><desc_sig_name classes=\"n\"><pending_xref py:class=\"True\" py:module=\"llama_index.finetuning\" refdomain=\"py\" refspecific=\"False\" reftarget=\"str\" reftype=\"class\">str</pending_xref></desc_sig_name><desc_sig_space classes=\"w\"> </desc_sig_space><desc_sig_operator classes=\"o\">=</desc_sig_operator><desc_sig_space classes=\"w\"> </desc_sig_space><inline classes=\"default_value\" support_smartquotes=\"False\">'english'</inline></desc_parameter><desc_parameter xml:space=\"preserve\"><desc_sig_name classes=\"n\">api_key</desc_sig_name><desc_sig_punctuation classes=\"p\">:</desc_sig_punctuation><desc_sig_space classes=\"w\"> </desc_sig_space><desc_sig_name classes=\"n\"><pending_xref py:class=\"True\" py:module=\"llama_index.finetuning\" refdomain=\"py\" refspecific=\"False\" reftarget=\"typing.Optional\" reftype=\"obj\">Optional</pending_xref><desc_sig_punctuation classes=\"p\">[</desc_sig_punctuation><pending_xref py:class=\"True\" py:module=\"llama_index.finetuning\" refdomain=\"py\" refspecific=\"False\" reftarget=\"str\" reftype=\"class\">str</pending_xref><desc_sig_punctuation classes=\"p\">]</desc_sig_punctuation></desc_sig_name><desc_sig_space classes=\"w\"> </desc_sig_space><desc_sig_operator classes=\"o\">=</desc_sig_operator><desc_sig_space classes=\"w\"> </desc_sig_space><inline classes=\"default_value\" support_smartquotes=\"False\">None</inline></desc_parameter></desc_parameterlist></desc_signature>\n",
      "\n",
      "Examples for <class 'docutils.nodes.system_message'>:\n",
      "<system_message backrefs=\"id2\" ids=\"id1\" level=\"3\" line=\"1\" source=\"/Users/sasha/github/LlamaIndex/llama_index/llama_index/vector_stores/postgres.py:docstring of llama_index.vector_stores.postgres.PGVectorStore\" type=\"ERROR\"><paragraph>Unknown interpreted text role ‚Äúparamref‚Äù.</paragraph></system_message>\n",
      "\n",
      "Examples for <class 'docutils.nodes.problematic'>:\n",
      "<problematic ids=\"id2\" refid=\"id1\">:paramref:`_sql.Select.with_only_columns.maintain_column_froms`</problematic>\n"
     ]
    }
   ],
   "source": [
    "# sphinx.addnodes.document\n",
    "# links are: docutils.nodes.target, section is docutils.nodes.section \n",
    "# ids is linked content, target and sections\n",
    "\n",
    "# Print all the children types of all the doctrees ids, and store in a dict\n",
    "import glob\n",
    "\n",
    "unique_types = set()\n",
    "doctree_files = glob.glob('/Users/sasha/github/LlamaIndex/llama_index/docs/_build/doctrees/**/*.doctree', recursive=True)\n",
    "doctree_files += glob.glob('/Users/sasha/github/LlamaIndex/llama_index/docs/_build/doctrees/*.doctree')\n",
    "\n",
    "type_examples = {}\n",
    "\n",
    "for file_path in doctree_files:\n",
    "    doctree = read_doctree(file_path)\n",
    "    for id in doctree.ids:\n",
    "        type_id = type(doctree.ids[id])\n",
    "        unique_types.add(type_id)\n",
    "        if type_id not in type_examples:\n",
    "            type_examples[type_id] = []\n",
    "        type_examples[type_id].append(doctree.ids[id])\n",
    "\n",
    "print(unique_types)\n",
    "for type_id, examples in type_examples.items():\n",
    "    print(f\"\\nExamples for {type_id}:\")\n",
    "    for example in examples:\n",
    "        print(example)\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discovered: these are different types of values for ids:\n",
    "- docutils.nodes.section,\n",
    "- docutils.nodes.target,\n",
    "- sphinx.addnodes.desc_signature,\n",
    "- docutils.nodes.system_message,\n",
    "- docutils.nodes.problematic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure\n",
      "{'rawsource': '', 'children': [<title: <#text: 'Welcome to Lla ...'>>, <paragraph: <#text: 'LlamaIndex is  ...'><reference...><target \"llm\" ...>, <section \"üöÄ why llamaindex?\": <title...><paragraph...><paragraph...><paragraph...><bul ...>, <section \"ü¶ô how can llamaindex help?\": <title...><paragraph...><bullet_list...>>, <section \"üë®‚Äçüë©‚Äçüëß‚Äçüë¶ who is llamaindex for?\": <title...><paragraph...><paragraph...><paragraph...>>, <section \"getting started\": <title...><paragraph...><paragraph...><paragraph...>>, <section \"üó∫Ô∏è ecosystem\": <title...><paragraph...><bullet_list...><section \"commun ...>], 'attributes': {'ids': ['welcome-to-llamaindex'], 'classes': [], 'names': ['welcome to llamaindex ü¶ô !'], 'dupnames': [], 'backrefs': []}, 'tagname': 'section', 'parent': <document: <section \"welcome to llamaindex ü¶ô !\"...>>, 'document': <document: <section \"welcome to llamaindex ü¶ô !\"...>>, 'source': '/Users/sasha/github/LlamaIndex/llama_index/docs/index.rst', 'line': 2}\n",
      "dict_keys(['rawsource', 'children', 'attributes', 'tagname', 'parent', 'document', 'source', 'line'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<title: <#text: 'Welcome to Lla ...'>>,\n",
       " <paragraph: <#text: 'LlamaIndex is  ...'><reference...><target \"llm\" ...>,\n",
       " <section \"üöÄ why llamaindex?\": <title...><paragraph...><paragraph...><paragraph...><bul ...>,\n",
       " <section \"ü¶ô how can llamaindex help?\": <title...><paragraph...><bullet_list...>>,\n",
       " <section \"üë®‚Äçüë©‚Äçüëß‚Äçüë¶ who is llamaindex for?\": <title...><paragraph...><paragraph...><paragraph...>>,\n",
       " <section \"getting started\": <title...><paragraph...><paragraph...><paragraph...>>,\n",
       " <section \"üó∫Ô∏è ecosystem\": <title...><paragraph...><bullet_list...><section \"commun ...>]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_index = 0\n",
    "target_index = 1\n",
    "desc_index = 2\n",
    "system_message_index = 3\n",
    "problematic_index = 4\n",
    "all_sections = list(type_examples.values())[section_index]\n",
    "print(f\"Structure\")\n",
    "print(all_sections[0].__dict__)\n",
    "print(all_sections[0].__dict__.keys())\n",
    "all_sections[0].children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_children_type = collections.defaultdict(list)\n",
    "for section in all_sections:\n",
    "    for children in section.children:\n",
    "        section_children_type[type(children)].append(children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([<class 'docutils.nodes.title'>, <class 'docutils.nodes.paragraph'>, <class 'docutils.nodes.section'>, <class 'docutils.nodes.bullet_list'>, <class 'docutils.nodes.enumerated_list'>, <class 'docutils.nodes.compound'>, <class 'docutils.nodes.transition'>, <class 'docutils.nodes.reference'>, <class 'docutils.nodes.comment'>, <class 'docutils.nodes.block_quote'>, <class 'docutils.nodes.literal_block'>, <class 'docutils.nodes.tip'>, <class 'docutils.nodes.target'>, <class 'docutils.nodes.container'>, <class 'sphinx.addnodes.index'>, <class 'sphinx.addnodes.desc'>, <class 'sphinx.addnodes.tabular_col_spec'>, <class 'sphinx.ext.autosummary.autosummary_table'>, <class 'sphinx.ext.autosummary.autosummary_toc'>, <class 'docutils.nodes.line_block'>, <class 'docutils.nodes.raw'>, <class 'docutils.nodes.table'>, <class 'docutils.nodes.warning'>, <class 'docutils.nodes.admonition'>])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_children_type.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: <class 'docutils.nodes.title'>\n",
      "First Value Dict: {'rawsource': 'Welcome to LlamaIndex ü¶ô !', 'children': [<#text: 'Welcome to LlamaIndex ü¶ô !'>], 'attributes': {'ids': [], 'classes': [], 'names': [], 'dupnames': [], 'backrefs': []}, 'tagname': 'title', 'parent': <section \"welcome to llamaindex ü¶ô !\": <title...><paragraph...><section \"üöÄ why llamaindex?\"...> ...>, 'document': <document: <section \"welcome to llamaindex ü¶ô !\"...>>, 'source': '/Users/sasha/github/LlamaIndex/llama_index/docs/index.rst', 'line': 2}\n"
     ]
    }
   ],
   "source": [
    "for key, values in section_children_type.items():\n",
    "    print(f\"Key: {key}\")\n",
    "    print(f\"First Value Dict: {values[0].__dict__}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: Inspecting the section is interesting, but I think what benefit me more now is to look at the structure of 1 document hollistically, which I will experiment in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Holistically look at the doctree for index.doctree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<title: <#text: 'Community'>>,\n",
       " <paragraph: <#text: 'Need help? Hav ...'>>,\n",
       " <bullet_list: <list_item...><list_item...>>]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doctree.ids[\"ecosystem\"].children[3].children\n",
    "\n",
    "# ecosystem is a section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: this informs me that I can build a parser that auto populates the content for each documents to store/index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Langchain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "markdown_path = \"/Users/sasha/github/LlamaIndex/llama_index/docs/index.rst\"\n",
    "loader = UnstructuredMarkdownLoader(markdown_path)\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Welcome to LlamaIndex ü¶ô !\\n\\nLlamaIndex is a data framework for LLM <https://en.wikipedia.org/wiki/Large_language_model>-based applications to ingest, structure, and access private or domain-specific data. It\\'s available in Python (these docs) and Typescript <https://ts.llamaindex.ai/>.\\n\\nüöÄ Why LlamaIndex?\\n\\nLLMs offer a natural language interface between humans and data. Widely available models come pre-trained on huge amounts of publicly available data like Wikipedia, mailing lists, textbooks, source code and more.\\n\\nHowever, while LLMs are trained on a great deal of data, they are not trained on your data, which may be private or specific to the problem you\\'re trying to solve. It\\'s behind APIs, in SQL databases, or trapped in PDFs and slide decks.\\n\\nYou may choose to fine-tune a LLM with your data, but:\\n\\nTraining a LLM is expensive.\\n\\nDue to the cost to train, it\\'s hard to update a LLM with latest information.\\n\\nObservability is lacking. When you ask a LLM a question, it\\'s not obvious how the LLM arrived at its answer.\\n\\nLlamaIndex takes a different approach called Retrieval-Augmented Generation (RAG) <./getting_started/concepts.html>_. Instead of asking LLM to generate an answer immediately, LlamaIndex:\\n\\nretrieves information from your data sources first,\\n\\nadds it to your question as context, and\\n\\nasks the LLM to answer based on the enriched prompt.\\n\\nRAG overcomes all three weaknesses of the fine-tuning approach:\\n\\nThere\\'s no training involved, so it\\'s cheap.\\n\\nData is fetched only when you ask for them, so it\\'s always up to date.\\n\\nLlamaIndex can show you the retrieved documents, so it\\'s more trustworthy.\\n\\nLlamaIndex imposes no restriction on how you use LLMs. You can still use LLMs as auto-complete, chatbots, semi-autonomous agents, and more (see Use Cases on the left). It only makes LLMs more relevant to you.\\n\\nü¶ô How can LlamaIndex help?\\n\\nLlamaIndex provides the following tools:\\n\\nData connectors ingest your existing data from their native source and format. These could be APIs, PDFs, SQL, and (much) more.\\n\\nData indexes structure your data in intermediate representations that are easy and performant for LLMs to consume.\\n\\nEngines provide natural language access to your data. For example:\\n\\nQuery engines are powerful retrieval interfaces for knowledge-augmented output.\\n\\nChat engines are conversational interfaces for multi-message, \"back and forth\" interactions with your data.\\n\\nData agents are LLM-powered knowledge workers augmented by tools, from simple helper functions to API integrations and more.\\n\\nApplication integrations tie LlamaIndex back into the rest of your ecosystem. This could be LangChain, Flask, Docker, ChatGPT, or‚Ä¶ anything else!\\n\\nüë®\\u200düë©\\u200düëß\\u200düë¶ Who is LlamaIndex for?\\n\\nLlamaIndex provides tools for beginners, advanced users, and everyone in between.\\n\\nOur high-level API allows beginner users to use LlamaIndex to ingest and query their data in 5 lines of code.\\n\\nFor more complex applications, our lower-level APIs allow advanced users to customize and extend any module‚Äîdata connectors, indices, retrievers, query engines, reranking modules‚Äîto fit their needs.\\n\\nGetting Started\\n\\nTo install the library:\\n\\npip install llama-index\\n\\nWe recommend starting at how to read these docs <./getting_started/reading.html>_, which will point you to the right place based on your experience level.\\n\\nüó∫Ô∏è Ecosystem\\n\\nTo download or contribute, find LlamaIndex on:\\n\\nGithub: https://github.com/jerryjliu/llama_index\\n\\nPyPi:\\n\\nLlamaIndex: https://pypi.org/project/llama-index/.\\n\\nGPT Index (duplicate): https://pypi.org/project/gpt-index/.\\n\\nNPM (Typescript/Javascript):\\n\\nGithub: https://github.com/run-llama/LlamaIndexTS\\n\\nDocs: https://ts.llamaindex.ai/\\n\\nLlamaIndex.TS: https://www.npmjs.com/package/llamaindex\\n\\nCommunity\\n\\nNeed help? Have a feature suggestion? Join the LlamaIndex community:\\n\\nTwitter: https://twitter.com/llama_index\\n\\nDiscord https://discord.gg/dGcwcsnxhU\\n\\nAssociated projects\\n\\nüè° LlamaHub: https://llamahub.ai | A large (and growing!) collection of custom data connectors\\n\\nüß™ LlamaLab: https://github.com/run-llama/llama-lab | Ambitious projects built on top of LlamaIndex\\n\\n.. toctree::\\n   :maxdepth: 1\\n   :caption: Getting Started\\n   :hidden:\\n\\ngetting_started/installation.md\\n   getting_started/reading.md\\n   getting_started/starter_example.md\\n   getting_started/concepts.md\\n   getting_started/customization.rst\\n   getting_started/discover_llamaindex.md\\n\\n.. toctree::\\n   :maxdepth: 2\\n   :caption: Use Cases\\n   :hidden:\\n\\nuse_cases/q_and_a.md\\n   use_cases/chatbots.md\\n   use_cases/agents.md\\n   use_cases/extraction.md\\n   use_cases/multimodal.md\\n\\n.. toctree::\\n   :maxdepth: 2\\n   :caption: Understanding\\n   :hidden:\\n\\nunderstanding/understanding.md\\n   understanding/using_llms/using_llms.md\\n   understanding/loading/loading.md\\n   understanding/indexing/indexing.md\\n   understanding/storing/storing.md\\n   understanding/querying/querying.md\\n   understanding/putting_it_all_together/putting_it_all_together.md\\n   understanding/tracing_and_debugging/tracing_and_debugging.md\\n   understanding/evaluating/evaluating.md\\n\\n.. toctree::\\n   :maxdepth: 2\\n   :caption: Optimizing\\n   :hidden:\\n\\noptimizing/basic_strategies/basic_strategies.md\\n   optimizing/advanced_retrieval/advanced_retrieval.md\\n   optimizing/agentic_strategies/agentic_strategies.md\\n   optimizing/evaluation/evaluation.md\\n   optimizing/fine-tuning/fine-tuning.md\\n   optimizing/production_rag.md\\n   optimizing/building_rag_from_scratch.md\\n.. toctree::\\n   :maxdepth: 2\\n   :caption: Module Guides\\n   :hidden:\\n\\nmodule_guides/models/models.md\\n   module_guides/models/prompts.md\\n   module_guides/loading/loading.md\\n   module_guides/indexing/indexing.md\\n   module_guides/storing/storing.md\\n   module_guides/querying/querying.md\\n   module_guides/observability/observability.md\\n   module_guides/evaluating/root.md\\n   module_guides/supporting_modules/supporting_modules.md\\n\\n.. toctree::\\n   :maxdepth: 1\\n   :caption: API Reference\\n   :hidden:\\n\\napi_reference/index.rst\\n\\n.. toctree::\\n   :maxdepth: 2\\n   :caption: Community\\n   :hidden:\\n\\ncommunity/integrations.md\\n   community/frequently_asked_questions.md\\n   community/full_stack_projects.md\\n\\n.. toctree::\\n   :maxdepth: 2\\n   :caption: Contributing\\n   :hidden:\\n\\ncontributing/contributing.rst\\n   contributing/documentation.rst\\n\\n.. toctree::\\n   :maxdepth: 2\\n   :caption: Changes\\n   :hidden:\\n\\nchanges/changelog.rst\\n   changes/deprecated_terms.md'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].__dict__[\"page_content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this does okay, but it did not capture the fact that \"use_cases/agents.md\" etc are links "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Unstructured?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observe index.rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.NarrativeText at 0x1bc36e190>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1bc3dc810>,\n",
       " <unstructured.documents.elements.Title at 0x1b82da3d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1b857ce90>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x179965d90>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1bba39f50>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bba3abd0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb9828d0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb981350>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1bb9822d0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb982650>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb982910>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb982290>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1bb982d50>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb981050>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb982750>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb9812d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1bb982c10>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1bb9836d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1bb980b50>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb981110>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb981d50>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb981fd0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb983450>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb9831d0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb982fd0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb983050>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1bb982e90>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1bb982ad0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1bb9838d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1bb983510>,\n",
       " <unstructured.documents.elements.Title at 0x1bb980090>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1bb981ad0>,\n",
       " <unstructured.documents.elements.Title at 0x1bb9813d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1bb981910>,\n",
       " <unstructured.documents.elements.Title at 0x1b84fae50>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x177803950>,\n",
       " <unstructured.documents.elements.ListItem at 0x1b85dded0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1b868ed50>,\n",
       " <unstructured.documents.elements.ListItem at 0x179a41bd0>,\n",
       " <unstructured.documents.elements.ListItem at 0x179cc2750>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bc340690>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bc3406d0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bc3404d0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bc340550>,\n",
       " <unstructured.documents.elements.Title at 0x1bc340350>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1bc340190>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bc3402d0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1b85bcf50>,\n",
       " <unstructured.documents.elements.Title at 0x1bbdaac90>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb97f250>,\n",
       " <unstructured.documents.elements.ListItem at 0x1bb97fbd0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1bb97fcd0>,\n",
       " <unstructured.documents.elements.Text at 0x1bb97ff90>,\n",
       " <unstructured.documents.elements.Text at 0x1bb97fd90>,\n",
       " <unstructured.documents.elements.Text at 0x1bb97f0d0>,\n",
       " <unstructured.documents.elements.Text at 0x1bb97f3d0>,\n",
       " <unstructured.documents.elements.Text at 0x1bb97f8d0>,\n",
       " <unstructured.documents.elements.Text at 0x1bb97eed0>,\n",
       " <unstructured.documents.elements.Text at 0x1bb97efd0>,\n",
       " <unstructured.documents.elements.Text at 0x1bb97ef50>,\n",
       " <unstructured.documents.elements.Text at 0x1bb97f4d0>,\n",
       " <unstructured.documents.elements.Title at 0x1bb97f610>,\n",
       " <unstructured.documents.elements.Text at 0x1b856eb10>,\n",
       " <unstructured.documents.elements.Title at 0x1b8666210>,\n",
       " <unstructured.documents.elements.Text at 0x1bc3dca10>,\n",
       " <unstructured.documents.elements.Title at 0x1bc3dca90>,\n",
       " <unstructured.documents.elements.Text at 0x1bc3dcc10>,\n",
       " <unstructured.documents.elements.Title at 0x1bc3dcd50>]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unstructured.partition.md import partition_md\n",
    "elements = partition_md(filename=\"/Users/sasha/github/LlamaIndex/llama_index/docs/index.rst\")\n",
    "elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'.. toctree::\\n   :maxdepth: 1\\n   :caption: Getting Started\\n   :hidden:'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(elements))\n",
    "elements[52].text # everything after 52 for index.rst is structure, observe another random file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observe another random md file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.Title at 0x17796f550>,\n",
       " <unstructured.documents.elements.Title at 0x1b868c790>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x179a9fe50>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x179a9c1d0>,\n",
       " <unstructured.documents.elements.Title at 0x179a9e250>,\n",
       " <unstructured.documents.elements.Title at 0x177406810>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x177406a90>,\n",
       " <unstructured.documents.elements.Title at 0x177404050>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x177404850>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x177404990>,\n",
       " <unstructured.documents.elements.Title at 0x177404510>,\n",
       " <unstructured.documents.elements.Title at 0x177404310>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x177404a90>,\n",
       " <unstructured.documents.elements.Title at 0x177406510>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x177404c50>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x177406a50>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x177406350>,\n",
       " <unstructured.documents.elements.ListItem at 0x1774060d0>,\n",
       " <unstructured.documents.elements.Text at 0x1b868f450>,\n",
       " <unstructured.documents.elements.ListItem at 0x1771812d0>,\n",
       " <unstructured.documents.elements.Title at 0x177405a90>,\n",
       " <unstructured.documents.elements.ListItem at 0x177405a50>,\n",
       " <unstructured.documents.elements.Title at 0x177406c50>,\n",
       " <unstructured.documents.elements.Title at 0x177406c90>,\n",
       " <unstructured.documents.elements.Title at 0x1799d3590>,\n",
       " <unstructured.documents.elements.Title at 0x1799d37d0>,\n",
       " <unstructured.documents.elements.Title at 0x1799d3250>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1799d3310>,\n",
       " <unstructured.documents.elements.Title at 0x1799d0c90>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1799d3050>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1799d30d0>,\n",
       " <unstructured.documents.elements.Title at 0x1799d3190>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1799d17d0>,\n",
       " <unstructured.documents.elements.Title at 0x1799d3890>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1799d3850>,\n",
       " <unstructured.documents.elements.Title at 0x1799d39d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1799d1590>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1799d3b50>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1799d3c90>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1799d2610>,\n",
       " <unstructured.documents.elements.Title at 0x1799d3d50>,\n",
       " <unstructured.documents.elements.Title at 0x1799d0190>,\n",
       " <unstructured.documents.elements.Title at 0x1799d3b90>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1799d0090>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1799d3cd0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1799d0790>,\n",
       " <unstructured.documents.elements.Title at 0x1799d3d10>,\n",
       " <unstructured.documents.elements.Title at 0x1799d1650>,\n",
       " <unstructured.documents.elements.Title at 0x1799d0550>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1799d0590>,\n",
       " <unstructured.documents.elements.ListItem at 0x1799d0990>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1773f6f10>,\n",
       " <unstructured.documents.elements.ListItem at 0x1773f5dd0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1773f55d0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1773f4e90>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1773f7810>,\n",
       " <unstructured.documents.elements.Title at 0x1773f6dd0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1776c8d90>,\n",
       " <unstructured.documents.elements.Title at 0x1776c8490>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1776cac10>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1776ca1d0>,\n",
       " <unstructured.documents.elements.Title at 0x179947dd0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1799473d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x179944650>,\n",
       " <unstructured.documents.elements.Title at 0x1799463d0>,\n",
       " <unstructured.documents.elements.Title at 0x1799478d0>,\n",
       " <unstructured.documents.elements.Title at 0x179945910>]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unstructured.partition.md import partition_md\n",
    "elements = partition_md(filename=\"/Users/sasha/github/LlamaIndex/llama_index/docs/module_guides/loading/documents_and_nodes/usage_documents.md\")\n",
    "elements "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
