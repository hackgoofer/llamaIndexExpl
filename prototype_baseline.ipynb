{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://127.0.0.1:6006/\n",
      "üì∫ To view the Phoenix app in a notebook, run `px.active_session().view()`\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "# Phoenix can display in real time the traces automatically\n",
    "# collected from your LlamaIndex application.\n",
    "import phoenix as px\n",
    "\n",
    "# Look for a URL in the output to open the App in a browser.\n",
    "px.launch_app()\n",
    "# The App is initially empty, but as you proceed with the steps below,\n",
    "# traces will appear automatically as your LlamaIndex application runs.\n",
    "\n",
    "import llama_index\n",
    "\n",
    "llama_index.set_global_handler(\"arize_phoenix\")\n",
    "\n",
    "# Run all of your LlamaIndex applications as usual and traces\n",
    "# will be collected and displayed in Phoenix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&1|High-Level Concepts &2|Retrieval Augmented Generation (RAG) &2|Stages within RAG &2|Important concepts within each step &3|Loading stage &3|Indexing Stage &3|Querying Stage &3|Putting it all together \n",
      " High-Level Concepts *******************\n",
      "\n",
      "This is a quick guide to the high-level concepts you'll encounter frequently when building LLM applications.\n",
      "\n",
      "Tip:\n",
      "\n",
      "  If you haven't, (install LlamaIndex)[installation.html] and complete   the (starter tutorial)[starter_example.html] before you read this.   It will help ground these steps in your experience.\n",
      "\n",
      " Retrieval Augmented Generation (RAG) ====================================\n",
      "\n",
      "LLMs are trained on enormous bodies of data but they aren't trained on **your** data. Retrieval-Augmented Generation (RAG) solves this problem by adding your data to the data LLMs already have access to. You will see references to RAG frequently in this documentation.\n",
      "\n",
      "In RAG, your data is loaded and prepared for queries or \"indexed\". User queries act on the index, which filters your data down to the most relevant context. This context and your query then go to the LLM along with a prompt, and the LLM provides a response.\n",
      "\n",
      "Even if what you're building is a chatbot or an agent, you'll want to know RAG techniques for getting data into your application.\n",
      "\n",
      "[image: ][image]\n",
      "\n",
      " Stages within RAG =================\n",
      "\n",
      "There are five key stages within RAG, which in turn will be a part of any larger application you build. These are:\n",
      "\n",
      "* **Loading**: this refers to getting your data from where it lives --   whether it's text files, PDFs, another website, a database, or an   API -- into your pipeline. (LlamaHub)[https://llamahub.ai/] provides   hundreds of connectors to choose from.\n",
      "\n",
      "* **Indexing**: this means creating a data structure that allows for   querying the data. For LLMs this nearly always means creating   \"vector embeddings\", numerical representations of the meaning of   your data, as well as numerous other metadata strategies to make it   easy to accurately find contextually relevant data.\n",
      "\n",
      "* **Storing**: once your data is indexed you will almost always want   to store your index, as well as other metadata, to avoid having to   re-index it.\n",
      "\n",
      "* **Querying**: for any given indexing strategy there are many ways   you can utilize LLMs and LlamaIndex data structures to query,   including sub-queries, multi-step queries and hybrid strategies.\n",
      "\n",
      "* **Evaluation**: a critical step in any pipeline is checking how   effective it is relative to other strategies, or when you make   changes. Evaluation provides objective measures of how accurate,   faithful and fast your responses to queries are.\n",
      "\n",
      "[image: ][image]\n",
      "\n",
      " Important concepts within each step ===================================\n",
      "\n",
      "There are also some terms you'll encounter that refer to steps within each of these stages.\n",
      "\n",
      " Loading stage -------------\n",
      "\n",
      "(Nodes and Documents)[../module_guides/loading/documents_and_nodes/ro ot.html]**Nodes and Documents**: A \"Document\" is a container around any data source - for instance, a PDF, an API output, or retrieve data from a database. A \"Node\" is the atomic unit of data in LlamaIndex and represents a \"chunk\" of a source \"Document\". Nodes have metadata that relate them to the document they are in and to other nodes.\n",
      "\n",
      "(Connectors)[../module_guides/loading/connector/root.html]**Connector s**: A data connector (often called a \"Reader\") ingests data from different data sources and data formats into \"Documents\" and \"Nodes\".\n",
      "\n",
      " Indexing Stage --------------\n",
      "\n",
      "(Indexes)[../module_guides/indexing/indexing.html]**Indexes**: Once you've ingested your data, LlamaIndex will help you index the data into a structure that's easy to retrieve. This usually involves generating \"vector embeddings\" which are stored in a specialized database called a \"vector store\". Indexes can also store a variety of metadata about your data.\n",
      "\n",
      "(Embeddings)[../module_guides/models/embeddings.html]**Embeddings** LLMs generate numerical representations of data called \"embeddings\". When filtering your data for relevance, LlamaIndex will convert queries into embeddings, and your vector store will find data that is numerically similar to the embedding of your query.\n",
      "\n",
      " Querying Stage --------------\n",
      "\n",
      "(Retrievers)[../module_guides/querying/retriever/root.html]**Retrieve rs**: A retriever defines how to efficiently retrieve relevant context from an index when given a query. Your retrieval strategy is key to the relevancy of the data retrieved and the efficiency with which it's done.\n",
      "\n",
      "(Routers)[../module_guides/querying/router/root.html]**Routers**: A router determines which retriever will be used to retrieve relevant context from the knowledge base. More specifically, the \"RouterRetriever\"¬†class, is responsible for selecting one or multiple candidate retrievers to execute a query. They use a selector to choose the best option based on each candidate's metadata and the query.\n",
      "\n",
      "(Node Postprocessors)[../module_guides/querying/node_postprocessors/r oot.html]**Node Postprocessors**: A node postprocessor takes in a set of retrieved nodes and applies transformations, filtering, or re- ranking logic to them.\n",
      "\n",
      "(Response Synthesizers)[../module_guides/querying/response_synthesize rs/root.html]**Response Synthesizers**: A response synthesizer generates a response from an LLM, using a user query and a given set of retrieved text chunks.\n",
      "\n",
      " Putting it all together -----------------------\n",
      "\n",
      "There are endless use cases for data-backed LLM applications but they can be roughly grouped into three categories:\n",
      "\n",
      "(Query Engines)[../module_guides/deploying/query_engine/root.html]**Query Engines**: A query engine is an end-to-end pipeline that allows you to ask questions over your data. It takes in a natural language query, and returns a response, along with reference context retrieved and passed to the LLM.\n",
      "\n",
      "(Chat Engines)[../module_guides/deploying/chat_engines/root.html]**Chat Engines**: A chat engine is an end-to-end pipeline for having a conversation with your data (multiple back-and-forth instead of a single question-and-answer).\n",
      "\n",
      "(Agents)[../module_guides/deploying/agents/root.html]**Agents**: An agent is an automated decision-maker powered by an LLM that interacts with the world via a set of (tools)[../module_guides/deploying/agents /tools/llamahub_tools_guide.html]. Agents can take an arbitrary number of steps to complete a given task, dynamically deciding on the best course of action rather than following pre-determined steps. This gives it additional flexibility to tackle more complex tasks.\n",
      "\n",
      "Next Steps:\n",
      "\n",
      "* Tell me how to (customize things)[customization.html]\n",
      "\n",
      "* Continue learning with our (understanding   LlamaIndex)[../understanding/understanding.html] guide\n",
      "\n",
      "* Ready to dig deep? Check out the module guides on the left\n",
      "\n",
      "&1|High-Level Concepts &2|Retrieval Augmented Generation (RAG) &2|Stages within RAG &2|Important concepts within each step &3|Loading stage &3|Indexing Stage &3|Querying Stage &3|Putting it all together \n"
     ]
    }
   ],
   "source": [
    "# raw md data\n",
    "raw_md_data = \"\"\"\n",
    "# High-Level Concepts\n",
    "\n",
    "This is a quick guide to the high-level concepts you'll encounter frequently when building LLM applications.\n",
    "\n",
    "```{tip}\n",
    "If you haven't, [install LlamaIndex](/getting_started/installation.md) and complete the [starter tutorial](/getting_started/starter_example.md) before you read this. It will help ground these steps in your experience.\n",
    "```\n",
    "\n",
    "## Retrieval Augmented Generation (RAG)\n",
    "\n",
    "LLMs are trained on enormous bodies of data but they aren't trained on **your** data. Retrieval-Augmented Generation (RAG) solves this problem by adding your data to the data LLMs already have access to. You will see references to RAG frequently in this documentation.\n",
    "\n",
    "In RAG, your data is loaded and prepared for queries or \"indexed\". User queries act on the index, which filters your data down to the most relevant context. This context and your query then go to the LLM along with a prompt, and the LLM provides a response.\n",
    "\n",
    "Even if what you're building is a chatbot or an agent, you'll want to know RAG techniques for getting data into your application.\n",
    "\n",
    "![](/_static/getting_started/basic_rag.png)\n",
    "\n",
    "## Stages within RAG\n",
    "\n",
    "There are five key stages within RAG, which in turn will be a part of any larger application you build. These are:\n",
    "\n",
    "- **Loading**: this refers to getting your data from where it lives -- whether it's text files, PDFs, another website, a database, or an API -- into your pipeline. [LlamaHub](https://llamahub.ai/) provides hundreds of connectors to choose from.\n",
    "\n",
    "- **Indexing**: this means creating a data structure that allows for querying the data. For LLMs this nearly always means creating `vector embeddings`, numerical representations of the meaning of your data, as well as numerous other metadata strategies to make it easy to accurately find contextually relevant data.\n",
    "\n",
    "- **Storing**: once your data is indexed you will almost always want to store your index, as well as other metadata, to avoid having to re-index it.\n",
    "\n",
    "- **Querying**: for any given indexing strategy there are many ways you can utilize LLMs and LlamaIndex data structures to query, including sub-queries, multi-step queries and hybrid strategies.\n",
    "\n",
    "- **Evaluation**: a critical step in any pipeline is checking how effective it is relative to other strategies, or when you make changes. Evaluation provides objective measures of how accurate, faithful and fast your responses to queries are.\n",
    "\n",
    "![](/_static/getting_started/stages.png)\n",
    "\n",
    "## Important concepts within each step\n",
    "\n",
    "There are also some terms you'll encounter that refer to steps within each of these stages.\n",
    "\n",
    "### Loading stage\n",
    "\n",
    "[**Nodes and Documents**](/module_guides/loading/documents_and_nodes/root.md): A `Document` is a container around any data source - for instance, a PDF, an API output, or retrieve data from a database. A `Node` is the atomic unit of data in LlamaIndex and represents a \"chunk\" of a source `Document`. Nodes have metadata that relate them to the document they are in and to other nodes.\n",
    "\n",
    "[**Connectors**](/module_guides/loading/connector/root.md):\n",
    "A data connector (often called a `Reader`) ingests data from different data sources and data formats into `Documents` and `Nodes`.\n",
    "\n",
    "### Indexing Stage\n",
    "\n",
    "[**Indexes**](/module_guides/indexing/indexing.md):\n",
    "Once you've ingested your data, LlamaIndex will help you index the data into a structure that's easy to retrieve. This usually involves generating `vector embeddings` which are stored in a specialized database called a `vector store`. Indexes can also store a variety of metadata about your data.\n",
    "\n",
    "[**Embeddings**](/module_guides/models/embeddings.md) LLMs generate numerical representations of data called `embeddings`. When filtering your data for relevance, LlamaIndex will convert queries into embeddings, and your vector store will find data that is numerically similar to the embedding of your query.\n",
    "\n",
    "### Querying Stage\n",
    "\n",
    "[**Retrievers**](/module_guides/querying/retriever/root.md):\n",
    "A retriever defines how to efficiently retrieve relevant context from an index when given a query. Your retrieval strategy is key to the relevancy of the data retrieved and the efficiency with which it's done.\n",
    "\n",
    "[**Routers**](/module_guides/querying/router/root.md):\n",
    "A router determines which retriever will be used to retrieve relevant context from the knowledge base. More specifically, the `RouterRetriever`¬†class, is responsible for selecting one or multiple candidate retrievers to execute a query. They use a selector to choose the best option based on each candidate's metadata and the query.\n",
    "\n",
    "[**Node Postprocessors**](/module_guides/querying/node_postprocessors/root.md):\n",
    "A node postprocessor takes in a set of retrieved nodes and applies transformations, filtering, or re-ranking logic to them.\n",
    "\n",
    "[**Response Synthesizers**](/module_guides/querying/response_synthesizers/root.md):\n",
    "A response synthesizer generates a response from an LLM, using a user query and a given set of retrieved text chunks.\n",
    "\n",
    "### Putting it all together\n",
    "\n",
    "There are endless use cases for data-backed LLM applications but they can be roughly grouped into three categories:\n",
    "\n",
    "[**Query Engines**](/module_guides/deploying/query_engine/root.md):\n",
    "A query engine is an end-to-end pipeline that allows you to ask questions over your data. It takes in a natural language query, and returns a response, along with reference context retrieved and passed to the LLM.\n",
    "\n",
    "[**Chat Engines**](/module_guides/deploying/chat_engines/root.md):\n",
    "A chat engine is an end-to-end pipeline for having a conversation with your data (multiple back-and-forth instead of a single question-and-answer).\n",
    "\n",
    "[**Agents**](/module_guides/deploying/agents/root.md):\n",
    "An agent is an automated decision-maker powered by an LLM that interacts with the world via a set of [tools](/module_guides/deploying/agents/tools/llamahub_tools_guide.md). Agents can take an arbitrary number of steps to complete a given task, dynamically deciding on the best course of action rather than following pre-determined steps. This gives it additional flexibility to tackle more complex tasks.\n",
    "\n",
    "```{admonition} Next Steps\n",
    "* Tell me how to [customize things](/getting_started/customization.rst)\n",
    "* Continue learning with our [understanding LlamaIndex](/understanding/understanding.md) guide\n",
    "* Ready to dig deep? Check out the module guides on the left\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "# Generated with make text\n",
    "test_data = \"\"\"\n",
    "High-Level Concepts\n",
    "*******************\n",
    "\n",
    "This is a quick guide to the high-level concepts you'll encounter\n",
    "frequently when building LLM applications.\n",
    "\n",
    "Tip:\n",
    "\n",
    "  If you haven't, (install LlamaIndex)[installation.html] and complete\n",
    "  the (starter tutorial)[starter_example.html] before you read this.\n",
    "  It will help ground these steps in your experience.\n",
    "\n",
    "\n",
    "Retrieval Augmented Generation (RAG)\n",
    "====================================\n",
    "\n",
    "LLMs are trained on enormous bodies of data but they aren't trained on\n",
    "**your** data. Retrieval-Augmented Generation (RAG) solves this\n",
    "problem by adding your data to the data LLMs already have access to.\n",
    "You will see references to RAG frequently in this documentation.\n",
    "\n",
    "In RAG, your data is loaded and prepared for queries or \"indexed\".\n",
    "User queries act on the index, which filters your data down to the\n",
    "most relevant context. This context and your query then go to the LLM\n",
    "along with a prompt, and the LLM provides a response.\n",
    "\n",
    "Even if what you're building is a chatbot or an agent, you'll want to\n",
    "know RAG techniques for getting data into your application.\n",
    "\n",
    "[image: ][image]\n",
    "\n",
    "\n",
    "Stages within RAG\n",
    "=================\n",
    "\n",
    "There are five key stages within RAG, which in turn will be a part of\n",
    "any larger application you build. These are:\n",
    "\n",
    "* **Loading**: this refers to getting your data from where it lives --\n",
    "  whether it's text files, PDFs, another website, a database, or an\n",
    "  API -- into your pipeline. (LlamaHub)[https://llamahub.ai/] provides\n",
    "  hundreds of connectors to choose from.\n",
    "\n",
    "* **Indexing**: this means creating a data structure that allows for\n",
    "  querying the data. For LLMs this nearly always means creating\n",
    "  \"vector embeddings\", numerical representations of the meaning of\n",
    "  your data, as well as numerous other metadata strategies to make it\n",
    "  easy to accurately find contextually relevant data.\n",
    "\n",
    "* **Storing**: once your data is indexed you will almost always want\n",
    "  to store your index, as well as other metadata, to avoid having to\n",
    "  re-index it.\n",
    "\n",
    "* **Querying**: for any given indexing strategy there are many ways\n",
    "  you can utilize LLMs and LlamaIndex data structures to query,\n",
    "  including sub-queries, multi-step queries and hybrid strategies.\n",
    "\n",
    "* **Evaluation**: a critical step in any pipeline is checking how\n",
    "  effective it is relative to other strategies, or when you make\n",
    "  changes. Evaluation provides objective measures of how accurate,\n",
    "  faithful and fast your responses to queries are.\n",
    "\n",
    "[image: ][image]\n",
    "\n",
    "\n",
    "Important concepts within each step\n",
    "===================================\n",
    "\n",
    "There are also some terms you'll encounter that refer to steps within\n",
    "each of these stages.\n",
    "\n",
    "\n",
    "Loading stage\n",
    "-------------\n",
    "\n",
    "(Nodes and Documents)[../module_guides/loading/documents_and_nodes/ro\n",
    "ot.html]**Nodes and Documents**: A \"Document\" is a container around\n",
    "any data source - for instance, a PDF, an API output, or retrieve data\n",
    "from a database. A \"Node\" is the atomic unit of data in LlamaIndex and\n",
    "represents a \"chunk\" of a source \"Document\". Nodes have metadata that\n",
    "relate them to the document they are in and to other nodes.\n",
    "\n",
    "(Connectors)[../module_guides/loading/connector/root.html]**Connector\n",
    "s**: A data connector (often called a \"Reader\") ingests data from\n",
    "different data sources and data formats into \"Documents\" and \"Nodes\".\n",
    "\n",
    "\n",
    "Indexing Stage\n",
    "--------------\n",
    "\n",
    "(Indexes)[../module_guides/indexing/indexing.html]**Indexes**: Once\n",
    "you've ingested your data, LlamaIndex will help you index the data\n",
    "into a structure that's easy to retrieve. This usually involves\n",
    "generating \"vector embeddings\" which are stored in a specialized\n",
    "database called a \"vector store\". Indexes can also store a variety of\n",
    "metadata about your data.\n",
    "\n",
    "(Embeddings)[../module_guides/models/embeddings.html]**Embeddings**\n",
    "LLMs generate numerical representations of data called \"embeddings\".\n",
    "When filtering your data for relevance, LlamaIndex will convert\n",
    "queries into embeddings, and your vector store will find data that is\n",
    "numerically similar to the embedding of your query.\n",
    "\n",
    "\n",
    "Querying Stage\n",
    "--------------\n",
    "\n",
    "(Retrievers)[../module_guides/querying/retriever/root.html]**Retrieve\n",
    "rs**: A retriever defines how to efficiently retrieve relevant context\n",
    "from an index when given a query. Your retrieval strategy is key to\n",
    "the relevancy of the data retrieved and the efficiency with which it's\n",
    "done.\n",
    "\n",
    "(Routers)[../module_guides/querying/router/root.html]**Routers**: A\n",
    "router determines which retriever will be used to retrieve relevant\n",
    "context from the knowledge base. More specifically, the\n",
    "\"RouterRetriever\"¬†class, is responsible for selecting one or multiple\n",
    "candidate retrievers to execute a query. They use a selector to choose\n",
    "the best option based on each candidate's metadata and the query.\n",
    "\n",
    "(Node Postprocessors)[../module_guides/querying/node_postprocessors/r\n",
    "oot.html]**Node Postprocessors**: A node postprocessor takes in a set\n",
    "of retrieved nodes and applies transformations, filtering, or re-\n",
    "ranking logic to them.\n",
    "\n",
    "(Response Synthesizers)[../module_guides/querying/response_synthesize\n",
    "rs/root.html]**Response Synthesizers**: A response synthesizer\n",
    "generates a response from an LLM, using a user query and a given set\n",
    "of retrieved text chunks.\n",
    "\n",
    "\n",
    "Putting it all together\n",
    "-----------------------\n",
    "\n",
    "There are endless use cases for data-backed LLM applications but they\n",
    "can be roughly grouped into three categories:\n",
    "\n",
    "(Query\n",
    "Engines)[../module_guides/deploying/query_engine/root.html]**Query\n",
    "Engines**: A query engine is an end-to-end pipeline that allows you to\n",
    "ask questions over your data. It takes in a natural language query,\n",
    "and returns a response, along with reference context retrieved and\n",
    "passed to the LLM.\n",
    "\n",
    "(Chat\n",
    "Engines)[../module_guides/deploying/chat_engines/root.html]**Chat\n",
    "Engines**: A chat engine is an end-to-end pipeline for having a\n",
    "conversation with your data (multiple back-and-forth instead of a\n",
    "single question-and-answer).\n",
    "\n",
    "(Agents)[../module_guides/deploying/agents/root.html]**Agents**: An\n",
    "agent is an automated decision-maker powered by an LLM that interacts\n",
    "with the world via a set of (tools)[../module_guides/deploying/agents\n",
    "/tools/llamahub_tools_guide.html]. Agents can take an arbitrary number\n",
    "of steps to complete a given task, dynamically deciding on the best\n",
    "course of action rather than following pre-determined steps. This\n",
    "gives it additional flexibility to tackle more complex tasks.\n",
    "\n",
    "Next Steps:\n",
    "\n",
    "* Tell me how to (customize things)[customization.html]\n",
    "\n",
    "* Continue learning with our (understanding\n",
    "  LlamaIndex)[../understanding/understanding.html] guide\n",
    "\n",
    "* Ready to dig deep? Check out the module guides on the left\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Generated by overriding Sphinx's Text Translator\n",
    "metadata = \"\"\"&1|High-Level Concepts\n",
    "&2|Retrieval Augmented Generation (RAG) &2|Stages within RAG\n",
    "&2|Important concepts within each step &3|Loading stage &3|Indexing\n",
    "Stage &3|Querying Stage &3|Putting it all together\n",
    "\"\"\"\n",
    "\n",
    "def clean_data(metadata):\n",
    "  metadata = metadata.replace(\"\\n\\n\", \"<br>\")\n",
    "  metadata = metadata.replace(\"\\n\", \" \")\n",
    "  metadata = metadata.replace(\"<br>\", \"\\n\\n\")\n",
    "  return metadata\n",
    "\n",
    "print(clean_data(metadata))\n",
    "print(clean_data(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "from llama_index.node_parser import MarkdownNodeParser\n",
    "parser = MarkdownNodeParser()\n",
    "\n",
    "nodes = parser.get_nodes_from_documents([Document(text=raw_md_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextNode(id_='696b3006-e229-498e-aa84-0d04b06df9f3', embedding=None, metadata={'Header 1': 'High-Level Concepts'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='52fed0f2-4ba0-486e-8b5e-e715e1143f3c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='b203f3534d65f7865e5cb4ba9e3ed52f609c454bf795f00f7bfcb2c1ac761d27'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='d8c0ad37-b451-4fd0-8394-f12392048841', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='9dfd4b3e-04a3-43a6-8fa1-5abc2ba3fd14', node_type=<ObjectType.TEXT: '1'>, metadata={'Header 1': 'High-Level Concepts', 'Header 2': 'Retrieval Augmented Generation (RAG)'}, hash='678f12bf6a73c517c7d3dc9b67c78af1347fd26d1959f2441186d2b3c75da49b')}, text=\"High-Level Concepts\\n\\nThis is a quick guide to the high-level concepts you'll encounter frequently when building LLM applications.\\n\\n```{tip}\\nIf you haven't, [install LlamaIndex](/getting_started/installation.md) and complete the [starter tutorial](/getting_started/starter_example.md) before you read this. It will help ground these steps in your experience.\\n```\", start_char_idx=3, end_char_idx=364, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan 1/Baseline: MultiDocument Agents\n",
    "* https://docs.llamaindex.ai/en/stable/examples/agent/multi_document_agents.html\n",
    "* https://github.com/cobusgreyling/LlamaIndex/blob/13b60af90119a4ee91389fa1be53f90b814c0ffa/Agentic_RAG_Multi_Document_Agents-v1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SummaryIndex,\n",
    "    SimpleKeywordTableIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    ")\n",
    "from llama_index.schema import IndexNode\n",
    "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.llms import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm)\n",
    "# sample_folder = \"/Users/sasha/github/LlamaIndex/llama_index/docs/_build/text/getting_started\"\n",
    "sample_folder = \"/Users/sasha/github/LlamaIndex/llama_index/docs/_build/text/optimizing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Multi-Document Agents\n",
    "\n",
    "In this section we show you how to construct the multi-document agent. We first build a document agent for each document, and then define the top-level parent agent with an object index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SummaryIndex\n",
    "import glob\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent import OpenAIAgent\n",
    "from llama_index import load_index_from_storage, StorageContext\n",
    "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.node_parser import SentenceSplitter\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "\n",
    "async def build_agent_per_doc(nodes, file_base):\n",
    "    print(file_base)\n",
    "\n",
    "    vi_out_path = f\"./data/llamaindex_docs/{file_base}\"\n",
    "    summary_out_path = f\"./data/llamaindex_docs/{file_base}_summary.pkl\"\n",
    "    if not os.path.exists(vi_out_path):\n",
    "        Path(\"./data/llamaindex_docs/\").mkdir(parents=True, exist_ok=True)\n",
    "        # build vector index\n",
    "        vector_index = VectorStoreIndex(nodes, service_context=service_context)\n",
    "        vector_index.storage_context.persist(persist_dir=vi_out_path)\n",
    "    else:\n",
    "        vector_index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=vi_out_path),\n",
    "            service_context=service_context,\n",
    "        )\n",
    "\n",
    "    # build summary index\n",
    "    summary_index = SummaryIndex(nodes, service_context=service_context)\n",
    "\n",
    "    # define query engines\n",
    "    vector_query_engine = vector_index.as_query_engine()\n",
    "    summary_query_engine = summary_index.as_query_engine(\n",
    "        response_mode=\"tree_summarize\"\n",
    "    )\n",
    "\n",
    "    # extract a summary\n",
    "    if not os.path.exists(summary_out_path):\n",
    "        Path(summary_out_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        summary = str(\n",
    "            await summary_query_engine.aquery(\n",
    "                \"Extract a concise 1-2 line summary of this document\"\n",
    "            )\n",
    "        )\n",
    "        pickle.dump(summary, open(summary_out_path, \"wb\"))\n",
    "    else:\n",
    "        summary = pickle.load(open(summary_out_path, \"rb\"))\n",
    "\n",
    "    # define tools\n",
    "    query_engine_tools = [\n",
    "        QueryEngineTool(\n",
    "            query_engine=vector_query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=f\"vector_tool_{file_base}\",\n",
    "                description=f\"Useful for questions related to specific facts\",\n",
    "            ),\n",
    "        ),\n",
    "        QueryEngineTool(\n",
    "            query_engine=summary_query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=f\"summary_tool_{file_base}\",\n",
    "                description=f\"Useful for summarization questions\",\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # build agent\n",
    "    function_llm = OpenAI(model=\"gpt-4\")\n",
    "    agent = OpenAIAgent.from_tools(\n",
    "        query_engine_tools,\n",
    "        llm=function_llm,\n",
    "        verbose=True,\n",
    "        system_prompt=f\"\"\"\\\n",
    "You are a specialized agent designed to answer queries about the `{file_base}.txt` part of the LlamaIndex docs.\n",
    "You must ALWAYS use at least one of the tools provided when answering a question; do NOT rely on prior knowledge.\\\n",
    "\"\"\",\n",
    "    )\n",
    "\n",
    "    return agent, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def build_agents(docs):\n",
    "    node_parser = SentenceSplitter()\n",
    "\n",
    "    # Build agents dictionary\n",
    "    agents_dict = {}\n",
    "    extra_info_dict = {}\n",
    "\n",
    "    # # this is for the baseline\n",
    "    # all_nodes = []\n",
    "\n",
    "    for idx, doc in enumerate(tqdm(docs)):\n",
    "        nodes = node_parser.get_nodes_from_documents([doc])\n",
    "        # all_nodes.extend(nodes)\n",
    "\n",
    "        # ID will be base + parent\n",
    "        file_path = Path(doc.metadata[\"path\"])\n",
    "        file_base = str(file_path.parent.stem) + \"_\" + str(file_path.stem)\n",
    "        agent, summary = await build_agent_per_doc(nodes, file_base)\n",
    "\n",
    "        agents_dict[file_base] = agent\n",
    "        print(f\"Summary for {file_base}: {summary}\")\n",
    "        extra_info_dict[file_base] = {\"summary\": summary, \"nodes\": nodes}\n",
    "\n",
    "    return agents_dict, extra_info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx 0/8\n",
      "/Users/sasha/github/LlamaIndex/llama_index/docs/_build/text/optimizing/agentic_strategies/agentic_strategies.txt\n",
      "Idx 1/8\n",
      "/Users/sasha/github/LlamaIndex/llama_index/docs/_build/text/optimizing/basic_strategies/basic_strategies.txt\n",
      "Idx 2/8\n",
      "/Users/sasha/github/LlamaIndex/llama_index/docs/_build/text/optimizing/fine-tuning/fine-tuning.txt\n",
      "Idx 3/8\n",
      "/Users/sasha/github/LlamaIndex/llama_index/docs/_build/text/optimizing/advanced_retrieval/query_transformations.txt\n",
      "Idx 4/8\n",
      "/Users/sasha/github/LlamaIndex/llama_index/docs/_build/text/optimizing/advanced_retrieval/advanced_retrieval.txt\n",
      "Idx 5/8\n",
      "/Users/sasha/github/LlamaIndex/llama_index/docs/_build/text/optimizing/evaluation/component_wise_evaluation.txt\n",
      "Idx 6/8\n",
      "/Users/sasha/github/LlamaIndex/llama_index/docs/_build/text/optimizing/evaluation/evaluation.txt\n",
      "Idx 7/8\n",
      "/Users/sasha/github/LlamaIndex/llama_index/docs/_build/text/optimizing/evaluation/e2e_evaluation.txt\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "doc_limit = 100\n",
    "all_files = glob.glob(f\"{sample_folder}/**/*\")\n",
    "for idx, f in enumerate(all_files):\n",
    "    if idx > doc_limit:\n",
    "        break\n",
    "    print(f\"Idx {idx}/{len(all_files)}\")\n",
    "    loaded_docs = SimpleDirectoryReader(\n",
    "        input_files=[f],\n",
    "        file_metadata = lambda f: {\"path\": str(f)}\n",
    "    ).load_data()\n",
    "    assert len(loaded_docs) == 1\n",
    "    print(loaded_docs[0].metadata[\"path\"])\n",
    "    docs.append(loaded_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='a2e6c147-fc18-488a-8f1d-10a8eaeeaaac', embedding=None, metadata={'path': '/Users/sasha/github/LlamaIndex/llama_index/docs/_build/text/optimizing/evaluation/e2e_evaluation.txt'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"End-to-End Evaluation\\n*********************\\n\\nEnd-to-End evaluation should be the guiding signal for your RAG\\napplication - will my pipeline generate the right responses given the\\ndata sources and a set of queries?\\n\\nWhile it helps initially to individually inspect queries and\\nresponses, as you deal with more failure and corner cases, it may stop\\nbeing feasible to look at each query individually, and rather it may\\nhelp instead to define a set of summary metrics or automated\\nevaluation, and gain an intuition for what they might be telling you\\nand where you might dive deeper.\\n\\n\\nSetting up an Evaluation Set\\n============================\\n\\nIt is helpful to start off with a small but diverse set of queries,\\nand build up more examples as one discovers problematic queries or\\ninteractions.\\n\\nWe've created some tools that automatically generate a dataset for you\\ngiven a set of documents to query. (See example below).\\n\\n* (QuestionGeneration)[../../examples/evaluation/QuestionGeneration.h\\n  tml]\\n\\nIn the future, we will also be able to create datasets automatically\\nagainst tools.\\n\\n\\nThe Spectrum of Evaluation Options\\n==================================\\n\\nQuantitative eval is more useful when evaluating applications where\\nthere is a correct answer - for instance, validating that the choice\\nof tools and their inputs are correct given the plan, or retrieving\\nspecific pieces of information, or attempting to produce intermediate\\noutput of a certain schema (e.g. JSON fields).\\n\\nQualitative eval is more useful when generating long-form responses\\nthat are meant to be *helpful* but not necessarily completely\\naccurate.\\n\\nThere is a spectrum of evaluation options ranging from metrics,\\ncheaper models, more expensive models (GPT4), and human evaluation.\\n\\nBelow is some example usage of the (evaluation\\nmodules)[evaluation.html]:\\n\\n* (BatchEvalRunner - Running Multiple\\n  Evaluations)[../../examples/evaluation/batch_eval.html]\\n\\n* (Correctness\\n  Evaluator)[../../examples/evaluation/correctness_eval.html]\\n\\n* (Faithfulness\\n  Evaluator)[../../examples/evaluation/faithfulness_eval.html]\\n\\n* (Guideline Evaluator)[../../examples/evaluation/guideline_eval.html]\\n\\n* (Pairwise Evaluator)[../../examples/evaluation/pairwise_eval.html]\\n\\n* (Relevancy Evaluator)[../../examples/evaluation/relevancy_eval.html]\\n\\n* (Embedding Similarity\\n  Evaluator)[../../examples/evaluation/semantic_similarity_eval.html]\\n\\n\\nDiscovery - Sensitivity Testing\\n===============================\\n\\nWith a complex pipeline, it may be unclear which parts of the pipeline\\nare affecting your results.\\n\\nSensitivity testing can be a good inroad into choosing which\\ncomponents to individually test or tweak more thoroughly, or which\\nparts of your dataset (e.g. queries) may be producing problematic\\nresults.\\n\\nMore details on how to discover issues automatically with methods such\\nas sensitivity testing will come soon.\\n\\nExamples of this in the more traditional ML domain include\\n(Giskard)[https://docs.giskard.ai/en/latest/getting-\\nstarted/quickstart.html].\\n\\n\\nMetrics Ensembling\\n==================\\n\\nIt may be expensive to use GPT-4 to carry out evaluation especially as\\nyour dev set grows large.\\n\\nMetrics ensembling uses an ensemble of weaker signals (exact match,\\nF1, ROUGE, BLEU, BERT-NLI and BERT-similarity) to predict the output\\nof a more expensive evaluation methods that are closer to the gold\\nlabels (human-labelled/GPT-4).\\n\\nIt is intended for two purposes:\\n\\n1. Evaluating changes cheaply and quickly across a large dataset\\n   during the development stage.\\n\\n2. Flagging outliers for further evaluation (GPT-4 / human alerting)\\n   during the production monitoring stage.\\n\\nWe also want the metrics ensembling to be interpretable - the\\ncorrelation and weighting scores should give an indication of which\\nmetrics best capture the evaluation criteria.\\n\\nWe will discuss more about the methodology in future updates.\\n\\n&1|End-to-End Evaluation &2|Setting up an Evaluation Set &2|The\\nSpectrum of Evaluation Options &2|Discovery - Sensitivity Testing\\n&2|Metrics Ensembling\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "439eb0780c9b4346b23ff13f8bfc3462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agentic_strategies_agentic_strategies\n",
      "Summary for agentic_strategies_agentic_strategies: This document discusses agentic strategies in the context of the LlamaIndex RAG pipeline. It covers simpler agentic strategies such as routing and query transformations, as well as more advanced strategies involving data agents. The document also provides example guides on building OpenAI agents with query engine tools and retrieval augmentation.\n",
      "basic_strategies_basic_strategies\n",
      "Summary for basic_strategies_basic_strategies: This document provides strategies for optimizing the performance of the RAG pipeline, including prompt engineering, choosing the right embedding model, customizing chunk sizes, implementing hybrid search, using metadata filters, and utilizing multi-tenancy in RAG systems.\n",
      "fine-tuning_fine-tuning\n",
      "Summary for fine-tuning_fine-tuning: This document provides an overview of fine-tuning, including its benefits for embedding models and LLMs, integrations with LlamaIndex, and specific use cases such as text-to-SQL and knowledge distillation.\n",
      "advanced_retrieval_query_transformations\n",
      "Summary for advanced_retrieval_query_transformations: This document provides information about query transformations in LlamaIndex. It explains what query transformations are, their use cases, and provides examples of different types of query transformations such as HyDE (Hypothetical Document Embeddings), single-step query decomposition, and multi-step query transformations.\n",
      "advanced_retrieval_advanced_retrieval\n",
      "Summary for advanced_retrieval_advanced_retrieval: This document provides information on advanced retrieval strategies, including reranking, recursive retrieval, embedded tables, small-to-big retrieval, and more. It also discusses query transformations and composable retrievers. Additionally, it provides links to third-party resources on advanced retrieval strategies.\n",
      "evaluation_component_wise_evaluation\n",
      "Summary for evaluation_component_wise_evaluation: This document discusses the importance of component-wise evaluation in pipeline optimization and suggests utilizing public benchmarks such as the BEIR dataset for evaluating retrieval models and the HotpotQA dataset for evaluating query engine components.\n",
      "evaluation_evaluation\n",
      "Summary for evaluation_evaluation: This document provides an overview of the evaluation process for LlamaIndex, including strategies, metrics, and resources for both end-to-end and component-wise evaluation.\n",
      "evaluation_e2e_evaluation\n",
      "Summary for evaluation_e2e_evaluation: This document discusses the importance of end-to-end evaluation in RAG applications and provides guidance on setting up an evaluation set, exploring different evaluation options, conducting sensitivity testing, and using metrics ensembling for evaluation purposes.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "agents_dict, extra_info_dict = await build_agents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool_agentic_strategies_agentic_strategies\n",
      "tool_basic_strategies_basic_strategies\n",
      "tool_fine_tuning_fine_tuning\n",
      "tool_advanced_retrieval_query_transformations\n",
      "tool_advanced_retrieval_advanced_retrieval\n",
      "tool_evaluation_component_wise_evaluation\n",
      "tool_evaluation_evaluation\n",
      "tool_evaluation_e2e_evaluation\n"
     ]
    }
   ],
   "source": [
    "# define tool for each document agent\n",
    "all_tools = []\n",
    "for file_base, agent in agents_dict.items():\n",
    "    summary = extra_info_dict[file_base][\"summary\"]\n",
    "    \n",
    "    if \"-\" in file_base:\n",
    "        file_base = file_base.replace('-', '_')\n",
    "\n",
    "    doc_tool = QueryEngineTool(\n",
    "        query_engine=agent,\n",
    "        metadata=ToolMetadata(\n",
    "            name=f\"tool_{file_base}\",\n",
    "            description=summary,\n",
    "        ),\n",
    "    )\n",
    "    print(f\"tool_{file_base}\")\n",
    "    all_tools.append(doc_tool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToolMetadata(description='This document discusses agentic strategies in the context of the LlamaIndex RAG pipeline. It covers simpler agentic strategies such as routing and query transformations, as well as more advanced strategies involving data agents. The document also provides example guides on building OpenAI agents with query engine tools and retrieval augmentation.', name='tool_agentic_strategies_agentic_strategies', fn_schema=<class 'llama_index.tools.types.DefaultToolFnSchema'>)\n"
     ]
    }
   ],
   "source": [
    "print(all_tools[0].metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index.objects import (\n",
    "    ObjectIndex,\n",
    "    SimpleToolNodeMapping,\n",
    "    ObjectRetriever,\n",
    ")\n",
    "from llama_index.retrievers import BaseRetriever\n",
    "from llama_index.postprocessor import CohereRerank\n",
    "from llama_index.tools import QueryPlanTool\n",
    "from llama_index.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-4-0613\")\n",
    "\n",
    "tool_mapping = SimpleToolNodeMapping.from_objects(all_tools)\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    tool_mapping,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "vector_node_retriever = obj_index.as_node_retriever(similarity_top_k=3)\n",
    "\n",
    "\n",
    "# define a custom retriever with reranking\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    def __init__(self, vector_retriever, postprocessor=None):\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._postprocessor = postprocessor or CohereRerank(top_n=5)\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle):\n",
    "        retrieved_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "        filtered_nodes = self._postprocessor.postprocess_nodes(\n",
    "            retrieved_nodes, query_bundle=query_bundle\n",
    "        )\n",
    "\n",
    "        return filtered_nodes\n",
    "\n",
    "# define a custom object retriever that adds in a query planning tool\n",
    "class CustomObjectRetriever(ObjectRetriever):\n",
    "    def __init__(self, retriever, object_node_mapping, all_tools, llm=None):\n",
    "        self._retriever = retriever\n",
    "        self._object_node_mapping = object_node_mapping\n",
    "        self._llm = llm or OpenAI(\"gpt-4-0613\")\n",
    "\n",
    "    def retrieve(self, query_bundle):\n",
    "        nodes = self._retriever.retrieve(query_bundle)\n",
    "        tools = [self._object_node_mapping.from_node(n.node) for n in nodes]\n",
    "\n",
    "        sub_question_sc = ServiceContext.from_defaults(llm=self._llm)\n",
    "        sub_question_engine = SubQuestionQueryEngine.from_defaults(\n",
    "            query_engine_tools=tools, service_context=sub_question_sc\n",
    "        )\n",
    "        sub_question_description = f\"\"\"\\\n",
    "Useful for any queries that involve comparing multiple documents. ALWAYS use this tool for comparison queries - make sure to call this \\\n",
    "tool with the original query. Do NOT use the other tools for any queries involving multiple documents.\n",
    "\"\"\"\n",
    "        sub_question_tool = QueryEngineTool(\n",
    "            query_engine=sub_question_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"compare_tool\", description=sub_question_description\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        return tools + [sub_question_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_node_retriever = CustomRetriever(vector_node_retriever)\n",
    "\n",
    "# wrap it with ObjectRetriever to return objects\n",
    "custom_obj_retriever = CustomObjectRetriever(\n",
    "    custom_node_retriever, tool_mapping, all_tools, llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_query_engine': <llama_index.agent.openai.base.OpenAIAgent at 0x1672e9b90>,\n",
       " '_metadata': ToolMetadata(description='This document discusses agentic strategies in the context of the LlamaIndex RAG pipeline. It covers simpler agentic strategies such as routing and query transformations, as well as more advanced strategies involving data agents. The document also provides example guides on building OpenAI agents with query engine tools and retrieval augmentation.', name='tool_agentic_strategies_agentic_strategies', fn_schema=<class 'llama_index.tools.types.DefaultToolFnSchema'>),\n",
       " '_resolve_input_errors': True}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmps = custom_obj_retriever.retrieve(\"Agents\")\n",
    "tmps[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent import ReActAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_agent = ReActAgent.from_tools(\n",
    "     tool_retriever=custom_obj_retriever,\n",
    "     system_prompt=\"\"\" \\\n",
    " You are an agent designed to answer queries about the documentation.\n",
    " Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    " \"\"\",\n",
    "     llm=llm,\n",
    "     verbose=True,\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = [\n",
    "    n for extra_info in extra_info_dict.values() for n in extra_info[\"nodes\"]\n",
    "]\n",
    "base_index = VectorStoreIndex(all_nodes)\n",
    "base_query_engine = base_index.as_query_engine(similarity_top_k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"Which section should I look into to find out how to build Agent Based RAG pipeline?\" # the plain question/answer did a much better job then this complicated format\n",
    "# question = \"How to install llamaindex if I am working with supabase\" # the plain question/answer did a much better job then this complicated format\n",
    "# question = \"Which was described first? Set your OpenAI API key or Download data?\" # the plain question/answer did a much better job then this complicated format, React was just wrong, screenshotted, tree_summarizer was the reason\n",
    "\n",
    "# question = \"When would one pefer finetuning vs using one of the advanced retrieval strategies?\"\n",
    "# question = \"What are the few ways to do query transformations?\"\n",
    "question = \"What is the difference between query engine tools vs query planning, can you demonstrate the difference in code?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: To explain the difference between query engine tools and query planning, I can provide a code example that demonstrates each concept.\n",
      "Action: tool_advanced_retrieval_advanced_retrieval\n",
      "Action Input: {'input': 'query engine tools'}\n",
      "\u001b[0mAdded user message to memory: query engine tools\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_advanced_retrieval_advanced_retrieval with args: {\n",
      "  \"input\": \"query engine tools\"\n",
      "}\n",
      "Got output: The context information does not provide any specific information about query engine tools.\n",
      "========================\n",
      "\n",
      "\u001b[1;3;34mObservation: I'm sorry, but the `advanced_retrieval_advanced_retrieval.txt` part of the LlamaIndex docs does not provide any specific information about query engine tools.\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: I apologize for the confusion. It seems that the tool I used does not provide the specific information about query engine tools. Let me try a different approach to explain the difference between query engine tools and query planning.\n",
      "Action: tool_advanced_retrieval_query_transformations\n",
      "Action Input: {'input': 'query engine tools'}\n",
      "\u001b[0mAdded user message to memory: query engine tools\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_advanced_retrieval_query_transformations with args: {\n",
      "  \"input\": \"query engine tools\"\n",
      "}\n",
      "Got output: LlamaIndex provides query engine tools that allow users to transform and execute queries against index structures. These query transformations can be single-step or multi-step. Single-step query transformations are run once before the query is executed against an index, while multi-step query transformations involve transforming and executing subsequent queries in a sequential fashion. These query transformations have various use cases, such as transforming queries into a form that can be more easily embedded or breaking down complex queries into smaller subquestions. LlamaIndex offers specific query transformation modules, such as the HyDE (Hypothetical Document Embeddings) query transform and the single-step query decomposition transform. These tools enable users to enhance the performance of their queries and obtain more accurate and relevant results from their index structures.\n",
      "========================\n",
      "\n",
      "\u001b[1;3;34mObservation: LlamaIndex provides query engine tools that allow users to transform and execute queries against index structures. These query transformations can be single-step or multi-step. Single-step query transformations are run once before the query is executed against an index, while multi-step query transformations involve transforming and executing subsequent queries in a sequential fashion. \n",
      "\n",
      "These query transformations have various use cases, such as transforming queries into a form that can be more easily embedded or breaking down complex queries into smaller subquestions. LlamaIndex offers specific query transformation modules, such as the HyDE (Hypothetical Document Embeddings) query transform and the single-step query decomposition transform. These tools enable users to enhance the performance of their queries and obtain more accurate and relevant results from their index structures.\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not parse output: Thought: Now that we have an understanding of query engine tools and their use cases, let's move on to query planning. Query planning involves the process of determining the most efficient way to execute a query. It includes tasks such as query optimization, cost estimation, and selecting the appropriate execution plan.\n\nTo demonstrate the difference between query engine tools and query planning, let's consider a code example:\n\n```python\n# Query Engine Tools\ndef preprocess_query(query):\n    # Perform query transformations\n    transformed_query = hyde_transform(query)\n    return transformed_query\n\ndef execute_query(query):\n    # Execute the transformed query against the index\n    results = index.search(query)\n    return results\n\n# Query Planning\ndef plan_query(query):\n    # Perform query planning tasks\n    optimized_query = optimize_query(query)\n    estimated_cost = estimate_cost(optimized_query)\n    execution_plan = select_execution_plan(optimized_query)\n    return execution_plan\n\ndef execute_plan(execution_plan):\n    # Execute the query execution plan\n    results = execute(execution_plan)\n    return results\n\n# Example usage\nquery = \"What are the top restaurants in New York City?\"\ntransformed_query = preprocess_query(query)\nresults = execute_query(transformed_query)\n\nexecution_plan = plan_query(query)\nresults = execute_plan(execution_plan)\n```\n\nIn this code example, the `preprocess_query` function represents the query engine tools. It takes a query as input, performs query transformations using the HyDE transform, and returns the transformed query. The `execute_query` function then executes the transformed query against the index and returns the results.\n\nOn the other hand, the `plan_query` function represents the query planning process. It takes a query as input and performs query planning tasks such as query optimization, cost estimation, and selecting the appropriate execution plan. The `execute_plan` function then executes the query execution plan and returns the results.\n\nIn summary, query engine tools focus on transforming and executing queries against index structures, while query planning involves determining the most efficient way to execute a query by optimizing, estimating costs, and selecting execution plans.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/github/LlamaIndex/llama_index/llama_index/agent/react/step.py:201\u001b[0m, in \u001b[0;36mReActAgentWorker._extract_reasoning_step\u001b[0;34m(self, output, is_streaming)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     reasoning_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_output_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_streaming\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/github/LlamaIndex/llama_index/llama_index/agent/react/output_parser.py:109\u001b[0m, in \u001b[0;36mReActOutputParser.parse\u001b[0;34m(self, output, is_streaming)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parse_action_reasoning_step(output)\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not parse output: Thought: Now that we have an understanding of query engine tools and their use cases, let's move on to query planning. Query planning involves the process of determining the most efficient way to execute a query. It includes tasks such as query optimization, cost estimation, and selecting the appropriate execution plan.\n\nTo demonstrate the difference between query engine tools and query planning, let's consider a code example:\n\n```python\n# Query Engine Tools\ndef preprocess_query(query):\n    # Perform query transformations\n    transformed_query = hyde_transform(query)\n    return transformed_query\n\ndef execute_query(query):\n    # Execute the transformed query against the index\n    results = index.search(query)\n    return results\n\n# Query Planning\ndef plan_query(query):\n    # Perform query planning tasks\n    optimized_query = optimize_query(query)\n    estimated_cost = estimate_cost(optimized_query)\n    execution_plan = select_execution_plan(optimized_query)\n    return execution_plan\n\ndef execute_plan(execution_plan):\n    # Execute the query execution plan\n    results = execute(execution_plan)\n    return results\n\n# Example usage\nquery = \"What are the top restaurants in New York City?\"\ntransformed_query = preprocess_query(query)\nresults = execute_query(transformed_query)\n\nexecution_plan = plan_query(query)\nresults = execute_plan(execution_plan)\n```\n\nIn this code example, the `preprocess_query` function represents the query engine tools. It takes a query as input, performs query transformations using the HyDE transform, and returns the transformed query. The `execute_query` function then executes the transformed query against the index and returns the results.\n\nOn the other hand, the `plan_query` function represents the query planning process. It takes a query as input and performs query planning tasks such as query optimization, cost estimation, and selecting the appropriate execution plan. The `execute_plan` function then executes the query execution plan and returns the results.\n\nIn summary, query engine tools focus on transforming and executing queries against index structures, while query planning involves determining the most efficient way to execute a query by optimizing, estimating costs, and selecting execution plans.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mtop_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m   \u001b[49m\u001b[43mquestion\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/LlamaIndex/llama_index/llama_index/core/base_query_engine.py:40\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     39\u001b[0m     str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/LlamaIndex/llama_index/llama_index/callbacks/utils.py:41\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m cast(CallbackManager, callback_manager)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mas_trace(trace_id):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/LlamaIndex/llama_index/llama_index/agent/types.py:36\u001b[0m, in \u001b[0;36mBaseAgent._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;129m@trace_method\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_bundle: QueryBundle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RESPONSE_TYPE:\n\u001b[0;32m---> 36\u001b[0m     agent_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m     41\u001b[0m         response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(agent_response), source_nodes\u001b[38;5;241m=\u001b[39magent_response\u001b[38;5;241m.\u001b[39msource_nodes\n\u001b[1;32m     42\u001b[0m     )\n",
      "File \u001b[0;32m~/github/LlamaIndex/llama_index/llama_index/callbacks/utils.py:41\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m cast(CallbackManager, callback_manager)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mas_trace(trace_id):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/LlamaIndex/llama_index/llama_index/agent/runner/base.py:497\u001b[0m, in \u001b[0;36mAgentRunner.chat\u001b[0;34m(self, message, chat_history, tool_choice)\u001b[0m\n\u001b[1;32m    492\u001b[0m     tool_choice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_tool_choice\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    494\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mAGENT_STEP,\n\u001b[1;32m    495\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mMESSAGES: [message]},\n\u001b[1;32m    496\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 497\u001b[0m     chat_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatResponseMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWAIT\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chat_response, AgentChatResponse)\n\u001b[1;32m    501\u001b[0m     e\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mRESPONSE: chat_response})\n",
      "File \u001b[0;32m~/github/LlamaIndex/llama_index/llama_index/agent/runner/base.py:442\u001b[0m, in \u001b[0;36mAgentRunner._chat\u001b[0;34m(self, message, chat_history, tool_choice, mode)\u001b[0m\n\u001b[1;32m    439\u001b[0m result_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;66;03m# pass step queue in as argument, assume step executor is stateless\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m     cur_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cur_step_output\u001b[38;5;241m.\u001b[39mis_last:\n\u001b[1;32m    447\u001b[0m         result_output \u001b[38;5;241m=\u001b[39m cur_step_output\n",
      "File \u001b[0;32m~/github/LlamaIndex/llama_index/llama_index/agent/runner/base.py:304\u001b[0m, in \u001b[0;36mAgentRunner._run_step\u001b[0;34m(self, task_id, step, mode, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# TODO: figure out if you can dynamically swap in different step executors\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# not clear when you would do that by theoretically possible\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m ChatResponseMode\u001b[38;5;241m.\u001b[39mWAIT:\n\u001b[0;32m--> 304\u001b[0m     cur_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m ChatResponseMode\u001b[38;5;241m.\u001b[39mSTREAM:\n\u001b[1;32m    306\u001b[0m     cur_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_worker\u001b[38;5;241m.\u001b[39mstream_step(step, task, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/github/LlamaIndex/llama_index/llama_index/callbacks/utils.py:41\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m cast(CallbackManager, callback_manager)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mas_trace(trace_id):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/LlamaIndex/llama_index/llama_index/agent/react/step.py:608\u001b[0m, in \u001b[0;36mReActAgentWorker.run_step\u001b[0;34m(self, step, task, **kwargs)\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;129m@trace_method\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_step\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, step: TaskStep, task: Task, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TaskStepOutput:\n\u001b[1;32m    607\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run step.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/LlamaIndex/llama_index/llama_index/agent/react/step.py:424\u001b[0m, in \u001b[0;36mReActAgentWorker._run_step\u001b[0;34m(self, step, task)\u001b[0m\n\u001b[1;32m    422\u001b[0m chat_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm\u001b[38;5;241m.\u001b[39mchat(input_chat)\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# given react prompt outputs, call tools or return response\u001b[39;00m\n\u001b[0;32m--> 424\u001b[0m reasoning_steps, is_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_actions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat_response\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m task\u001b[38;5;241m.\u001b[39mextra_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_reasoning\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mextend(reasoning_steps)\n\u001b[1;32m    428\u001b[0m agent_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_response(\n\u001b[1;32m    429\u001b[0m     task\u001b[38;5;241m.\u001b[39mextra_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_reasoning\u001b[39m\u001b[38;5;124m\"\u001b[39m], task\u001b[38;5;241m.\u001b[39mextra_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msources\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    430\u001b[0m )\n",
      "File \u001b[0;32m~/github/LlamaIndex/llama_index/llama_index/agent/react/step.py:227\u001b[0m, in \u001b[0;36mReActAgentWorker._process_actions\u001b[0;34m(self, task, tools, output, is_streaming)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_process_actions\u001b[39m(\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    219\u001b[0m     task: Task,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m     is_streaming: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    223\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[BaseReasoningStep], \u001b[38;5;28mbool\u001b[39m]:\n\u001b[1;32m    224\u001b[0m     tools_dict: Dict[\u001b[38;5;28mstr\u001b[39m, AsyncBaseTool] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    225\u001b[0m         tool\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget_name(): tool \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m tools\n\u001b[1;32m    226\u001b[0m     }\n\u001b[0;32m--> 227\u001b[0m     _, current_reasoning, is_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_reasoning_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_streaming\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_done:\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m current_reasoning, \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/github/LlamaIndex/llama_index/llama_index/agent/react/step.py:203\u001b[0m, in \u001b[0;36mReActAgentWorker._extract_reasoning_step\u001b[0;34m(self, output, is_streaming)\u001b[0m\n\u001b[1;32m    201\u001b[0m     reasoning_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_parser\u001b[38;5;241m.\u001b[39mparse(message_content, is_streaming)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage_content\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose:\n\u001b[1;32m    205\u001b[0m     print_text(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreasoning_step\u001b[38;5;241m.\u001b[39mget_content()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpink\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not parse output: Thought: Now that we have an understanding of query engine tools and their use cases, let's move on to query planning. Query planning involves the process of determining the most efficient way to execute a query. It includes tasks such as query optimization, cost estimation, and selecting the appropriate execution plan.\n\nTo demonstrate the difference between query engine tools and query planning, let's consider a code example:\n\n```python\n# Query Engine Tools\ndef preprocess_query(query):\n    # Perform query transformations\n    transformed_query = hyde_transform(query)\n    return transformed_query\n\ndef execute_query(query):\n    # Execute the transformed query against the index\n    results = index.search(query)\n    return results\n\n# Query Planning\ndef plan_query(query):\n    # Perform query planning tasks\n    optimized_query = optimize_query(query)\n    estimated_cost = estimate_cost(optimized_query)\n    execution_plan = select_execution_plan(optimized_query)\n    return execution_plan\n\ndef execute_plan(execution_plan):\n    # Execute the query execution plan\n    results = execute(execution_plan)\n    return results\n\n# Example usage\nquery = \"What are the top restaurants in New York City?\"\ntransformed_query = preprocess_query(query)\nresults = execute_query(transformed_query)\n\nexecution_plan = plan_query(query)\nresults = execute_plan(execution_plan)\n```\n\nIn this code example, the `preprocess_query` function represents the query engine tools. It takes a query as input, performs query transformations using the HyDE transform, and returns the transformed query. The `execute_query` function then executes the transformed query against the index and returns the results.\n\nOn the other hand, the `plan_query` function represents the query planning process. It takes a query as input and performs query planning tasks such as query optimization, cost estimation, and selecting the appropriate execution plan. The `execute_plan` function then executes the query execution plan and returns the results.\n\nIn summary, query engine tools focus on transforming and executing queries against index structures, while query planning involves determining the most efficient way to execute a query by optimizing, estimating costs, and selecting execution plans."
     ]
    }
   ],
   "source": [
    "response = top_agent.query(\n",
    "   question\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can install LlamaIndex if you are working with Supabase by following the installation instructions provided in the LlamaIndex documentation. The documentation should provide step-by-step instructions on how to install and set up LlamaIndex for use with Supabase.\n"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "response = base_query_engine.query(\n",
    "    question\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan 1/Agentic: get the document in md text format with the document structure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agentic Approach 1: \n",
    "We have two controlled variables: \n",
    "* document\n",
    "* metadata\n",
    "\n",
    "^ notice that the above ReActAgent not only errors, the result is completely hallucinated; And the baseline code does not return relevant info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement User Feedback as a tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Can you tell me the story of the creator of mona lisa? Ask user if the replying language is unspecified\n",
      "=== Calling Function ===\n",
      "Calling function: ask_reply_language with args: {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you provide in what language are you expecting the answer to be? chinese\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got output: chinese\n",
      "========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response='Êä±Ê≠âÔºåÊàëÂè™ËÉΩ‰ΩøÁî®Ëã±ËØ≠ËøõË°åÂõûÂ§ç„ÄÇÊòØÂê¶ÂèØ‰ª•‰ΩøÁî®Ëã±ËØ≠ËøõË°å‰∫§ÊµÅÔºü', sources=[ToolOutput(content='chinese', tool_name='ask_reply_language', raw_input={'args': (), 'kwargs': {}}, raw_output='chinese')], source_nodes=[])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.agent import OpenAIAgent\n",
    "from llama_index.tools.function_tool import FunctionTool\n",
    "\n",
    "# Use a tool spec from Llama-Hub\n",
    "\n",
    "\n",
    "# Create a custom tool. Type annotations and docstring are used for the\n",
    "# tool definition sent to the Function calling API.\n",
    "def ask_reply_language() -> str:\n",
    "    \"\"\"\n",
    "    Ask the human of the replying language\n",
    "    \"\"\"\n",
    "    return input(\"Can you provide in what language are you expecting the answer to be?\")\n",
    "\n",
    "\n",
    "function_tool = FunctionTool.from_defaults(fn=ask_reply_language)\n",
    "\n",
    "tools = [function_tool]\n",
    "agent = OpenAIAgent.from_tools(tools, verbose=True)\n",
    "\n",
    "# use agent\n",
    "agent.chat(\n",
    "    \"Can you tell me the story of the creator of mona lisa? Ask user if the replying language is unspecified\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-api-python-client\n",
      "  Downloading google_api_python_client-2.116.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting httplib2<1.dev0,>=0.15.0 (from google-api-python-client)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-auth<3.0.0.dev0,>=1.19.0 in /Users/sasha/miniconda3/envs/llama/lib/python3.11/site-packages (from google-api-python-client) (2.26.2)\n",
      "Collecting google-auth-httplib2>=0.1.0 (from google-api-python-client)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /Users/sasha/miniconda3/envs/llama/lib/python3.11/site-packages (from google-api-python-client) (2.15.0)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /Users/sasha/miniconda3/envs/llama/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.62.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /Users/sasha/miniconda3/envs/llama/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.25.2)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /Users/sasha/miniconda3/envs/llama/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.31.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/sasha/miniconda3/envs/llama/lib/python3.11/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/sasha/miniconda3/envs/llama/lib/python3.11/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/sasha/miniconda3/envs/llama/lib/python3.11/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (4.9)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2<1.dev0,>=0.15.0->google-api-python-client)\n",
      "  Using cached pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/sasha/miniconda3/envs/llama/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sasha/miniconda3/envs/llama/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sasha/miniconda3/envs/llama/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sasha/miniconda3/envs/llama/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sasha/miniconda3/envs/llama/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2023.11.17)\n",
      "Downloading google_api_python_client-2.116.0-py2.py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Using cached pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "Installing collected packages: uritemplate, pyparsing, httplib2, google-auth-httplib2, google-api-python-client\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "matplotlib 3.8.2 requires pillow>=8, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-api-python-client-2.116.0 google-auth-httplib2-0.2.0 httplib2-0.22.0 pyparsing-3.1.1 uritemplate-4.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade google-api-python-client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subclassing CustomAgentWorker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import CustomSimpleAgentWorker, Task, AgentChatResponse\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "from llama_index.core.tools import BaseTool, QueryEngineTool\n",
    "from llama_index.core.program import LLMTextCompletionProgram\n",
    "from llama_index.core.output_parsers import PydanticOutputParser\n",
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "from llama_index.core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from llama_index.core.selectors import PydanticSingleSelector\n",
    "from pydantic import Field, BaseModel\n",
    "\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "\n",
    "DEFAULT_PROMPT_STR = \"\"\"\n",
    "Given previous question/response pairs, please determine if an error has occurred in the response, and suggest \\\n",
    "    a modified question that will not trigger the error.\n",
    "\n",
    "Examples of modified questions:\n",
    "- The question itself is modified to elicit a non-erroneous response\n",
    "- The question is augmented with context that will help the downstream system better answer the question.\n",
    "- The question is augmented with examples of negative responses, or other negative questions.\n",
    "\n",
    "An error means that either an exception has triggered, or the response is completely irrelevant to the question.\n",
    "\n",
    "Please return the evaluation of the response in the following JSON format.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_chat_prompt_template(\n",
    "    system_prompt: str, current_reasoning: Tuple[str, str]\n",
    ") -> ChatPromptTemplate:\n",
    "    system_msg = ChatMessage(role=MessageRole.SYSTEM, content=system_prompt)\n",
    "    messages = [system_msg]\n",
    "    for raw_msg in current_reasoning:\n",
    "        if raw_msg[0] == \"user\":\n",
    "            messages.append(\n",
    "                ChatMessage(role=MessageRole.USER, content=raw_msg[1])\n",
    "            )\n",
    "        else:\n",
    "            messages.append(\n",
    "                ChatMessage(role=MessageRole.ASSISTANT, content=raw_msg[1])\n",
    "            )\n",
    "    return ChatPromptTemplate(message_templates=messages)\n",
    "\n",
    "\n",
    "class ResponseEval(BaseModel):\n",
    "    \"\"\"Evaluation of whether the response has an error.\"\"\"\n",
    "\n",
    "    has_error: bool = Field(..., description=\"Whether the response has an error\")\n",
    "    requires_human_input: bool = Field(\n",
    "        ..., description=\"Whether the response needs human input, if human input is required, it means it is not erroneous\"\n",
    "    )\n",
    "    clarifying_question: str = Field(..., description=\"The suggested new question, if human input is required\")\n",
    "    explanation: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"The explanation for the error as well as for the clarifying question.\"\n",
    "            \"Can include the direct stack trace as well.\"\n",
    "        ),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import PrivateAttr\n",
    "from typing import Any, Deque, Dict, List, Optional, Union, cast\n",
    "\n",
    "from llama_index.core.agent.types import (\n",
    "    BaseAgentWorker,\n",
    "    Task,\n",
    "    TaskStep,\n",
    "    TaskStepOutput,\n",
    ")\n",
    "from llama_index.core.agent.custom.simple import CustomSimpleAgentWorker\n",
    "from llama_index.core.callbacks import (\n",
    "    CallbackManager,\n",
    "    trace_method,\n",
    ")\n",
    "from llama_index.core.chat_engine.types import (\n",
    "    AGENT_CHAT_RESPONSE_TYPE\n",
    ")\n",
    "\n",
    "class HumanInputRequiredException(Exception):\n",
    "    \"\"\"Exception raised when human input is required.\"\"\"\n",
    "\n",
    "    def __init__(self, message=\"Human input is required\", task_id: Optional[str] = None, step: TaskStep = None):\n",
    "        self.message = message\n",
    "        self.task_id = task_id\n",
    "        self.step = step\n",
    "        super().__init__(self.message)\n",
    "\n",
    "\n",
    "class RetryAgentWorker(CustomSimpleAgentWorker):\n",
    "    \"\"\"Agent worker that adds a retry layer on top of a router.\n",
    "\n",
    "    Continues iterating until there's no errors / task is done.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_str: str = Field(default=DEFAULT_PROMPT_STR)\n",
    "    max_iterations: int = Field(default=10)\n",
    "\n",
    "    _router_query_engine: RouterQueryEngine = PrivateAttr()\n",
    "\n",
    "    def __init__(self, tools: List[BaseTool], **kwargs: Any) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        # validate that all tools are query engine tools\n",
    "        for tool in tools:\n",
    "            if not isinstance(tool, QueryEngineTool):\n",
    "                raise ValueError(\n",
    "                    f\"Tool {tool.metadata.name} is not a query engine tool.\"\n",
    "                )\n",
    "        self._router_query_engine = RouterQueryEngine(\n",
    "            selector=PydanticSingleSelector.from_defaults(),\n",
    "            query_engine_tools=tools,\n",
    "            verbose=kwargs.get(\"verbose\", False),\n",
    "        )\n",
    "        super().__init__(\n",
    "            tools=tools,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def _initialize_state(self, task: Task, **kwargs: Any) -> Dict[str, Any]:\n",
    "        \"\"\"Initialize state.\"\"\"\n",
    "        return {\"count\": 0, \"current_reasoning\": []}\n",
    "\n",
    "    def _run_step(\n",
    "        self, state: Dict[str, Any], task: Task, input: Optional[str] = None\n",
    "    ) -> Tuple[AgentChatResponse, bool, bool]:\n",
    "        \"\"\"Run step.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (agent_response, is_done)\n",
    "\n",
    "        \"\"\"\n",
    "        if input is not None:\n",
    "            # if input is specified, override input\n",
    "            new_input = input\n",
    "        elif \"new_input\" not in state:\n",
    "            new_input = task.input\n",
    "        else:\n",
    "            new_input = state[\"new_input\"][\"text\"]\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"> Current Input: {new_input}\")\n",
    "\n",
    "        # first run router query engine\n",
    "        response = self._router_query_engine.query(new_input)\n",
    "\n",
    "        # append to current reasoning\n",
    "        state[\"current_reasoning\"].extend(\n",
    "            [(\"user\", new_input), (\"assistant\", str(response))]\n",
    "        )\n",
    "        print(f'current_reasoning: {state[\"current_reasoning\"]}')\n",
    "\n",
    "        # Then, check for errors\n",
    "        # dynamically create pydantic program for structured output extraction based on template\n",
    "        chat_prompt_tmpl = get_chat_prompt_template(\n",
    "            self.prompt_str, state[\"current_reasoning\"]\n",
    "        )\n",
    "        llm_program = LLMTextCompletionProgram.from_defaults(\n",
    "            output_parser=PydanticOutputParser(output_cls=ResponseEval),\n",
    "            prompt=chat_prompt_tmpl,\n",
    "            llm=self.llm,\n",
    "        )\n",
    "        # run program, look at the result\n",
    "        response_eval = llm_program(\n",
    "            query_str=new_input, response_str=str(response)\n",
    "        )\n",
    "        print(f\"result: {response_eval}\")\n",
    "        if not response_eval.has_error:\n",
    "            is_done = True\n",
    "        else:\n",
    "            is_done = False\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"> Question: {new_input}\")\n",
    "            print(f\"> Response: {response}\")\n",
    "            print(f\"> Response eval: {response_eval.dict()}\")\n",
    "\n",
    "        # return response\n",
    "        if response_eval.requires_human_input:\n",
    "            return AgentChatResponse(response=str(response_eval.clarifying_question)), is_done, True\n",
    "            \n",
    "        return AgentChatResponse(response=str(response)), is_done, False\n",
    "\n",
    "    @trace_method(\"run_step\")\n",
    "    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:\n",
    "        \"\"\"Run step.\"\"\"\n",
    "\n",
    "        agent_response, is_done, is_interrupted = self._run_step(\n",
    "            step.step_state, task, input=step.input\n",
    "        )\n",
    "        if is_interrupted:\n",
    "             raise HumanInputRequiredException(agent_response, task_id=task.task_id, step=step)\n",
    "    \n",
    "        response = self._get_task_step_response(agent_response, step, is_done)\n",
    "        # sync step state with task state\n",
    "        task.extra_state.update(step.step_state)\n",
    "        return response\n",
    "    \n",
    "    def _finalize_task(self, state: Dict[str, Any], **kwargs) -> None:\n",
    "        \"\"\"Finalize task.\"\"\"\n",
    "        # nothing to finalize here\n",
    "        # this is usually if you want to modify any sort of\n",
    "        # internal state beyond what is set in `_initialize_state`\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.wikipedia import WikipediaReader\n",
    "from llama_index.core import VectorStoreIndex\n",
    "cities = [\"Toronto\", \"Berlin\", \"Tokyo\"]\n",
    "wiki_docs = WikipediaReader().load_data(pages=cities)\n",
    "# build a separate vector index per city\n",
    "# You could also choose to define a single vector index across all docs, and annotate each chunk by metadata\n",
    "vector_tools = []\n",
    "for city, wiki_doc in zip(cities, wiki_docs):\n",
    "    vector_index = VectorStoreIndex.from_documents([wiki_doc])\n",
    "    vector_query_engine = vector_index.as_query_engine()\n",
    "    vector_tool = QueryEngineTool.from_defaults(\n",
    "        query_engine=vector_query_engine,\n",
    "        description=f\"Useful for answering semantic questions about {city}\",\n",
    "    )\n",
    "    vector_tools.append(vector_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\"RetryAgentWorker\" object has no field \"_router_query_engine\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mcallback_manager\n\u001b[1;32m      7\u001b[0m query_engine_tools \u001b[38;5;241m=\u001b[39m vector_tools\n\u001b[0;32m----> 8\u001b[0m agent_worker \u001b[38;5;241m=\u001b[39m \u001b[43mRetryAgentWorker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tools\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_engine_tools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m agent \u001b[38;5;241m=\u001b[39m AgentRunner(agent_worker, callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/llama_index/core/agent/custom/simple.py:113\u001b[0m, in \u001b[0;36mCustomSimpleAgentWorker.from_tools\u001b[0;34m(cls, tools, tool_retriever, llm, callback_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callback_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m     llm\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m callback_manager\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtool_retriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_retriever\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 49\u001b[0m, in \u001b[0;36mRetryAgentWorker.__init__\u001b[0;34m(self, tools, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool, QueryEngineTool):\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     47\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTool \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtool\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a query engine tool.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m         )\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_router_query_engine\u001b[49m \u001b[38;5;241m=\u001b[39m RouterQueryEngine(\n\u001b[1;32m     50\u001b[0m     selector\u001b[38;5;241m=\u001b[39mPydanticSingleSelector\u001b[38;5;241m.\u001b[39mfrom_defaults(),\n\u001b[1;32m     51\u001b[0m     query_engine_tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m     52\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     53\u001b[0m )\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     55\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     57\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/pydantic/v1/main.py:357\u001b[0m, in \u001b[0;36mBaseModel.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m object_setattr(\u001b[38;5;28mself\u001b[39m, name, value)\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__config__\u001b[38;5;241m.\u001b[39mextra \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Extra\u001b[38;5;241m.\u001b[39mallow \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__fields__:\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m object has no field \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__config__\u001b[38;5;241m.\u001b[39mallow_mutation \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__config__\u001b[38;5;241m.\u001b[39mfrozen:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is immutable and does not support item assignment\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: \"RetryAgentWorker\" object has no field \"_router_query_engine\""
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent import AgentRunner\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4\")\n",
    "callback_manager = llm.callback_manager\n",
    "\n",
    "query_engine_tools = vector_tools\n",
    "agent_worker = RetryAgentWorker.from_tools(\n",
    "    query_engine_tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    callback_manager=callback_manager,\n",
    ")\n",
    "agent = AgentRunner(agent_worker, callback_manager=callback_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input is not none\n",
      "> Current Input: Which countries are each city from?\n",
      "\u001b[1;3;38;5;200mSelecting query engine 0: The choice is about answering semantic questions about cities, which includes identifying the countries they belong to..\n",
      "\u001b[0mresult: has_error=True requires_human_input=True clarifying_question='Could you please specify which cities you are referring to?' explanation='The original question is too vague as it does not specify which cities the user is interested in. Therefore, it is impossible to provide a correct response without additional information.'\n",
      "> Question: Which countries are each city from?\n",
      "> Response: The context information does not provide any specific information about the countries that each city mentioned belongs to.\n",
      "> Response eval: {'has_error': True, 'requires_human_input': True, 'clarifying_question': 'Could you please specify which cities you are referring to?', 'explanation': 'The original question is too vague as it does not specify which cities the user is interested in. Therefore, it is impossible to provide a correct response without additional information.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could you please specify which cities you are referring to? toronto and tokyo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input is not none\n",
      "> Current Input: Which countries are the cities Toronto and Tokyo located in?\n",
      "\u001b[1;3;38;5;200mSelecting query engine 2: Tokyo is mentioned in choice 3.\n",
      "\u001b[0mresult: has_error=False requires_human_input=False clarifying_question='' explanation='The response is correct and relevant to the question. No error has occurred.'\n",
      "> Question: Which countries are the cities Toronto and Tokyo located in?\n",
      "> Response: Canada and Japan.\n",
      "> Response eval: {'has_error': False, 'requires_human_input': False, 'clarifying_question': '', 'explanation': 'The response is correct and relevant to the question. No error has occurred.'}\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "orig_question = \"Which countries are each city from?\"\n",
    "llm = OpenAI(model=\"gpt-4\")\n",
    "clarifying_questions = []\n",
    "try:\n",
    "    response = agent.chat(orig_question)\n",
    "except HumanInputRequiredException as e:\n",
    "    response = input(e.message)\n",
    "    question = orig_question\n",
    "    clarifying_questions.append((e.message, response))\n",
    "    should_end = False\n",
    "    while not should_end:\n",
    "        clarifying_texts = \"\\n\".join([f\"\"\"\n",
    "   Q: {question}\n",
    "   A: {answer}\n",
    "        \"\"\" for question, answer in clarifying_questions])\n",
    "        query_text = f\"\"\"\n",
    "Given a query and a set of clarifying questions, please rewrite the query to be more clear.\n",
    "Example:\n",
    "Q: What trajectory is the monthly earning from the three months: April, May and June?\n",
    "Clarifying Questions:\n",
    "   Q: What year are xyou referring to?\n",
    "   A: In 2022\n",
    "   Q: What company are you referring to?\n",
    "   A: Uber\n",
    "Rewrite: What was the trajectory of Uber's monthly earnings for the months of April, May, and June in 2022?\n",
    "\n",
    "Q:{question}\n",
    "Clarifying Questions: {clarifying_texts}\n",
    "Rewrite: \"\"\"\n",
    "        rewrite_response = llm.complete(query_text)\n",
    "        e.step.input = rewrite_response.text\n",
    "        e.step.step_state[\"current_reasoning\"].append(('user', f'revised query: {rewrite_response}'))\n",
    "        question = rewrite_response\n",
    "        try:\n",
    "            output = agent.run_step(task_id=e.task_id, step=e.step)\n",
    "            should_end = output.is_last\n",
    "        except HumanInputRequiredException as er:\n",
    "            response = input(er.message)\n",
    "            clarifying_questions.append((er.message, response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "orig_question = \"Which countries are each city from?\"\n",
    "llm = OpenAI(model=\"gpt-4\")\n",
    "clarifying_questions = []\n",
    "try:\n",
    "    response = agent.chat(orig_question)\n",
    "except HumanInputRequiredException as e:\n",
    "    response = input(e.message)\n",
    "    clarifying_questions.append((e.message, response))\n",
    "    should_end = False\n",
    "    while not should_end:\n",
    "        clarifying_texts = \"\\n\".join([f\"\"\"\n",
    "   Q: {question}\n",
    "   A: {answer}\n",
    "        \"\"\" for question, answer in clarifying_questions])\n",
    "        query_text = f\"\"\"\n",
    "Given a query and a set of clarifying questions, please rewrite the query to be more clear.\n",
    "Example:\n",
    "Q: What trajectory is the monthly earning from the three months: April, May and June?\n",
    "Clarifying Questions:\n",
    "   Q: What year are you referring to?\n",
    "   A: In 2022\n",
    "   Q: What company are you referring to?\n",
    "   A: Uber\n",
    "Rewrite: What was the trajectory of Uber's monthly earnings for the months of April, May, and June in 2022?\n",
    "\n",
    "Q:{orig_question}\n",
    "Clarifying Questions: {clarifying_texts}\n",
    "Rewrite: \"\"\"\n",
    "        rewrite_response = llm.complete(query_text)\n",
    "        orig_question = rewrite_response\n",
    "        try:\n",
    "            output = agent.chat(rewrite_response.text)\n",
    "            should_end = True\n",
    "        except HumanInputRequiredException as er:\n",
    "            response = input(er.message)\n",
    "            clarifying_questions.append((er.message, response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
