{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SummaryIndex,\n",
    "    SimpleKeywordTableIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    ")\n",
    "from llama_index.schema import IndexNode\n",
    "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.llms import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from llama_index import VectorStoreIndex, SummaryIndex\n",
    "import glob\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm)\n",
    "sample_folder = \"/Users/sasha/github/LlamaIndex/llama_index/docs/_build/text/optimizing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent import OpenAIAgent\n",
    "from llama_index import load_index_from_storage, StorageContext\n",
    "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.node_parser import SentenceSplitter\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "\n",
    "async def build_agent_per_doc(nodes, file_base):\n",
    "    print(file_base)\n",
    "\n",
    "    vi_out_path = f\"./data/llamaindex_docs_prototype/{file_base}\"\n",
    "    summary_out_path = f\"./data/llamaindex_docs_prototype/{file_base}_summary.pkl\"\n",
    "    if not os.path.exists(vi_out_path):\n",
    "        Path(\"./data/llamaindex_docs/\").mkdir(parents=True, exist_ok=True)\n",
    "        # build vector index\n",
    "        vector_index = VectorStoreIndex(nodes, service_context=service_context)\n",
    "        vector_index.storage_context.persist(persist_dir=vi_out_path)\n",
    "    else:\n",
    "        vector_index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=vi_out_path),\n",
    "            service_context=service_context,\n",
    "        )\n",
    "\n",
    "    # build summary index\n",
    "    summary_index = SummaryIndex(nodes, service_context=service_context)\n",
    "\n",
    "    # define query engines\n",
    "    vector_query_engine = vector_index.as_query_engine()\n",
    "    summary_query_engine = summary_index.as_query_engine(\n",
    "        # response_mode=\"tree_summarize\"\n",
    "    )\n",
    "\n",
    "    # extract a summary\n",
    "    if not os.path.exists(summary_out_path):\n",
    "        Path(summary_out_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        summary = str(\n",
    "            await summary_query_engine.aquery(\n",
    "                \"Extract a concise 1-2 line summary of this document\"\n",
    "            )\n",
    "        )\n",
    "        pickle.dump(summary, open(summary_out_path, \"wb\"))\n",
    "    else:\n",
    "        summary = pickle.load(open(summary_out_path, \"rb\"))\n",
    "\n",
    "    # define tools\n",
    "    query_engine_tools = [\n",
    "        QueryEngineTool(\n",
    "            query_engine=vector_query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=f\"vector_tool_{file_base}\",\n",
    "                description=f\"Useful for questions related to specific facts\",\n",
    "            ),\n",
    "        ),\n",
    "        QueryEngineTool(\n",
    "            query_engine=summary_query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=f\"summary_tool_{file_base}\",\n",
    "                description=f\"Useful for summarization questions\",\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # build agent\n",
    "    function_llm = OpenAI(model=\"gpt-4\")\n",
    "    agent = OpenAIAgent.from_tools(\n",
    "        query_engine_tools,\n",
    "        llm=function_llm,\n",
    "        verbose=True,\n",
    "        system_prompt=f\"\"\"\\\n",
    "You are a specialized agent designed to answer queries about the `{file_base}.html` part of the LlamaIndex docs.\n",
    "You must ALWAYS use at least one of the tools provided when answering a question; do NOT rely on prior knowledge.\\\n",
    "\"\"\",\n",
    "    )\n",
    "\n",
    "    return agent, summary"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
