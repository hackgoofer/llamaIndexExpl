{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 98/98 [00:11<00:00,  8.53it/s]\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.html import partition_html\n",
    "cnn_lite_url = \"https://lite.cnn.com/\"\n",
    "elements = partition_html(url=cnn_lite_url)\n",
    "links = []\n",
    "for element in elements:\n",
    "    if element.metadata.link_urls is not None:\n",
    "        relative_link = element.metadata.link_urls[0][1:]\n",
    "        if relative_link.startswith(\"2024\"):\n",
    "            links.append(f\"{cnn_lite_url}{relative_link}\")\n",
    "\n",
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "\n",
    "loaders = UnstructuredURLLoader(urls=links, show_progress_bar=True)\n",
    "docs = loaders.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "getting_started = \"/Users/sasha/github/LlamaIndex/llama_index/docs/_build/html/getting_started\"\n",
    "html_files = glob.glob(os.path.join(getting_started, \"*.html\"))\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "\n",
    "for file in html_files:\n",
    "    loaders = UnstructuredHTMLLoader(file_path=file, show_progress_bar=True)\n",
    "    docs = loaders.load()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = UnstructuredHTMLLoader(file_path=getting_started + \"/starter_example_wo_sidebar.html\", show_progress_bar=True)\n",
    "docs = loaders.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sasha/miniconda3/envs/llama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(docs, embeddings)\n",
    "query_docs = vectorstore.similarity_search(\"Paul Graham\", k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sasha/miniconda3/envs/llama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "/Users/sasha/miniconda3/envs/llama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This tutorial provides a step-by-step guide on how to use LlamaIndex, a tool for building and querying indexes. It includes instructions on downloading data, setting up the OpenAI API key, loading data and building an index, querying the data, viewing queries and events using logging, and storing the index. The tutorial also suggests next steps for further customization and learning.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\")\n",
    "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
    "print(chain.run(query_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Starter Tutorial\\uf0c1\\n\\nTip\\n\\nMake sure youâ€™ve followed the installation steps first.\\n\\nThis is our famous â€œ5 lines of codeâ€ starter example.\\n\\nDownload data\\uf0c1\\n\\nThis example uses the text of Paul Grahamâ€™s essay, â€œWhat I Worked Onâ€. This and many other examples can be found in the examples folder of our repo.\\n\\nThe easiest way to get it is to download it via this link and save it in a folder called data.\\n\\nSet your OpenAI API key\\uf0c1\\n\\nLlamaIndex uses OpenAIâ€™s gpt-3.5-turbo by default. Make sure your API key is available to your code by setting it as an environment variable. In MacOS and Linux, this is the command:\\n\\nexport\\n\\nOPENAI_API_KEY\\n\\nXXXXX\\n\\nand on windows it is\\n\\nset\\n\\nOPENAI_API_KEY\\n\\nXXXXX\\n\\nLoad data and build an index\\uf0c1\\n\\nIn the same folder where you created the data folder, create a file called starter.py file with the following:\\n\\nfrom\\n\\nllama_index\\n\\nimport\\n\\nVectorStoreIndex\\n\\nSimpleDirectoryReader\\n\\ndocuments\\n\\nSimpleDirectoryReader\\n\\n\"data\"\\n\\nload_data\\n\\n()\\n\\nindex\\n\\nVectorStoreIndex\\n\\nfrom_documents\\n\\ndocuments\\n\\nThis builds an index over the documents in the data folder (which in this case just consists of the essay text, but could contain many documents).\\n\\nYour directory structure should look like this:\\n\\nâ”œâ”€â”€ starter.py\\nâ””â”€â”€ data\\n \\xa0\\xa0 â””â”€â”€ paul_graham_essay.txt\\n\\nQuery your data\\uf0c1\\n\\nAdd the following lines to starter.py\\n\\nquery_engine\\n\\nindex\\n\\nas_query_engine\\n\\n()\\n\\nresponse\\n\\nquery_engine\\n\\nquery\\n\\n\"What did the author do growing up?\"\\n\\nprint\\n\\nresponse\\n\\nThis creates an engine for Q&A over your index and asks a simple question. You should get back a response similar to the following: The author wrote short stories and tried to program on an IBM 1401.\\n\\nViewing Queries and Events Using Logging\\uf0c1\\n\\nWant to see whatâ€™s happening under the hood? Letâ€™s add some logging. Add these lines to the top of starter.py:\\n\\nimport\\n\\nlogging\\n\\nimport\\n\\nsys\\n\\nlogging\\n\\nbasicConfig\\n\\nstream\\n\\nsys\\n\\nstdout\\n\\nlevel\\n\\nlogging\\n\\nDEBUG\\n\\nlogging\\n\\ngetLogger\\n\\n()\\n\\naddHandler\\n\\nlogging\\n\\nStreamHandler\\n\\nstream\\n\\nsys\\n\\nstdout\\n\\n))\\n\\nYou can set the level to DEBUG for verbose output, or use level=logging.INFO for less.\\n\\nStoring your index\\uf0c1\\n\\nBy default, the data you just loaded is stored in memory as a series of vector embeddings. You can save time (and requests to OpenAI) by saving the embeddings to disk. That can be done with this line:\\n\\nindex\\n\\nstorage_context\\n\\npersist\\n\\n()\\n\\nBy default, this will save the data to the directory storage, but you can change that by passing a persist_dir parameter.\\n\\nOf course, you donâ€™t get the benefits of persisting unless you load the data. So letâ€™s modify starter.py to generate and store the index if it doesnâ€™t exist, but load it if it does:\\n\\nimport\\n\\nos.path\\n\\nfrom\\n\\nllama_index\\n\\nimport\\n\\nVectorStoreIndex\\n\\nSimpleDirectoryReader\\n\\nStorageContext\\n\\nload_index_from_storage\\n\\n# check if storage already exists\\n\\nPERSIST_DIR\\n\\n\"./storage\"\\n\\nif\\n\\nnot\\n\\nos\\n\\npath\\n\\nexists\\n\\nPERSIST_DIR\\n\\n):\\n\\n# load the documents and create the index\\n\\ndocuments\\n\\nSimpleDirectoryReader\\n\\n\"data\"\\n\\nload_data\\n\\n()\\n\\nindex\\n\\nVectorStoreIndex\\n\\nfrom_documents\\n\\ndocuments\\n\\n# store it for later\\n\\nindex\\n\\nstorage_context\\n\\npersist\\n\\npersist_dir\\n\\nPERSIST_DIR\\n\\nelse\\n\\n# load the existing index\\n\\nstorage_context\\n\\nStorageContext\\n\\nfrom_defaults\\n\\npersist_dir\\n\\nPERSIST_DIR\\n\\nindex\\n\\nload_index_from_storage\\n\\nstorage_context\\n\\n# either way we can now query the index\\n\\nquery_engine\\n\\nindex\\n\\nas_query_engine\\n\\n()\\n\\nresponse\\n\\nquery_engine\\n\\nquery\\n\\n\"What did the author do growing up?\"\\n\\nprint\\n\\nresponse\\n\\nNow you can efficiently query to your heartâ€™s content! But this is just the beginning of what you can do with LlamaIndex.\\n\\nNext Steps\\n\\nlearn more about the high-level concepts.\\n\\ntell me how to customize things.\\n\\ncurious about a specific module? check out the guides on the left ðŸ‘ˆ\\n\\nNext\\n                \\n                High-Level Concepts\\n\\nPrevious\\n                \\n                \\n                How to read these docs\\n\\nCopyright Â© 2023, Jerry Liu\\n            \\n            Made with\\n\\nSphinx and\\n\\n@pradyunsg\\'s\\n\\nFuro\\n\\nOn this page\\n\\nStarter Tutorial\\nDownload data\\nSet your OpenAI API key\\nLoad data and build an index\\nQuery your data\\nViewing Queries and Events Using Logging\\nStoring your index', metadata={'source': '/Users/sasha/github/LlamaIndex/llama_index/docs/_build/html/getting_started/starter_example_wo_sidebar.html'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]\n",
    "# Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
